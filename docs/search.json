[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/hospital/ReplicationBiasPost.html",
    "href": "posts/hospital/ReplicationBiasPost.html",
    "title": "Replication Study - Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "In this blogpost I seek to recreate the findings of Obermeyer et. al. (2019), in which they explore a potential algorithm for use scoring high risk patients. This score would then effect the resources and attention dedicated to a patient. Obermeyer et. al. found that the model was biased in that it used costs as an informant of risk scores, and as Black patients costs were typically lower than identical white patients. Obermeyer et. al. posited that this difference could be due to lack of access to healthcare despite appearance of chronic illness, and not that Black patients truly required less healthcare (and therefore healthcare spending) in the next year. We use simulated data based on the data Obermeyer et. al. used to examine whether or not we see a similar pattern in our own recreation of their model and analysis.\n\n\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\n\n\nIn my recreation of figure 1, I decided to follow Professor Chodrow’s example of seperating the data by gender.\n\nimport matplotlib as plt\nimport seaborn as sns\n\nFirst, I calculated the risk percentile scores for every entry of the dataset.\n\ndf['risk_perc'] = (df['risk_score_t'].rank(pct = True)*100).round()\n\nThen, I seperated the entries out into seperate datasets to make them easier to work with, one for each gender and racial combination.\n\nfemale_b_data = df[(df['dem_female'] == 1) & (df['race'] == 'black')]\nfemale_w_data = df[(df['dem_female'] == 1) & (df['race'] == 'white')]\n\nmale_b_data = df[(df['dem_female'] == 0) & (df['race'] == 'black')]\nmale_w_data = df[(df['dem_female'] == 0) & (df['race'] == 'white')]\n\nThen I performed a series of calculations on each of the individual datasets. For each I calculated the mean number of chronic illnesses by risk percentile, and copied over the original racial and gender distinctions. Afterwards, I re-combined all four into one large dataset for graphing.\n\nmean_chronic_illnesses_female_b = female_b_data.groupby('risk_perc')['gagne_sum_t'].mean()\nmean_chronic_illnesses_female_w = female_w_data.groupby('risk_perc')['gagne_sum_t'].mean()\n\nfemb = mean_chronic_illnesses_female_b.reset_index()\nfemb['gender'] = 1\nfemb['race'] = 'black'\n\nfemw = mean_chronic_illnesses_female_w.reset_index()\nfemw['gender'] = 1\nfemw['race'] = 'white'\n\n\nmean_chronic_illnesses_male_b = male_b_data.groupby('risk_perc')['gagne_sum_t'].mean()\nmean_chronic_illnesses_male_w = male_w_data.groupby('risk_perc')['gagne_sum_t'].mean()\n\nmalb = mean_chronic_illnesses_male_b.reset_index()\nmalb['gender'] = 0\nmalb['race'] = 'black'\n\nmalw = mean_chronic_illnesses_male_w.reset_index()\nmalw['gender'] = 0\nmalw['race'] = 'white'\n\ncombined_data = pd.concat([malw, malb, femw, femb])\n\n\ncombined_data\n\n\n\n\n\n\n\n\n\nrisk_perc\ngagne_sum_t\ngender\nrace\n\n\n\n\n0\n0.0\n0.030043\n0\nwhite\n\n\n1\n1.0\n0.113043\n0\nwhite\n\n\n2\n2.0\n0.112676\n0\nwhite\n\n\n3\n3.0\n0.276596\n0\nwhite\n\n\n4\n4.0\n0.117089\n0\nwhite\n\n\n...\n...\n...\n...\n...\n\n\n95\n96.0\n5.500000\n1\nblack\n\n\n96\n97.0\n4.880000\n1\nblack\n\n\n97\n98.0\n6.024390\n1\nblack\n\n\n98\n99.0\n6.228070\n1\nblack\n\n\n99\n100.0\n8.294118\n1\nblack\n\n\n\n\n403 rows × 4 columns\n\n\n\n\nI used the seaborn facetgrid function to graph two scatterplots side by side seperated by gender and colored according to race.\n\nsns.set_theme(style=\"whitegrid\")\nsns.set_theme(style=\"whitegrid\")\n\n\nhue_markers = {'white': 'o', 'black': 'X'}\n\ng = sns.FacetGrid(combined_data, col=\"gender\", hue='race', palette='Set2', height=5, aspect=1.5)\ng.map(sns.scatterplot, 'gagne_sum_t', 'risk_perc')\ng.add_legend()\ng.set_xlabels(\"Mean Number of Chronic Illnesses\")\ng.set_ylabels(\"Risk Score Percentile\")\ntitles = ['Male', 'Female']\nfor ax, title in zip(g.axes.flat, titles):\n    ax.set_title(title)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nIn order to recreate figure three, I used the risk percentile calculations from the previous figure recreation, and once again separated the data based on race for ease of calculation. For each group I calculated the average cost by number of chronic illness and risk percentile. Then I recombined these datasets for graphing.\n\nbdata = df[df['race']=='black']\nwdata = df[df['race']=='white']\n\nmeanb_cost = bdata.groupby('risk_perc')['cost_t'].mean().reset_index()\nmeanb_cost['race'] = 'black'\nmeanw_cost = wdata.groupby('risk_perc')['cost_t'].mean().reset_index()\nmeanw_cost['race'] = 'white'\n\nchronicb = bdata.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicw = wdata.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicb['race'] = 'black'\nchronicw['race'] = 'white'\n\ncombined_data_risk = pd.concat([meanw_cost, meanb_cost])\ncombined_data_chronic = pd.concat([chronicb, chronicw])\n\n\ncombined_data_chronic\n\n\n\n\n\n\n\n\n\ngagne_sum_t\ncost_t\nrace\n\n\n\n\n0\n0\n3309.931507\nblack\n\n\n1\n1\n5242.040185\nblack\n\n\n2\n2\n7176.976744\nblack\n\n\n3\n3\n10621.153846\nblack\n\n\n4\n4\n11261.046512\nblack\n\n\n5\n5\n15723.715415\nblack\n\n\n6\n6\n21571.153846\nblack\n\n\n7\n7\n31575.000000\nblack\n\n\n8\n8\n22543.478261\nblack\n\n\n9\n9\n48452.941176\nblack\n\n\n10\n10\n54806.000000\nblack\n\n\n11\n11\n57418.518519\nblack\n\n\n12\n12\n86836.842105\nblack\n\n\n13\n13\n78950.000000\nblack\n\n\n14\n14\n81916.666667\nblack\n\n\n15\n15\n88825.000000\nblack\n\n\n16\n16\n31150.000000\nblack\n\n\n0\n0\n4372.520651\nwhite\n\n\n1\n1\n6440.395738\nwhite\n\n\n2\n2\n7984.731978\nwhite\n\n\n3\n3\n10682.347236\nwhite\n\n\n4\n4\n13160.115607\nwhite\n\n\n5\n5\n18093.610548\nwhite\n\n\n6\n6\n21461.850649\nwhite\n\n\n7\n7\n28755.897436\nwhite\n\n\n8\n8\n32033.744856\nwhite\n\n\n9\n9\n40892.907801\nwhite\n\n\n10\n10\n45715.315315\nwhite\n\n\n11\n11\n45588.888889\nwhite\n\n\n12\n12\n52650.909091\nwhite\n\n\n13\n13\n72557.142857\nwhite\n\n\n14\n14\n75841.176471\nwhite\n\n\n15\n15\n62800.000000\nwhite\n\n\n16\n16\n38500.000000\nwhite\n\n\n17\n17\n55750.000000\nwhite\n\n\n\n\n\n\n\n\nI made two, side-by-side graphs to show average cost by risk percentile and number of chronic illnesses just like the authors did.\n\nf, axs = plt.subplots(1,2, figsize=(9,5), sharey=True)\nsns.scatterplot(data = combined_data_risk,x= 'risk_perc', y='cost_t', hue='race', ax=axs[0], palette='Set2')\naxs[0].set_xlabel('Risk Percentile')\naxs[0].set_ylabel('Mean Total Cost')\nplt.yscale('log')\n\nsns.scatterplot(data=combined_data_chronic,x='gagne_sum_t', y='cost_t', hue='race', ax=axs[1], palette='Set2')\naxs[1].set_xlabel('Mean Number of Chronic Illnesses')\naxs[1].set_ylabel('')\nplt.yscale('log')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\nThis figure shows how mean cost increases with the assigned risk percentile and number of chronic illnesses."
  },
  {
    "objectID": "posts/hospital/ReplicationBiasPost.html#abstract",
    "href": "posts/hospital/ReplicationBiasPost.html#abstract",
    "title": "Replication Study - Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "In this blogpost I seek to recreate the findings of Obermeyer et. al. (2019), in which they explore a potential algorithm for use scoring high risk patients. This score would then effect the resources and attention dedicated to a patient. Obermeyer et. al. found that the model was biased in that it used costs as an informant of risk scores, and as Black patients costs were typically lower than identical white patients. Obermeyer et. al. posited that this difference could be due to lack of access to healthcare despite appearance of chronic illness, and not that Black patients truly required less healthcare (and therefore healthcare spending) in the next year. We use simulated data based on the data Obermeyer et. al. used to examine whether or not we see a similar pattern in our own recreation of their model and analysis.\n\n\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\n\n\nIn my recreation of figure 1, I decided to follow Professor Chodrow’s example of seperating the data by gender.\n\nimport matplotlib as plt\nimport seaborn as sns\n\nFirst, I calculated the risk percentile scores for every entry of the dataset.\n\ndf['risk_perc'] = (df['risk_score_t'].rank(pct = True)*100).round()\n\nThen, I seperated the entries out into seperate datasets to make them easier to work with, one for each gender and racial combination.\n\nfemale_b_data = df[(df['dem_female'] == 1) & (df['race'] == 'black')]\nfemale_w_data = df[(df['dem_female'] == 1) & (df['race'] == 'white')]\n\nmale_b_data = df[(df['dem_female'] == 0) & (df['race'] == 'black')]\nmale_w_data = df[(df['dem_female'] == 0) & (df['race'] == 'white')]\n\nThen I performed a series of calculations on each of the individual datasets. For each I calculated the mean number of chronic illnesses by risk percentile, and copied over the original racial and gender distinctions. Afterwards, I re-combined all four into one large dataset for graphing.\n\nmean_chronic_illnesses_female_b = female_b_data.groupby('risk_perc')['gagne_sum_t'].mean()\nmean_chronic_illnesses_female_w = female_w_data.groupby('risk_perc')['gagne_sum_t'].mean()\n\nfemb = mean_chronic_illnesses_female_b.reset_index()\nfemb['gender'] = 1\nfemb['race'] = 'black'\n\nfemw = mean_chronic_illnesses_female_w.reset_index()\nfemw['gender'] = 1\nfemw['race'] = 'white'\n\n\nmean_chronic_illnesses_male_b = male_b_data.groupby('risk_perc')['gagne_sum_t'].mean()\nmean_chronic_illnesses_male_w = male_w_data.groupby('risk_perc')['gagne_sum_t'].mean()\n\nmalb = mean_chronic_illnesses_male_b.reset_index()\nmalb['gender'] = 0\nmalb['race'] = 'black'\n\nmalw = mean_chronic_illnesses_male_w.reset_index()\nmalw['gender'] = 0\nmalw['race'] = 'white'\n\ncombined_data = pd.concat([malw, malb, femw, femb])\n\n\ncombined_data\n\n\n\n\n\n\n\n\n\nrisk_perc\ngagne_sum_t\ngender\nrace\n\n\n\n\n0\n0.0\n0.030043\n0\nwhite\n\n\n1\n1.0\n0.113043\n0\nwhite\n\n\n2\n2.0\n0.112676\n0\nwhite\n\n\n3\n3.0\n0.276596\n0\nwhite\n\n\n4\n4.0\n0.117089\n0\nwhite\n\n\n...\n...\n...\n...\n...\n\n\n95\n96.0\n5.500000\n1\nblack\n\n\n96\n97.0\n4.880000\n1\nblack\n\n\n97\n98.0\n6.024390\n1\nblack\n\n\n98\n99.0\n6.228070\n1\nblack\n\n\n99\n100.0\n8.294118\n1\nblack\n\n\n\n\n403 rows × 4 columns\n\n\n\n\nI used the seaborn facetgrid function to graph two scatterplots side by side seperated by gender and colored according to race.\n\nsns.set_theme(style=\"whitegrid\")\nsns.set_theme(style=\"whitegrid\")\n\n\nhue_markers = {'white': 'o', 'black': 'X'}\n\ng = sns.FacetGrid(combined_data, col=\"gender\", hue='race', palette='Set2', height=5, aspect=1.5)\ng.map(sns.scatterplot, 'gagne_sum_t', 'risk_perc')\ng.add_legend()\ng.set_xlabels(\"Mean Number of Chronic Illnesses\")\ng.set_ylabels(\"Risk Score Percentile\")\ntitles = ['Male', 'Female']\nfor ax, title in zip(g.axes.flat, titles):\n    ax.set_title(title)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nIn order to recreate figure three, I used the risk percentile calculations from the previous figure recreation, and once again separated the data based on race for ease of calculation. For each group I calculated the average cost by number of chronic illness and risk percentile. Then I recombined these datasets for graphing.\n\nbdata = df[df['race']=='black']\nwdata = df[df['race']=='white']\n\nmeanb_cost = bdata.groupby('risk_perc')['cost_t'].mean().reset_index()\nmeanb_cost['race'] = 'black'\nmeanw_cost = wdata.groupby('risk_perc')['cost_t'].mean().reset_index()\nmeanw_cost['race'] = 'white'\n\nchronicb = bdata.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicw = wdata.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicb['race'] = 'black'\nchronicw['race'] = 'white'\n\ncombined_data_risk = pd.concat([meanw_cost, meanb_cost])\ncombined_data_chronic = pd.concat([chronicb, chronicw])\n\n\ncombined_data_chronic\n\n\n\n\n\n\n\n\n\ngagne_sum_t\ncost_t\nrace\n\n\n\n\n0\n0\n3309.931507\nblack\n\n\n1\n1\n5242.040185\nblack\n\n\n2\n2\n7176.976744\nblack\n\n\n3\n3\n10621.153846\nblack\n\n\n4\n4\n11261.046512\nblack\n\n\n5\n5\n15723.715415\nblack\n\n\n6\n6\n21571.153846\nblack\n\n\n7\n7\n31575.000000\nblack\n\n\n8\n8\n22543.478261\nblack\n\n\n9\n9\n48452.941176\nblack\n\n\n10\n10\n54806.000000\nblack\n\n\n11\n11\n57418.518519\nblack\n\n\n12\n12\n86836.842105\nblack\n\n\n13\n13\n78950.000000\nblack\n\n\n14\n14\n81916.666667\nblack\n\n\n15\n15\n88825.000000\nblack\n\n\n16\n16\n31150.000000\nblack\n\n\n0\n0\n4372.520651\nwhite\n\n\n1\n1\n6440.395738\nwhite\n\n\n2\n2\n7984.731978\nwhite\n\n\n3\n3\n10682.347236\nwhite\n\n\n4\n4\n13160.115607\nwhite\n\n\n5\n5\n18093.610548\nwhite\n\n\n6\n6\n21461.850649\nwhite\n\n\n7\n7\n28755.897436\nwhite\n\n\n8\n8\n32033.744856\nwhite\n\n\n9\n9\n40892.907801\nwhite\n\n\n10\n10\n45715.315315\nwhite\n\n\n11\n11\n45588.888889\nwhite\n\n\n12\n12\n52650.909091\nwhite\n\n\n13\n13\n72557.142857\nwhite\n\n\n14\n14\n75841.176471\nwhite\n\n\n15\n15\n62800.000000\nwhite\n\n\n16\n16\n38500.000000\nwhite\n\n\n17\n17\n55750.000000\nwhite\n\n\n\n\n\n\n\n\nI made two, side-by-side graphs to show average cost by risk percentile and number of chronic illnesses just like the authors did.\n\nf, axs = plt.subplots(1,2, figsize=(9,5), sharey=True)\nsns.scatterplot(data = combined_data_risk,x= 'risk_perc', y='cost_t', hue='race', ax=axs[0], palette='Set2')\naxs[0].set_xlabel('Risk Percentile')\naxs[0].set_ylabel('Mean Total Cost')\nplt.yscale('log')\n\nsns.scatterplot(data=combined_data_chronic,x='gagne_sum_t', y='cost_t', hue='race', ax=axs[1], palette='Set2')\naxs[1].set_xlabel('Mean Number of Chronic Illnesses')\naxs[1].set_ylabel('')\nplt.yscale('log')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\nThis figure shows how mean cost increases with the assigned risk percentile and number of chronic illnesses."
  },
  {
    "objectID": "posts/hospital/ReplicationBiasPost.html#discussion",
    "href": "posts/hospital/ReplicationBiasPost.html#discussion",
    "title": "Replication Study - Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "DISCUSSION",
    "text": "DISCUSSION\nThe results of these findings, as well as the paper from Obermeyer et. al. (2019) which this post reproduces, indicate that there is a clear statistical bias in using this model to predict high risk patients. Most obviously, the model violates the fairness measure of error rate parity, as defined by Barocas, Hardt, and Narayanan (2023). Error rate parity is a principle of statistical fairness that aims to ensure fairness in predictive models by requiring that the error rates (i.e. false positive rates and false negative rates) are balanced across different demographic groups. Our model, however, assigns Black patients lower risk scores compared to white patients with the same number of chronic illnesses. This indicates a discrepancy in how errors are distributed between these groups. If the model assigns lower risk scores to Black patients despite having identical medical histories to white patients, it suggests that the model is more likely to make errors, in this context, underestimate risk, for Black patients compared to white patients. This discrepancy in error rates undermines the fairness of the model, as it means that Black patients may not receive appropriate healthcare resources or interventions due to the inaccuracies in the risk assessment. In their paper, Oberymeyer er. al. explore methodologies to account for this potential bias, and find some success is either excluding race as a feature, decorrelating other features from the race variable, or basing predictions more heavily on a combination of health history and patient spending."
  },
  {
    "objectID": "posts/penguins/penguinblog.html",
    "href": "posts/penguins/penguinblog.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post is to identify a machine learning model that can classify penguins as one of three species, Gentoo, Chinstrap, and Adelie, with 100% accuracy. I explore five different algorithms of classification: logistic regression, decision trees, random forests, support vector machine, and k-nearest neighbors. I use cross validation and grid-search cross validation to find the most accurate measurement of each model’s effectiveness, and to avoid overfitting. I then compared the results of all five modelsto identify the most accurate model overall, which was a logistic regression model trained on either island, culmen length, culmen depth, or sex, culmen length, and culment depth. In the end I chose to train my final model on island, culmen length, and culmen depth, due to concerns about the real-world value of sex as a predictor for penguins."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#abstract",
    "href": "posts/penguins/penguinblog.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post is to identify a machine learning model that can classify penguins as one of three species, Gentoo, Chinstrap, and Adelie, with 100% accuracy. I explore five different algorithms of classification: logistic regression, decision trees, random forests, support vector machine, and k-nearest neighbors. I use cross validation and grid-search cross validation to find the most accurate measurement of each model’s effectiveness, and to avoid overfitting. I then compared the results of all five modelsto identify the most accurate model overall, which was a logistic regression model trained on either island, culmen length, culmen depth, or sex, culmen length, and culment depth. In the end I chose to train my final model on island, culmen length, and culmen depth, due to concerns about the real-world value of sex as a predictor for penguins."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#data-preperation",
    "href": "posts/penguins/penguinblog.html#data-preperation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preperation",
    "text": "Data Preperation\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n# changing species name for plotting ease\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)"
  },
  {
    "objectID": "posts/penguins/penguinblog.html#data-exploration",
    "href": "posts/penguins/penguinblog.html#data-exploration",
    "title": "Classifying Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n1. SCATTER PLOT – Delta 15 N and Delta 13 C\nFor my first graph, I chose to explore how delta 15 N values and delta 13 C values vary according to species.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"darkgrid\")\nsns.scatterplot(data=train, x='Delta 15 N (o/oo)', y='Delta 13 C (o/oo)', hue='Species', palette = \"magma\")\nplt.title(\"Delta 15 N and Delta 13 C Values by Species\")\nplt.legend(title=\"Species\", loc='upper left')\n\n\n\n\n\n\n\n\nThis graph shows the delta 15 N values and the delta 13 C values by species. Delta 15 N is a measure of the ratio between two stable nitrogen isotopes, nitrogen-15 and nitrogen-14. In biological contexts, this measure is often used to relative food chain position of an organism, with higher a higher value indicating a higher trophic position. Delta 13 C is a measure of the ratio between two stable carbon isotopes, carbon-13 and carbon-12. Delta 13 C is used in biology to analyze the diet of an organism, and can also be used to determine its trophic levels, with a higher delta 13 C value indicating greater productivity. Because chinstrap penguins have on average both a higher delta 15 n and delta 13 c value, this could indicate they exist slightly at a slightly higher trophic level, and consume food in a climate with a distinct inclination towards plants containing one carbon signature over the other. On the other hand adelie penguins seem to inhabit mostly an environment with a more balanced mix of the two in relation to chinstrap penguins, and gentoo penguins seems to be relatively skewed towards the other carbon signature. This would also indicate that adelie penguins are slightly lower in trophic level than chinstrap, and gentoo penguins even more so. For further exploration this raises the question of both relative sizes (body mass) of the penguins, and whether this agrees with the relative trophic levels seen here, and the islands on which the penguins are more likely to exist. For example does one island contain mostly chinstrap penguins and plants of one carbon signature while the other has mostly gentoo penguins and plants containing more of the other carbon signature.\n\n\n2. VIOLIN PLOT: BODY MASS BY SPECIES\nFor my second graph I wanted to explore the body mass distribution within the different species.\n\nsns.violinplot(data = train, x=\"Species\", y=\"Body Mass (g)\", palette = \"viridis\")\nplt.title(\"Body Mass by Species\")\nsns.pointplot(x=\"Species\", y=\"Body Mass (g)\", data=train, estimator='mean', color='red', markers='o', linestyles='')\n\n\n\n\n\n\n\n\nThis plot examines the body masses, and their relative frequency, within the three penguin species. The chinstrap figure shows that most chinstrap penguins fall within the interquartile range, and are very likely to exist near the median of the overall species body mass (shown in red). This contrasts somewhat with the adelie and gentoo penguins, who are not quite as densely centered around the median body mass of their species. This could be helpful in identifying chinstrap penguins in particular, as this tells us that a penguin with a body mass largely different from the chinstrap median is much less likely to be a chinstrap than a gentoo or an adelie. This plot is also interesting in terms of the derived trophic levels from the first figure, as it does not intuitively agree with the relative levels shown there. This could tell us that delta 15 N and delta 13 C might be less valuale predictors than we thought, as the patterns shown using these predictors are not necessarily reflected in other predictors.\n\n\n3. SUMMARY TABLE: SPECIES BY ISLAND\n\npd.crosstab(train['Species'], train['Island'])\n\n\n\n\n\n\n\n\nIsland\nBiscoe\nDream\nTorgersen\n\n\nSpecies\n\n\n\n\n\n\n\nAdelie\n33\n45\n42\n\n\nChinstrap\n0\n57\n0\n\n\nGentoo\n98\n0\n0\n\n\n\n\n\n\n\n\nThis table shows the relative counts of each of the three penguin species on the three islands surveyed. This table agrees with the patterns in delta 13 C values we saw in the first plot, as chinstrap and gentoo are found exclusively on Dream and Biscoe island respectively, which would confirm the hypothesis that the delta 13 C values vary according to a difference in the available plants carbon signatures. Adelie penguins, on the other hand, are found at all three islands, but also are the only penguins on Torgersen island, speaking to their relatively large variation in delta 13 C values."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#creating-the-model",
    "href": "posts/penguins/penguinblog.html#creating-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Creating the Model",
    "text": "Creating the Model\nFirst I created a dataframe that would later store the best, cross-validated values for each of the model types.\n\n# creating dataframe to keep track of the max accuracies/best predictors for each algorithm\nbest_tracker = pd.DataFrame(columns = ['Logistic_Regression', 'Decision_Tree', 'Random_Forest', 'SVC', 'KNN'])\nbest_tracker.head()\n\n\n\n\n\n\n\n\n\nLogistic_Regression\nDecision_Tree\nRandom_Forest\nSVC\nKNN\n\n\n\n\n\n\n\n\n\n\nThen, using Professor Chodrow’s code from the assignment description that generates every possible combination of two quantitative features and one qualitative feature, I created a loop that ran through the process of both fitting and cross-validating each model individually and storing the maximum result (in terms of accuracy and tuning parameters) to the previously created dataframe. For models with tuning parameters I used GridSearchCV to test many possible options for these parameters before finding the best one.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC \nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage_Adult, 1 Egg Stage\"] # qualitative columns\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)'] # quantitative columns\n\n# initialize all the trackers of the best model \nbest_tree_score = 0\nbest_svc_score = 0\nbest_lr_score = 0\nbest_forest_score = 0\nbest_knn_score = 0\n\nbest_tree_depth = None\nbest_forest_depth = None\nbest_gamma = None\nbest_knn_neigh = None\n\nbest_tree_cols = []\nbest_svc_cols = []\nbest_lr_cols = []\nbest_forest_cols = []\nbest_knn_cols = []\n\n#######################\n\n# for each combination of 1 qualitative predictor and 2 quantitative predictors\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # LOGISTIC REGRESSION\n    LR = LogisticRegression(max_iter=200000000000000) # was running into max. iterations reached error\n    LR.fit(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv=5) \n    curr_score = cv_scores_LR.mean() # mean of the cross valid. scores is considered the best score we can get w/ the model\n    \n    # update vars if necessary\n    if curr_score &gt; best_lr_score:\n      best_lr_score = curr_score\n      best_lr_cols = cols\n    \n    # DECISION TREE\n    param_grid = {\n    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None] \n    } # None means no maximum depth, try a max. depth. Testing all depths from 1 to 10 (the total number of features) and no max. depth.\n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    curr_score = grid_search.best_score_ # mean of the cross valid. scores is considered the best score we can get w/ the model\n\n    # update vars if necessary\n    if curr_score &gt; best_tree_score:\n      best_tree_score = curr_score\n      best_tree_cols = cols\n      best_tree_depth = grid_search.best_params_\n\n    # RANDOM FOREST\n    param_grid = {\n    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n    } # same idea for max. depth as the decision tree\n    forest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forest, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    curr_score = grid_search.best_score_ # mean of the cross valid. scores is considered the best score we can get w/ the model\n\n    # update vars if necessary\n    if curr_score &gt; best_forest_score:\n      best_forest_score = curr_score\n      best_forest_cols = cols\n      best_forest_depth = grid_search.best_params_\n\n    # SVC\n    g = {'gamma': 10.0**np.arange(-5, 5)} # testing all gammas in the range given in the blog post instructions\n    grid_search = GridSearchCV(SVC(),g, cv = 5)\n    grid_search.fit(X_train[cols],y_train)\n    curr_score = grid_search.best_score_ # mean of the cross valid. scores is considered the best score we can get w/ the model\n\n    # update vars if necessary\n    if curr_score &gt; best_svc_score:\n      best_svc_score = curr_score\n      best_svc_cols = cols\n      best_gamma = grid_search.best_params_\n\n    # K-NEAREST NEIGHBORS\n    param_grid = {\n    'n_neighbors': [5, 6, 7, 8, 9, 10]  \n    }  # testing number of nearest neighbors from 5 to 10, didn't want groups to be too specialized or too generalized\n    KNN = KNeighborsClassifier(n_neighbors=5)\n    grid = GridSearchCV(KNN, param_grid, cv=5)\n    grid.fit(X_train[cols], y_train)\n    curr_score = grid.best_score_ # mean of the cross valid. scores is considered the best score we can get w/ the model\n\n    # update vars if necessary\n    if curr_score &gt; best_knn_score:\n      best_knn_score = curr_score\n      best_knn_cols = cols\n      best_knn_neigh = grid.best_params_\n    \n# update dataframe using the tracker variables for each model\nbest_tracker['Decision_Tree'] = [best_tree_score, best_tree_depth, best_tree_cols]\nbest_tracker['Logistic_Regression'] = [best_lr_score, None, best_lr_cols]\nbest_tracker['SVC'] = [best_svc_score, best_gamma, best_svc_cols]\nbest_tracker['Random_Forest'] = [best_forest_score, best_forest_depth, best_forest_cols]\nbest_tracker['KNN'] = [best_knn_score, best_knn_neigh, best_knn_cols]\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nThe final dataframe shows the best results for each model, which shows that logistic regression was the most accurate model, with the parameters sex, culmen length, and culmen depth.\n\n# best results for each of the five models\nbest_tracker.head()\n\n\n\n\n\n\n\n\n\nLogistic_Regression\nDecision_Tree\nRandom_Forest\nSVC\nKNN\n\n\n\n\n0\n0.988311\n0.976546\n0.984465\n0.980543\n0.976621\n\n\n1\nNone\n{'max_depth': 5}\n{'max_depth': 5}\n{'gamma': 0.1}\n{'n_neighbors': 5}\n\n\n2\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Cul...\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Cul...\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Fli...\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Cul...\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Cul..."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#testing-the-model",
    "href": "posts/penguins/penguinblog.html#testing-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Testing the Model",
    "text": "Testing the Model\n\n# read in and prepare test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nX_test, y_test = prepare_data(test)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nRetraining and testing model with selected columns (1)\n\ncols = [\"Sex_FEMALE\", \"Sex_MALE\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\nLR1 = LogisticRegression(max_iter=20000000000000)\nLR1.fit(X_train[cols], y_train)\nLR1.score(X_test[cols], y_test)\n\n0.9852941176470589\n\n\nNOTE: the last time I ran my code the parameter of sex gave me less than a 100% accuracy rate despite previously being 100%, but I moved ahead anyways because I chose island, which I explain below, and the model trained on those features still reached 100% accuracy.\nOut of curiosity, I created a version of the dataframe that saved the best model and parameters for all of the features, which I have removed because of how long the code was and because I manually went through it to confirm a hypothesis. My hypothesis was that sex was not actually one of the best features for prediction, despite the dataframes finding, unless the data was skewed towards one sex or the other for a certain species, as biologically it says nothing about the species of a penguin to know its sex. When I looked through the data, I found that the logistic regression model using the island feature had an identical accuracy to the model fit to sex. This told me that the dataframe chose sex simply because they were tied and it was the latter one run, meaning the more recent model to reach the max accuracy. Scientifically, it makes more sense to train on island, as we’ve seen that certain penguins do inhabit certain islands at different rates, rather than sex which seems more like a quirk of the model. Therefore, I also fit a model to these three parameters and scored the predictions of this model.\n\n# retraining and testing model with selected columns (2)\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nLR = LogisticRegression(max_iter=20000000000000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nI reached 100% accuracy using logisitic regression with the predictions parameters of sex, culmen length, and culmen depth, AND seperately with the prediction parameters of island, culmen length, and culment depth. Based on the context of predicting species, where sex is not a realistically valuable feature, I chose to move on using island instead."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#plotting-decision-regions",
    "href": "posts/penguins/penguinblog.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nBelow is the code I used to graph the decision regions, adapted from Professor Chodrow’s lecture notes.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nThe decision regions are shown split for each island, since they are seperated into three quantitative parameters, and the x-axis is the culmen length while the y-axis is the culmen depth.\n\n# plotting decision boundaries for model trained with island parameter\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(LR, X_train[cols], y_train)"
  },
  {
    "objectID": "posts/penguins/penguinblog.html#confusion-matrix",
    "href": "posts/penguins/penguinblog.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n# creating confusion matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ny_pred = LR.predict(X_test[cols])\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\ndisp.plot(cmap=plt.cm.Greens, colorbar=False)\nplt.title(\"Confusion Matrix for Logistic Classification Model\")\nplt.show()\n\n\n\n\n\n\n\n\nThe confusion matrix shows exactly what we would expect with a 100% accurate model, which is that all examples of each of the three species (31 for Adelie, 11 for Chinstrap, 26 for Gentoo) are correctly identified as their respective species."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#discussion",
    "href": "posts/penguins/penguinblog.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nThe findings of this blogpost include a 100% accurate logistic classification model for determining whether a penguin, based on its sex/island, culmen length, and culmen depth, is an Adelie, Chinstrap, or Gentoo penguin. In order to determine the most accurate model, I examined all combinations of one qualitative and two quantitative features, and five different models: logistic regression, decision trees, random forests, SVC, and K-nearest neighbors. I stored the results of the best version of each model, found using either regular cross-validation (logistic regression), or grid search cross validation (in order to determine the best values for the tuning factors of each model), in a dataframe, and then was able to compare the accuracy across models. Interestingly, all the models chose the columns of sex, culmen depth, and culmen length, as the most valuable predictors for the data. This was concerning to me, as sex is not usually a useful species differentation, and made me wonder whether this meant that our penguin data was skewed towards either male or female for one species of penguin or another. Upon examining the results for each column individually, I found that logistic regression performed equally as accurately when predicting based on island, culmen length, and culment depth. Creating models trained seperately on these two different sets of predictors showed that both were 100% accurate on the testing data. Because of my suspicion about using sex as a predictor of species, I chose to graph the boundaries and confusion matrix only for the model trained on island instead of sex, assuming it would be more generalizable if it were to be applied to different datasets.\nThrough the process of fitting and analyzing the various models I created I learned a lot about the standard machine learning workflow in Python, and became much more comfortable incoorporating new functions and modules that we hadn’t previously discussed in class. For example, I remembered K-nearest neighbors from my statistical learning class, and felt like it would be an effective algorithm for classifying penguin species, and so I sought out whether there was a built-in function that would allow me to implement it as one of the choices in my analysis."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html",
    "href": "posts/logistic/LogisticBlogPost.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Link to my logistic regression implementation."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#abstract",
    "href": "posts/logistic/LogisticBlogPost.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implement logistic regression, a fundamental algorithm of of machine learning, and perform experiments to explore the accuracy and behavior of my implementation. I implement vanilla gradient descent, and a variant of gradient descent known as gradient descent with momentum, aimed at achieving faster convergence. I conduct experiments and create visualizations to validate my implementation, examining the behavior of vanilla gradient descent versus gradient descent with momentum and addressing the phenomenon of overfitting. Through experimentation and analysis, I gained insights into the behavior and performance of logistic regression models under various conditions, as well as best practices when using the algorithm to make predictions.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer"
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#vanilla-gradient-descent",
    "href": "posts/logistic/LogisticBlogPost.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nThe first experiment I performed was implementing vanilla gradient descent using my model. This form of gradient descent involves ignoring beta as a paramater (setting it to 0), and instead only using alpha as a learning parameter. With these parameters and 2-dimensional data I hoped to observe a monotonous decrease in loss over time, eventually, over 1000 iterations, becoming very small, which would show that our model is learning and becoming better at separating our data.\nI used the classification_data function from the blog post description to create an instance of 2-D data for our model.\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n\n# DO NOT RE-RUN\n\nX, y = classification_data(noise = 0.5)\n\nBelow I plotted the data in order to visualize the separation.\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# adapted from perceptron blog post\ndef plot_classification_data(X, y, ax):\n    plt.figure(figsize=(10, 6))\n    targets = [0, 1]\n    markers = [\"o\" , \"X\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \n    \nfig, ax = plt.subplots(1, 1)\n\n# create and plot data\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nNext, I created a logistic regression instance, and ran a 6000 iteration training loop over my generated data. I chose 6000 after experimenting to see when the line began to level out and make slower progress.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\n# store each iterations loss in a vector, update weights each time for 1000 times\nfor _ in range(6000):\n    loss = LR.loss(X, y)\n    loss_vec.append(loss)\n    opt.step(X, y, a = 0.6, b=0)\n\n\n# taken from the perceptron blog post\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\nfig, ax = plt.subplots(1, 1)\n\n\n# plot data\nplot_classification_data(X, y, ax)\n\n# plot best dividing line\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2));\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nAs we can see, the line our model found does a very good job of estimating the divide between our two data groups. Our final loss was around 0.003. Next, I plotted the change in our loss over time using our loss vector.\n\n# taken from perceptron blog post, plots loss over iterations\ndef plot_loss(lvec):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(lvec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(lvec)), lvec, color = \"slategrey\", s=5)\n    plt.gca().set(xlabel = \"Logistic Regression Steps\", ylabel = \"Loss\")\n    \n    plt.title(f\"Final loss: {lvec[len(lvec)-1]:.3f}\")\n\nplot_loss(loss_vec);\n\n\n\n\n\n\n\n\nWe see that the loss changes as we would hope, in that it does in fact decrease with our iterations, and starts to converge at low numbers as our iterations increase."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#benefits-of-momentum",
    "href": "posts/logistic/LogisticBlogPost.html#benefits-of-momentum",
    "title": "Implementing Logistic Regression",
    "section": "Benefits of Momentum",
    "text": "Benefits of Momentum\nThe next experiment I performed was to use a non-zero beta value to explore momentum. What we expect to see is our model converging faster (in fewer iterations) to the same or smaller loss values. I chose 3000 iterations as I wanted to see how few iterations I could run the model for in order to acheive the same result as our first model.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec_m = []\n\n# a = 0.06, b = 0.05\n# took some experimentation because values too high caused loss to become so small it wouldn't format as a number\nfor _ in range(3000):\n    loss = LR.loss(X, y)\n    loss_vec_m.append(loss)\n    opt.step(X, y, a = 0.6, b=0.5)\n\n# plot data and loss\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2));\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nWe see here that we do in fact end with the same loss value in 3000 iterations that our training without momentum found in 6000 iterations. Next, I plotted the loss over iterations of the model with momentum over the loss over iterations of our vanilla model to see whether the loss decreased faster than the vanilla gradient descent loss.\n\nimport numpy as np\n\nplt.figure(figsize=(10,6))\n\n# plotting vanilla\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\", s=5)\nplt.gca().set(xlabel = \"Logistic Regression Steps\", ylabel = \"Loss\")\n\n# plotting with momentum\nplt.plot(loss_vec_m, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_m)), loss_vec_m, color = \"red\", s=5)\nplt.gca().set(xlabel = \"Logistic Regression Steps\", ylabel = \"Loss\");\n\n# plotting points where y = 0.0035\nplt.scatter(5778, loss_vec[5777].item(), color=\"blue\", label=f'Vanilla: ({5778}, {0.0035})', s=15, zorder=3)\nplt.scatter(2884, loss_vec[2883].item(), color=\"orange\", label=f'Momentum: ({2884}, {0.0035})', s=15, zorder=3)\n\nplt.legend()\n\n\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, the red line shows the model trained with momentum, and the grey line shows the vanilla model. It’s clear that the red line is moving towards convergence at or around zero faster than the grey line. As we saw in the model training, both models were able to reach a final (rounded) loss of 0.003 in the given iterations. In order to visualize the difference in speeds between the two models, I plotted where each model first reached a value \\(\\approx\\) 0.0035 (my cut off was anything less than 0.0036, which would then feasibly be rounded down to 0.003), these points are shown in orange and blue. We can see that the red line, the orange dot, reached 0.0035 in 2,884 iterations, while the grey line (the blue dot) took almost double the iterations to reach the same loss, 5,778. I decided to stop the momentum line in fewer iterations than the vanilla model also partly because of visualizations issues, as the lines start to look indistinguishable at very small values, but I found that 3000 iterations was enough to show the steeper initial slope of the momentum model, and to show the earlier appearance of the value our vanilla model settled on in a larger number of iterations."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#overfitting",
    "href": "posts/logistic/LogisticBlogPost.html#overfitting",
    "title": "Implementing Logistic Regression",
    "section": "Overfitting",
    "text": "Overfitting\nThe next experiment I performed was to examine the effects of overfitting our model on training and testing data. I created two sets of data, one training and one testing, both with 100 dimensions (features) and 50 data points. This creates a dataset with many more features than actual instances, which is very susceptible to overfitting.\n\nX_train,y_train = classification_data(n_points = 50, noise = 0.2, p_dims = 100)\nX_test,y_test = classification_data(n_points = 50, noise = 0.2, p_dims = 100)\n\nI trained the model on our training data with varying alpha, beta values until I found a training accuracy of 100%. I looped my training process untill I reached a loss of at most 0.005, which to me was close enough to zero to be considered fully trained.\n\nfrom sklearn.metrics import accuracy_score\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\nloss = 1.0\n\n# train model\nwhile loss &gt; 0.005:\n    loss = LR.loss(X_train, y_train)\n    loss_vec.append(loss)\n    opt.step(X_train, y_train, a = 0.06, b=0.9)\n\n# generate predictions\ny_train_pred = LR.predict(X_train)\n# calculate training accuracy using scikitlearn function\ntrain_acc = accuracy_score( y_train.numpy(), y_train_pred.numpy())\nprint(train_acc)\n\n1.0\n\n\nTo visualize the complete training of our model, I also plotted the loss over iterations.\n\nimport math\ncleaned_list = [x for x in loss_vec if not math.isnan(x)]\nplot_loss(cleaned_list)\n\n\n\n\n\n\n\n\nThen I used the trained model to generate predictions and accuracy for our testing data.\n\n# generate predictions\ny_test_pred = LR.predict(X_test)\n# calculate testing accuracy using scikitlearn function\naccuracy_test = accuracy_score(y_test.numpy(), y_test_pred.numpy())\nprint(accuracy_test)\n\n0.96\n\n\nAs we can see, despite acheiving 100% accuracy on our training data, we can only acheive 96% accuracy on our testing data, speaking to the drawbacks of overfitting a model to training instances."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#discussion",
    "href": "posts/logistic/LogisticBlogPost.html#discussion",
    "title": "Implementing Logistic Regression",
    "section": "Discussion",
    "text": "Discussion\nThese experiments on my implementation of logistic regression revealed intriguing insights into the behavior and performance of logistic regression models trained using gradient descent with/without momentum, and into overfitting with logistic regression. Firstly, my findings demonstrated that gradient descent with momentum outperforms vanilla gradient descent in terms of convergence speed. By incorporating momentum, the optimization process exhibits faster convergence towards an optimal solution (smaller loss). This observation underscores the effectiveness of momentum as a tool for logistic regression models. The experiments also shed light on the phenomenon of overfitting in logistic regression models. When the number of dimensions exceeds the number of data points, logistic regression models are susceptible to overfitting, as demonstrated by the attainment of 100% accuracy on the training data while achieving less optimal performance on testing data. This highlights the importance of model evaluation strategies to mitigate the risk of overfitting and ensure the generalization ability of the trained model. Overall, by understanding the nuances of optimization techniques and model behavior, I feel more able to make informed decisions that enhance the performance and robustness of logistic regression models for various classification tasks."
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html",
    "href": "posts/newtons/newtonblogpost.html",
    "title": "Implementing Newton’s Optimizer",
    "section": "",
    "text": "First I read in my logistic regression with newton’s optimizer .py file.\n%load_ext autoreload\n%autoreload 2\nfrom newton import LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html#abstract",
    "href": "posts/newtons/newtonblogpost.html#abstract",
    "title": "Implementing Newton’s Optimizer",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implement Newton’s Method for Logistic Regression, building on my previous implementation of logistic regression. Newton’s Method, a second-order optimization technique, requires knowledge of both the first and second derivatives of the loss function. I first implementation a NewtonOptimizer class, extending my logistic regression model with a new method to compute the Hessian matrix and perform the optimization step for logistic regression using this new factor. Through experiments and visualizations, I explore the effectiveness of Newton’s method in converging to the correct solution and its potential for faster convergence compared to standard gradient descent under certain conditions. Additionally, I calculate the computational costs associated with Newton’s method and gradient descent, providing insights into the trade-offs between the two methods."
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html#b.-experiments",
    "href": "posts/newtons/newtonblogpost.html#b.-experiments",
    "title": "Implementing Newton’s Optimizer",
    "section": "B. Experiments",
    "text": "B. Experiments\n\n1. Testing Convergence Behavior of Newtown Optimizer\nI used the classification data generation function from the logistic and perceptron blog posts to create linearly seperable data.\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nAfter generating my data, I plotted it to show what the class seperation looks like. Again, this code is taken from the logistic regression and perceptron blog posts.\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# adapted from logistic & perceptron blog post\ndef plot_classification_data(X, y, ax):\n    plt.figure(figsize=(10, 6))\n    targets = [0, 1]\n    markers = [\"o\" , \"X\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \n    \nfig, ax = plt.subplots(1, 1)\n\n# create and plot data\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nWe can see the data is clearly linearly seperable. Next, I defined a few functions, again from the previous blog posts, to help me visualize the progress of me model through its optimization steps\n\n# helpful functions from previous blog posts\n\n# draws found seperating line on graph\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# finds final model accuracy on data\ndef find_accuracy(X, y):\n    predictions = LR.predict(X)\n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\n# plots the generated loss vectors\ndef plot_loss(lvec):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(lvec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(lvec)), lvec, color = \"slategrey\", s=5)\n    plt.gca().set(xlabel = \"Logistic Regression Steps\", ylabel = \"Loss\")\n    \n    plt.title(f\"Final loss: {lvec[len(lvec)-1]:.3f}\")\n\nFirst, I wanted to observe whether my model implemented with the newton optimizer would converge regularly on seperable data.\n\n# instantiate a model\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\n# instantiate a loss vector\nloss_vec = []\n\n# for 6000 iterations, track the progress of the loss of our model\nfor index in range(6000):\n    \n    opt.step(X, y, a = 0.5)\n    \n    loss = LR.loss(X, y).item()\n    loss_vec.append(loss)\n\n# find our final accuracy\nfind_accuracy(X, y)\n\n# plot the final classification line against our data\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\nAccuracy: 1.0\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nFrom this we see that our model does in fact converge to the correct seperating line for our data, and ends with a loss of 0.00 and an accuracy of 100%.\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nAgain, here we can see our loss converging to zero as expected across our 6000 iterations.\n\n\n2. Comparing Newtown’s Optimizer to Standard Gradient Descent\nIn order to compare the performance of my Newtown’s optimizer implementation to my previously implemented standard gradient descent optimizer, I ran both optimizers for 8000 iterations, tracking the loss at each step, and then graphed them side-by-side to observe the difference. My goal was to observe that in at least some cases Newtown’s optimizer converges faster than gradient descent.\n\n# Initialize logistic regression model and Newton optimizer\nLR = LogisticRegression()\n\nopt_newton = NewtonOptimizer(LR)\nopt_gradient_descent = GradientDescentOptimizer(LR)\n\n# Lists to store losses for each iteration\nloss_vec_newton = []\nloss_vec_gradient_descent = []\nloss_gradient_descent = float('inf')\nloss_newton = float('inf')\n\n# Run Newton's method for 8000 iterations\nfor _ in range(8000):\n    loss_newton = LR.loss(X, y).item()\n    opt_newton.step(X, y, a=4)\n    loss_vec_newton.append(loss_newton)\n\n# Re-initialize weights for gradient descent\nLR.w = None\nLR.pastw = None\n\n# Run standard gradient descent for 8000 iterations\nfor _ in range(8000):\n    loss_gradient_descent = opt_gradient_descent.step(X, y, a=0.1, b=0)\n    loss_vec_gradient_descent.append(loss_gradient_descent)\n\n# Print final loss values\nprint(\"Final Loss (Newton's Method):\", loss_newton)\nprint(\"Final Loss (Gradient Descent):\", loss_gradient_descent)\n\n# Plot the convergence of both methods\nplt.figure(figsize=(10, 6))\nplt.plot(loss_vec_newton, label=\"Newton's Method\")\nplt.plot(loss_vec_gradient_descent, label=\"Gradient Descent\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Convergence Comparison: Newton's Method vs. Gradient Descent\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nFinal Loss (Newton's Method): 0.0\nFinal Loss (Gradient Descent): tensor(0.0081)\n\n\n\n\n\n\n\n\n\nFrom this graph we see that Newtown’s optimizer does in fact converge faster than gradient descent with these respective alphas on this linearly seperable data.\n\n\n3. When Alpha Is Too Large, Newtown’s Should Not Converge\nIn order to test this, I ran my loss vector creation loop over a small number of iterations for a very large alpha (745). At very large numbers I had an error, but still being able to graph the vector we can see it’s trend towards non-convergence.\n\nX, y = classification_data(n_points=300, noise = 0.5)\n\n# Initialize logistic regression model and Newton optimizer\nLR = LogisticRegression()\nopt_newton = NewtonOptimizer(LR)\n\n# Lists to store losses for each iteration\nloss_vec_newton = []\n\n# Run Newton's method for 10 iterations\nfor _ in range(10):\n    loss_newton = LR.loss(X, y).item()\n    opt_newton.step(X, y, a=745)\n    loss_vec_newton.append(loss_newton)\n\n\n# Print final loss values\nprint(\"Final Loss (Newton's Method):\", loss_vec_newton[-1])\n\nplot_loss(loss_vec_newton)\n\nFinal Loss (Newton's Method): 8.40453815460205\n\n\n\n\n\n\n\n\n\nFrom this we can see that our model is not starting to converge in the way we saw it do with smaller alpha values."
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html#c.-operation-counting",
    "href": "posts/newtons/newtonblogpost.html#c.-operation-counting",
    "title": "Implementing Newton’s Optimizer",
    "section": "C. Operation Counting",
    "text": "C. Operation Counting\nWorking with the following assumptions on computing costs:\nLoss, \\(L\\) = \\(c\\)\nGradient, \\(G\\) = \\(2c\\)\nHessian, \\(H\\) = \\(pc\\)\nInverting a \\(p\\times p\\) matrix = \\(k_1p^\\gamma\\)\nMultiplying by \\(H\\) = \\(k_2p^{2}\\)\nFinding the total computational costs of Newtown’s method.\nAssuming: Newtown’s method converges to an adequate solution in \\(t_\\mathrm{nm}\\) steps.\n\\(1.\\) First, we calculate the computing cost of a single step: - Computing \\(L\\) = \\(c\\) - For as many steps as it takes to converge, i.e. \\(t_\\mathrm{nm}\\) steps - Computing \\(G\\) = \\(2c\\) - Computing \\(H\\) = \\(pc\\) - Inverting hessian matrix = \\(k_1p^\\gamma\\) - Multiplying by \\(H\\) = \\(k_2p^{2}\\)\n\\(2.\\) Adding up all of the steps together, we get the total computational cost for our Newtown’s optimizer, which is: - \\(O(c + t_\\mathrm{nm} \\times (2c + pc + k_1p^\\gamma + k_2p^{2}))\\)\nFinding the total computational costs of standard gradient descent.\nAssuming: standard gradient descent converges to an adequate solution in \\(t_\\mathrm{gd}\\) steps.\n\\(1.\\) First, we calculate the computing cost of a single step: - Computing \\(L\\) = \\(c\\) - For as many steps as it takes to converge, i.e. \\(t_\\mathrm{gd}\\) steps - Computing \\(G\\) = \\(2c\\)\n\\(2.\\) Adding up all of the steps together, we get the total computational cost for our Newtown’s optimizer, which is: - \\(O(c + t_\\mathrm{gd} \\times (2c))\\)\nLooking at these two equations, we have a general sense that Newtown’s method is more computationally costly. However, to compare which is less computationally expensive mathematically, we can create a ratio between the two:\n\n\\(\\frac{t_\\mathrm{nm}}{t_\\mathrm{gd}} = \\frac{c + t_\\mathrm{nm} \\times (2c + pc + k_1p^\\gamma + k_2p^{2})}{c + t_\\mathrm{gd} \\times (2c)}\\)\n\nThe numerator of the second fraction, which represents the computational cost of Newton’s optimizer, contains terms that grow polynomially with the values of \\(p\\) and \\(\\gamma\\). This polynomial growth in the numerator’s terms indicates that the computational cost of Newton’s optimizer will increase significantly compared to the denominator as the values of \\(p\\) and \\(\\gamma\\) increase. This suggests that as the problem complexity or the size of the optimization space increases, the computational cost associated with Newton’s optimizer will grow polynomially relative to the computational cost associated with the denominator, the computational cost of standard gradient descent, as it does not depend on either \\(p\\) or \\(\\gamma\\). Therefore, for large or complex optimization problems, the computational resources required by Newton’s optimizer may become an obstacle that renders the method impossible to implement."
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html#discussion",
    "href": "posts/newtons/newtonblogpost.html#discussion",
    "title": "Implementing Newton’s Optimizer",
    "section": "Discussion",
    "text": "Discussion\nThrough completing this blog post, I’ve gained a deeper understanding of the inner workings of Newton’s method for optimization in terms of it’s mathematical process and its potential benefits and limitations in regression tasks. Understanding the pros in terms of faster convergences, but the cons in terms of greater computational expense and failure at large alphas reinforced for me the importance of selecting the right optimization algorithm for a task. By carefully examining computational costs and performance considerations, I was able to practice the computations required for selecting an optimization algorithm. The higher time complexity of Newton’s method, particularly when dealing with large feature spaces, again emphasized the need for thoughtful consideration in choosing optimization strategies. Overall, this post has enhanced my technical skills and broadened my perspective on the intricacies of optimization techniques in machine learning."
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html",
    "href": "posts/perceptron/PerceptronBlogPost.html",
    "title": "Implementing the Perceptron",
    "section": "",
    "text": "In this blog post, I explore the implementation and experimentation of the perceptron algorithm. Beginning with an implementation of the algorithm itself in the file perceptron.py, I then explore its functionality and effectiveness through various experiments. Through code implementation and visualizations, I demonstrate the algorithm’s behavior on both linearly separable and non-linearly separable data in 2D, as well as its extension to greater dimensions. Additionally, I explore the mini-batch perceptron algorithm and its impact on convergence and performance. Throughout this blog post I learned a lot about the advantages and disadvantages of the perceptron algorithm on various kinds of data.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#abstract",
    "href": "posts/perceptron/PerceptronBlogPost.html#abstract",
    "title": "Implementing the Perceptron",
    "section": "",
    "text": "In this blog post, I explore the implementation and experimentation of the perceptron algorithm. Beginning with an implementation of the algorithm itself in the file perceptron.py, I then explore its functionality and effectiveness through various experiments. Through code implementation and visualizations, I demonstrate the algorithm’s behavior on both linearly separable and non-linearly separable data in 2D, as well as its extension to greater dimensions. Additionally, I explore the mini-batch perceptron algorithm and its impact on convergence and performance. Throughout this blog post I learned a lot about the advantages and disadvantages of the perceptron algorithm on various kinds of data.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#linearly-separable-experiments",
    "href": "posts/perceptron/PerceptronBlogPost.html#linearly-separable-experiments",
    "title": "Implementing the Perceptron",
    "section": "Linearly Separable Experiments",
    "text": "Linearly Separable Experiments\nMy first step, after implementing the perceptron class, was to test my work on known linearly seperable data. I used the code from lecture 7 to generate linearly seperable data and a visualization of said data.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\n# function which creates linearly seperable data\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# creating data instance\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n# function that plots perceptron data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThis graph shows a clear distinction between labels in the data that can be easily divided by a single line.\n\n# My classmate Lindsey Schweitzer's idea to create a plot_loss function for easier plotting\ndef plot_loss(loss):\n\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n\n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    labs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\nI then ran an update cycle on the data, continuing to update the weights until the total loss reached 0.0 (since I knew the data would eventually reach this point).\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThis did indeed find that the data was linearly separable, and that my perceptron class and implementation was able to successfully find that separating line.\n\nVisualizing Perceptron’s Process\nI adapted code, again from lecture 7 to visualize a few specific steps of the perceptron algorithm, showing how the line changes as the weights are updated until it eventually finds where loss is equal to zero.\n\n# function from lec 7 to add line to plot\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # find i, random data point\n    i = torch.randint(n, size=(1,))\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(X[[i],:], y[i])\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#non-linearly-separable-experiments",
    "href": "posts/perceptron/PerceptronBlogPost.html#non-linearly-separable-experiments",
    "title": "Implementing the Perceptron",
    "section": "Non-Linearly Separable Experiments",
    "text": "Non-Linearly Separable Experiments\nNext, I tested my perceptron against data that I know to be non-linearly separable to see how it would perform. For this purpose, I created a function that creates data that cannot be separated by a single line.\n\n# creates non-linearly separable data\ndef perceptron_data_nonlinear(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    # same as the linearly seperable data\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # add random noise to y\n    y = y ^ (torch.randint(0, 2, size=(n_points,)).bool())\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data_nonlinear()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nClearly, looking at this data, there is no easy way to classify points using a perceptron-generated line. Next, I used my perceptron update algorithm on an instance of this data, to see how the loss would change over the iterations. In order to prevent looping forever, as the loss will never reach 0.0, I instead limited the algorithm to 1000 iterations.\n\nX, y = perceptron_data_nonlinear()\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\nweights = []\n\nn = X.size()[0]\n\nmaxit = 1000 # max iterations\n\nwhile maxit &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    weights.append(torch.clone(p.w))\n\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    maxit-=1\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThe algorithm is obviously struggling with this unseparable data, and isn’t able to land anywhere close to a 0.0 loss like it did for our previous data. After 1000 iterations it found a final loss of 0.47.\n\nVisualizing Perceptron’s Last Move\nLike for the linearly separable perceptron, I wanted to visualize the line my perceptron had drawn on the data. In this case, since there was build-up in terms of the line approaching a better separation as in the linear data, I just visualized the very last loss found.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X, y, ax)\ndraw_line(weights[-2], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n# draw_line(w, -10, 10, ax, color = \"black\")\nax.set_title(f\"loss = {loss_vec[-1]:.3f}\")\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplt.tight_layout()"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#data-with-5-dimensions",
    "href": "posts/perceptron/PerceptronBlogPost.html#data-with-5-dimensions",
    "title": "Implementing the Perceptron",
    "section": "Data with 5 Dimensions",
    "text": "Data with 5 Dimensions\nTo test whether the data is linearly seperable in the case of higher dimensions, I set the dimension to 5 for the creation of the data (using the regular, theoretically linearly-separable function), and ran the same perceptron algorithm and visualization process. I again used an iteration max instead of a loop until loss = 0, as I wasn’t sure whether or not the data would end up being linearly separable.\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims=5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nmaxit = 1000\n\nwhile maxit &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    maxit-=1\n    \nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nInterestingly, I found that the data in 5-dimensions was in fact linearly seperable within 1000 perceptron iterations. I noticed though, that upon increasing the noise in the data to 0.5 and above, the data become non-linearly seperable. The visualization of that noisier data is below.\n\nX, y = perceptron_data(n_points = 300, noise = 0.5, p_dims=5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nmaxit = 1000\n\nwhile maxit &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    maxit-=1\n    \nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nWith noise set to 0.5, the algorithm could only reach a final loss of 0.053."
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#mini-batch-implementation",
    "href": "posts/perceptron/PerceptronBlogPost.html#mini-batch-implementation",
    "title": "Implementing the Perceptron",
    "section": "Mini-Batch Implementation",
    "text": "Mini-Batch Implementation\nNext, I implemented the mini-batch perceptron algorithm, which computes a perceptron update using k points at once, rather than a single point. It also involves a hyper-parameter \\(\\alpha\\), which is the learning rate, and determines how quickly the weight vectors change over iterations.\n\nfrom perceptron_minibatch import PerceptronMB, PerceptronMBOptimizer\n\n\nK = 1 for Linearly and Non-Linearly Separable Data\nMy first experiment was performing the algorithm for k = 1, so essentially recreating a regular update step with a single random point, for both linearly and non-linearly separable data.\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims=2)\n\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n\n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nI found that the mini-batch algorithm was able to converge with linearly separable data much like the regular perceptron algorithm.\n\nX, y = perceptron_data_nonlinear(n_points = 300, noise = 0.2, p_dims=2)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\nweights = []\n\nn = X.size()[0]\n\nmaxit = 1000 # max iterations\n\nwhile maxit &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    weights.append(torch.clone(p.w))\n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    maxit-=1\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nFor non-linear data, I, again just as in the regular perceptron update, found that the algorithm wasn’t able to converge to loss equals zero.\n\n\nK = 10 for Linearly and Non-Linearly Separable Data\nNext, I tested the algorithm when it chose 10 random points in its update step, again for both linearly and non-linearly separable data.\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims=2)\n\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n\n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThe algorithm was able to find loss = 0.0 for linearly separable data as expected.\n\nX, y = perceptron_data_nonlinear(n_points = 300, noise = 0.2, p_dims=2)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\nweights = []\n\nn = X.size()[0]\n\nmaxit = 1000 # max iterations\n\nwhile maxit &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    weights.append(torch.clone(p.w))\n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    maxit-=1\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nAlso as expected, the algorithm was not able to find loss = 0.0 for non-linearly separable data.\n\n\nK = N for Non-Linearly Separable Data\nLastly, I checked whether, with k = n, in other words when the entire dataset is considered in a single update, the algorithm was able to converge even in the case of non-linearly separable data.\n\ntorch.manual_seed(1234567)\n\nX, y = perceptron_data_nonlinear()\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    # K is n\n    k = n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nInterestingly, I found that the algorithm was able to converge to close to zero (0.010) even with non-linearly separable data using k = n. I did have to adjust the learning rate to be very small (0.0001), in order for this to be possible."
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#perceptron-algorithm-runtime-analysis",
    "href": "posts/perceptron/PerceptronBlogPost.html#perceptron-algorithm-runtime-analysis",
    "title": "Implementing the Perceptron",
    "section": "Perceptron Algorithm Runtime Analysis",
    "text": "Perceptron Algorithm Runtime Analysis\nA single iteration of the perceptron algorithm involved taking the dot product of w, the weight vector, and any row of the feature matrix. This computation depends on the number of elements in each row of the feature matrix, i.e. the dimension of the feature matrix, p. A single iteration of the perceptron algorithm therefore takes \\(O(p)\\).\nA single iteration of the mini-batch perceptron algorithm involved taking the dot product of k points instead of just one. Therefore the runtime of a single iteration of the mini-batch algorithm is \\(O(k \\cdot p)\\)"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#discussion",
    "href": "posts/perceptron/PerceptronBlogPost.html#discussion",
    "title": "Implementing the Perceptron",
    "section": "Discussion",
    "text": "Discussion\nThrough this blog post, I gained valuable insights into strengths and limiations of perceptron algorithm. I successfully implemented the perceptron algorithm, both the regular and mini-batch versions. I performed experiments to validate the accuracy of my implementation, allowing me to grasp the nuances of perceptron updates and its behavior across diverse data types, including higher dimensions and non-linearly separable data. By exploring the mini-batch approach, I demonstrated its functionality akin to a single-point process with k = 1, its effectiveness in identifying separating lines in linearly separable data with k=10, and its ability to converge even in the absence of linear separability when k=n."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Newton’s Optimizer\n\n\n\n\n\nImplementing and testing the newton’s optimizer method for logistic regression.\n\n\n\n\n\nMay 1, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing and testing the logistic regression algorithm.\n\n\n\n\n\nApr 8, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron\n\n\n\n\n\nImplementing and testing the perceptron algorithm.\n\n\n\n\n\nMar 28, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study - Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nReplicating the findings of Obermeyer et. al. (2019) on healthcare risk scores.\n\n\n\n\n\nMar 5, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nFinding a 100% accurate classification model for the palmer penguins dataset.\n\n\n\n\n\nFeb 19, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]