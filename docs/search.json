[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/hospital/ReplicationBiasPost.html",
    "href": "posts/hospital/ReplicationBiasPost.html",
    "title": "Replication Study - Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "In this blogpost I seek to recreate the findings of Obermeyer et. al. (2019), in which they explore a potential algorithm for use scoring high risk patients. This score would then effect the resources and attention dedicated to a patient. Obermeyer et. al. found that the model was biased in that it used costs as an informant of risk scores, and as Black patients costs were typically lower than identical white patients. Obermeyer et. al. posited that this difference could be due to lack of access to healthcare despite appearance of chronic illness, and not that Black patients truly required less healthcare (and therefore healthcare spending) in the next year. We use simulated data based on the data Obermeyer et. al. used to examine whether or not we see a similar pattern in our own recreation of their model and analysis.\n\n\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\n\n\nIn my recreation of figure 1, I decided to follow Professor Chodrow’s example of seperating the data by gender.\n\nimport matplotlib as plt\nimport seaborn as sns\n\nFirst, I calculated the risk percentile scores for every entry of the dataset.\n\ndf['risk_perc'] = (df['risk_score_t'].rank(pct = True)*100).round()\n\nThen, I seperated the entries out into seperate datasets to make them easier to work with, one for each gender and racial combination.\n\nfemale_b_data = df[(df['dem_female'] == 1) & (df['race'] == 'black')]\nfemale_w_data = df[(df['dem_female'] == 1) & (df['race'] == 'white')]\n\nmale_b_data = df[(df['dem_female'] == 0) & (df['race'] == 'black')]\nmale_w_data = df[(df['dem_female'] == 0) & (df['race'] == 'white')]\n\nThen I performed a series of calculations on each of the individual datasets. For each I calculated the mean number of chronic illnesses by risk percentile, and copied over the original racial and gender distinctions. Afterwards, I re-combined all four into one large dataset for graphing.\n\nmean_chronic_illnesses_female_b = female_b_data.groupby('risk_perc')['gagne_sum_t'].mean()\nmean_chronic_illnesses_female_w = female_w_data.groupby('risk_perc')['gagne_sum_t'].mean()\n\nfemb = mean_chronic_illnesses_female_b.reset_index()\nfemb['gender'] = 1\nfemb['race'] = 'black'\n\nfemw = mean_chronic_illnesses_female_w.reset_index()\nfemw['gender'] = 1\nfemw['race'] = 'white'\n\n\nmean_chronic_illnesses_male_b = male_b_data.groupby('risk_perc')['gagne_sum_t'].mean()\nmean_chronic_illnesses_male_w = male_w_data.groupby('risk_perc')['gagne_sum_t'].mean()\n\nmalb = mean_chronic_illnesses_male_b.reset_index()\nmalb['gender'] = 0\nmalb['race'] = 'black'\n\nmalw = mean_chronic_illnesses_male_w.reset_index()\nmalw['gender'] = 0\nmalw['race'] = 'white'\n\ncombined_data = pd.concat([malw, malb, femw, femb])\n\n\ncombined_data\n\n\n\n\n\n\n\n\n\nrisk_perc\ngagne_sum_t\ngender\nrace\n\n\n\n\n0\n0.0\n0.030043\n0\nwhite\n\n\n1\n1.0\n0.113043\n0\nwhite\n\n\n2\n2.0\n0.112676\n0\nwhite\n\n\n3\n3.0\n0.276596\n0\nwhite\n\n\n4\n4.0\n0.117089\n0\nwhite\n\n\n...\n...\n...\n...\n...\n\n\n95\n96.0\n5.500000\n1\nblack\n\n\n96\n97.0\n4.880000\n1\nblack\n\n\n97\n98.0\n6.024390\n1\nblack\n\n\n98\n99.0\n6.228070\n1\nblack\n\n\n99\n100.0\n8.294118\n1\nblack\n\n\n\n\n403 rows × 4 columns\n\n\n\n\nI used the seaborn facetgrid function to graph two scatterplots side by side seperated by gender and colored according to race.\n\nsns.set_theme(style=\"whitegrid\")\nsns.set_theme(style=\"whitegrid\")\n\n\nhue_markers = {'white': 'o', 'black': 'X'}\n\ng = sns.FacetGrid(combined_data, col=\"gender\", hue='race', palette='Set2', height=5, aspect=1.5)\ng.map(sns.scatterplot, 'gagne_sum_t', 'risk_perc')\ng.add_legend()\ng.set_xlabels(\"Mean Number of Chronic Illnesses\")\ng.set_ylabels(\"Risk Score Percentile\")\ntitles = ['Male', 'Female']\nfor ax, title in zip(g.axes.flat, titles):\n    ax.set_title(title)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nIn order to recreate figure three, I used the risk percentile calculations from the previous figure recreation, and once again separated the data based on race for ease of calculation. For each group I calculated the average cost by number of chronic illness and risk percentile. Then I recombined these datasets for graphing.\n\nbdata = df[df['race']=='black']\nwdata = df[df['race']=='white']\n\nmeanb_cost = bdata.groupby('risk_perc')['cost_t'].mean().reset_index()\nmeanb_cost['race'] = 'black'\nmeanw_cost = wdata.groupby('risk_perc')['cost_t'].mean().reset_index()\nmeanw_cost['race'] = 'white'\n\nchronicb = bdata.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicw = wdata.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicb['race'] = 'black'\nchronicw['race'] = 'white'\n\ncombined_data_risk = pd.concat([meanw_cost, meanb_cost])\ncombined_data_chronic = pd.concat([chronicb, chronicw])\n\n\ncombined_data_chronic\n\n\n\n\n\n\n\n\n\ngagne_sum_t\ncost_t\nrace\n\n\n\n\n0\n0\n3309.931507\nblack\n\n\n1\n1\n5242.040185\nblack\n\n\n2\n2\n7176.976744\nblack\n\n\n3\n3\n10621.153846\nblack\n\n\n4\n4\n11261.046512\nblack\n\n\n5\n5\n15723.715415\nblack\n\n\n6\n6\n21571.153846\nblack\n\n\n7\n7\n31575.000000\nblack\n\n\n8\n8\n22543.478261\nblack\n\n\n9\n9\n48452.941176\nblack\n\n\n10\n10\n54806.000000\nblack\n\n\n11\n11\n57418.518519\nblack\n\n\n12\n12\n86836.842105\nblack\n\n\n13\n13\n78950.000000\nblack\n\n\n14\n14\n81916.666667\nblack\n\n\n15\n15\n88825.000000\nblack\n\n\n16\n16\n31150.000000\nblack\n\n\n0\n0\n4372.520651\nwhite\n\n\n1\n1\n6440.395738\nwhite\n\n\n2\n2\n7984.731978\nwhite\n\n\n3\n3\n10682.347236\nwhite\n\n\n4\n4\n13160.115607\nwhite\n\n\n5\n5\n18093.610548\nwhite\n\n\n6\n6\n21461.850649\nwhite\n\n\n7\n7\n28755.897436\nwhite\n\n\n8\n8\n32033.744856\nwhite\n\n\n9\n9\n40892.907801\nwhite\n\n\n10\n10\n45715.315315\nwhite\n\n\n11\n11\n45588.888889\nwhite\n\n\n12\n12\n52650.909091\nwhite\n\n\n13\n13\n72557.142857\nwhite\n\n\n14\n14\n75841.176471\nwhite\n\n\n15\n15\n62800.000000\nwhite\n\n\n16\n16\n38500.000000\nwhite\n\n\n17\n17\n55750.000000\nwhite\n\n\n\n\n\n\n\n\nI made two, side-by-side graphs to show average cost by risk percentile and number of chronic illnesses just like the authors did.\n\nf, axs = plt.subplots(1,2, figsize=(9,5), sharey=True)\nsns.scatterplot(data = combined_data_risk,x= 'risk_perc', y='cost_t', hue='race', ax=axs[0], palette='Set2')\naxs[0].set_xlabel('Risk Percentile')\naxs[0].set_ylabel('Mean Total Cost')\nplt.yscale('log')\n\nsns.scatterplot(data=combined_data_chronic,x='gagne_sum_t', y='cost_t', hue='race', ax=axs[1], palette='Set2')\naxs[1].set_xlabel('Mean Number of Chronic Illnesses')\naxs[1].set_ylabel('')\nplt.yscale('log')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\nThis figure shows how mean cost increases with the assigned risk percentile and number of chronic illnesses."
  },
  {
    "objectID": "posts/hospital/ReplicationBiasPost.html#abstract",
    "href": "posts/hospital/ReplicationBiasPost.html#abstract",
    "title": "Replication Study - Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "In this blogpost I seek to recreate the findings of Obermeyer et. al. (2019), in which they explore a potential algorithm for use scoring high risk patients. This score would then effect the resources and attention dedicated to a patient. Obermeyer et. al. found that the model was biased in that it used costs as an informant of risk scores, and as Black patients costs were typically lower than identical white patients. Obermeyer et. al. posited that this difference could be due to lack of access to healthcare despite appearance of chronic illness, and not that Black patients truly required less healthcare (and therefore healthcare spending) in the next year. We use simulated data based on the data Obermeyer et. al. used to examine whether or not we see a similar pattern in our own recreation of their model and analysis.\n\n\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\n\n\n\nIn my recreation of figure 1, I decided to follow Professor Chodrow’s example of seperating the data by gender.\n\nimport matplotlib as plt\nimport seaborn as sns\n\nFirst, I calculated the risk percentile scores for every entry of the dataset.\n\ndf['risk_perc'] = (df['risk_score_t'].rank(pct = True)*100).round()\n\nThen, I seperated the entries out into seperate datasets to make them easier to work with, one for each gender and racial combination.\n\nfemale_b_data = df[(df['dem_female'] == 1) & (df['race'] == 'black')]\nfemale_w_data = df[(df['dem_female'] == 1) & (df['race'] == 'white')]\n\nmale_b_data = df[(df['dem_female'] == 0) & (df['race'] == 'black')]\nmale_w_data = df[(df['dem_female'] == 0) & (df['race'] == 'white')]\n\nThen I performed a series of calculations on each of the individual datasets. For each I calculated the mean number of chronic illnesses by risk percentile, and copied over the original racial and gender distinctions. Afterwards, I re-combined all four into one large dataset for graphing.\n\nmean_chronic_illnesses_female_b = female_b_data.groupby('risk_perc')['gagne_sum_t'].mean()\nmean_chronic_illnesses_female_w = female_w_data.groupby('risk_perc')['gagne_sum_t'].mean()\n\nfemb = mean_chronic_illnesses_female_b.reset_index()\nfemb['gender'] = 1\nfemb['race'] = 'black'\n\nfemw = mean_chronic_illnesses_female_w.reset_index()\nfemw['gender'] = 1\nfemw['race'] = 'white'\n\n\nmean_chronic_illnesses_male_b = male_b_data.groupby('risk_perc')['gagne_sum_t'].mean()\nmean_chronic_illnesses_male_w = male_w_data.groupby('risk_perc')['gagne_sum_t'].mean()\n\nmalb = mean_chronic_illnesses_male_b.reset_index()\nmalb['gender'] = 0\nmalb['race'] = 'black'\n\nmalw = mean_chronic_illnesses_male_w.reset_index()\nmalw['gender'] = 0\nmalw['race'] = 'white'\n\ncombined_data = pd.concat([malw, malb, femw, femb])\n\n\ncombined_data\n\n\n\n\n\n\n\n\n\nrisk_perc\ngagne_sum_t\ngender\nrace\n\n\n\n\n0\n0.0\n0.030043\n0\nwhite\n\n\n1\n1.0\n0.113043\n0\nwhite\n\n\n2\n2.0\n0.112676\n0\nwhite\n\n\n3\n3.0\n0.276596\n0\nwhite\n\n\n4\n4.0\n0.117089\n0\nwhite\n\n\n...\n...\n...\n...\n...\n\n\n95\n96.0\n5.500000\n1\nblack\n\n\n96\n97.0\n4.880000\n1\nblack\n\n\n97\n98.0\n6.024390\n1\nblack\n\n\n98\n99.0\n6.228070\n1\nblack\n\n\n99\n100.0\n8.294118\n1\nblack\n\n\n\n\n403 rows × 4 columns\n\n\n\n\nI used the seaborn facetgrid function to graph two scatterplots side by side seperated by gender and colored according to race.\n\nsns.set_theme(style=\"whitegrid\")\nsns.set_theme(style=\"whitegrid\")\n\n\nhue_markers = {'white': 'o', 'black': 'X'}\n\ng = sns.FacetGrid(combined_data, col=\"gender\", hue='race', palette='Set2', height=5, aspect=1.5)\ng.map(sns.scatterplot, 'gagne_sum_t', 'risk_perc')\ng.add_legend()\ng.set_xlabels(\"Mean Number of Chronic Illnesses\")\ng.set_ylabels(\"Risk Score Percentile\")\ntitles = ['Male', 'Female']\nfor ax, title in zip(g.axes.flat, titles):\n    ax.set_title(title)\n\n\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nIn order to recreate figure three, I used the risk percentile calculations from the previous figure recreation, and once again separated the data based on race for ease of calculation. For each group I calculated the average cost by number of chronic illness and risk percentile. Then I recombined these datasets for graphing.\n\nbdata = df[df['race']=='black']\nwdata = df[df['race']=='white']\n\nmeanb_cost = bdata.groupby('risk_perc')['cost_t'].mean().reset_index()\nmeanb_cost['race'] = 'black'\nmeanw_cost = wdata.groupby('risk_perc')['cost_t'].mean().reset_index()\nmeanw_cost['race'] = 'white'\n\nchronicb = bdata.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicw = wdata.groupby('gagne_sum_t')['cost_t'].mean().reset_index()\nchronicb['race'] = 'black'\nchronicw['race'] = 'white'\n\ncombined_data_risk = pd.concat([meanw_cost, meanb_cost])\ncombined_data_chronic = pd.concat([chronicb, chronicw])\n\n\ncombined_data_chronic\n\n\n\n\n\n\n\n\n\ngagne_sum_t\ncost_t\nrace\n\n\n\n\n0\n0\n3309.931507\nblack\n\n\n1\n1\n5242.040185\nblack\n\n\n2\n2\n7176.976744\nblack\n\n\n3\n3\n10621.153846\nblack\n\n\n4\n4\n11261.046512\nblack\n\n\n5\n5\n15723.715415\nblack\n\n\n6\n6\n21571.153846\nblack\n\n\n7\n7\n31575.000000\nblack\n\n\n8\n8\n22543.478261\nblack\n\n\n9\n9\n48452.941176\nblack\n\n\n10\n10\n54806.000000\nblack\n\n\n11\n11\n57418.518519\nblack\n\n\n12\n12\n86836.842105\nblack\n\n\n13\n13\n78950.000000\nblack\n\n\n14\n14\n81916.666667\nblack\n\n\n15\n15\n88825.000000\nblack\n\n\n16\n16\n31150.000000\nblack\n\n\n0\n0\n4372.520651\nwhite\n\n\n1\n1\n6440.395738\nwhite\n\n\n2\n2\n7984.731978\nwhite\n\n\n3\n3\n10682.347236\nwhite\n\n\n4\n4\n13160.115607\nwhite\n\n\n5\n5\n18093.610548\nwhite\n\n\n6\n6\n21461.850649\nwhite\n\n\n7\n7\n28755.897436\nwhite\n\n\n8\n8\n32033.744856\nwhite\n\n\n9\n9\n40892.907801\nwhite\n\n\n10\n10\n45715.315315\nwhite\n\n\n11\n11\n45588.888889\nwhite\n\n\n12\n12\n52650.909091\nwhite\n\n\n13\n13\n72557.142857\nwhite\n\n\n14\n14\n75841.176471\nwhite\n\n\n15\n15\n62800.000000\nwhite\n\n\n16\n16\n38500.000000\nwhite\n\n\n17\n17\n55750.000000\nwhite\n\n\n\n\n\n\n\n\nI made two, side-by-side graphs to show average cost by risk percentile and number of chronic illnesses just like the authors did.\n\nf, axs = plt.subplots(1,2, figsize=(9,5), sharey=True)\nsns.scatterplot(data = combined_data_risk,x= 'risk_perc', y='cost_t', hue='race', ax=axs[0], palette='Set2')\naxs[0].set_xlabel('Risk Percentile')\naxs[0].set_ylabel('Mean Total Cost')\nplt.yscale('log')\n\nsns.scatterplot(data=combined_data_chronic,x='gagne_sum_t', y='cost_t', hue='race', ax=axs[1], palette='Set2')\naxs[1].set_xlabel('Mean Number of Chronic Illnesses')\naxs[1].set_ylabel('')\nplt.yscale('log')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\nThis figure shows how mean cost increases with the assigned risk percentile and number of chronic illnesses."
  },
  {
    "objectID": "posts/hospital/ReplicationBiasPost.html#discussion",
    "href": "posts/hospital/ReplicationBiasPost.html#discussion",
    "title": "Replication Study - Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "DISCUSSION",
    "text": "DISCUSSION\nThe results of these findings, as well as the paper from Obermeyer et. al. (2019) which this post reproduces, indicate that there is a clear statistical bias in using this model to predict high risk patients. Most obviously, the model violates the fairness measure of error rate parity, as defined by Barocas, Hardt, and Narayanan (2023). Error rate parity is a principle of statistical fairness that aims to ensure fairness in predictive models by requiring that the error rates (i.e. false positive rates and false negative rates) are balanced across different demographic groups. Our model, however, assigns Black patients lower risk scores compared to white patients with the same number of chronic illnesses. This indicates a discrepancy in how errors are distributed between these groups. If the model assigns lower risk scores to Black patients despite having identical medical histories to white patients, it suggests that the model is more likely to make errors, in this context, underestimate risk, for Black patients compared to white patients. This discrepancy in error rates undermines the fairness of the model, as it means that Black patients may not receive appropriate healthcare resources or interventions due to the inaccuracies in the risk assessment. In their paper, Oberymeyer er. al. explore methodologies to account for this potential bias, and find some success is either excluding race as a feature, decorrelating other features from the race variable, or basing predictions more heavily on a combination of health history and patient spending."
  },
  {
    "objectID": "posts/FinalProject/AnalysisDoc.html",
    "href": "posts/FinalProject/AnalysisDoc.html",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport seaborn as sns\nallergies = pd.read_csv('allergies.csv')\ncancer = pd.read_csv('cancer.csv')\ncareplans = pd.read_csv('careplans.csv')\nclaims = pd.read_csv('claims.csv')\nconditions = pd.read_csv('conditions.csv')\ndiabetes = pd.read_csv('diabetes.csv')\nencounters = pd.read_csv('encounters.csv') ##NOT USING RN, DO WE NEED?\netc = pd.read_csv('etc.csv')\nheart = pd.read_csv('heart.csv')\nimmunizations = pd.read_csv('immunizations.csv')\nlungs = pd.read_csv('lungs.csv')\nmedications = pd.read_csv('medications.csv')\nobservations = pd.read_csv('observations.csv')\npatients = pd.read_csv('patients.csv')\npregnancy = pd.read_csv('pregnancy.csv')\nprocedures = pd.read_csv('procedures.csv')\nExploring Data by Visualization\n# careplans #yeah\n# conditions #yeah\n# encounters #yeah\n# immunizations #not best\n# observations #might be cool to look at our average patient stats\n# patients #could combine with above to give ethnicity and gender too ^\n# careplans viz \ncareplanCounts = careplans.groupby('DESCRIPTION').size().reset_index(name='count')\ncareplanCounts = careplanCounts.sort_values(by='count', ascending=False)\ntop10Careplans = careplanCounts.head(10)\nprint(top10Careplans)\n\nplt.figure(figsize=(10, 6))\nplt.bar(top10Careplans['DESCRIPTION'], top10Careplans['count'], color='skyblue')\nplt.xlabel('Careplan')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Careplans')\nplt.xticks(rotation=45, ha='right')\nplt.grid()\nplt.show()\n\n                              DESCRIPTION  count\n67                      Stress management    830\n34                       Exercise therapy    719\n22  Deep breathing and coughing exercises    641\n57       Recommendation to avoid exercise    641\n60                    Respiratory therapy    641\n6                     Antenatal education    609\n62                 Routine antenatal care    609\n53               Pregnancy diet education    609\n27                          Diabetic diet    498\n26          Diabetes self management plan    498\n#conditions viz \nconditionsCounts = conditions.groupby('DESCRIPTION').size().reset_index(name='count')\nconditionsCounts = conditionsCounts.sort_values(by='count', ascending=False)\ntop10Conditions = conditionsCounts.head(10)\nprint(top10Conditions)\n\nplt.figure(figsize=(10, 6))\nplt.grid()\nplt.bar(top10Conditions['DESCRIPTION'], top10Conditions['count'], color='skyblue')\nplt.xlabel('Condition')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Conditions')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n#note that there is some matchup between top 10 careplans and top 10 conditions. this is good bc is expected behavior \n\n                              DESCRIPTION  count\n116            Viral sinusitis (disorder)   1125\n3      Acute viral pharyngitis (disorder)    602\n2             Acute bronchitis (disorder)    508\n86                            Prediabetes    458\n52                           Hypertension    373\n75                       Normal pregnancy    339\n20           Chronic sinusitis (disorder)    329\n79                           Otitis media    202\n110  Streptococcal sore throat (disorder)    146\n85                         Polyp of colon    108\n#encounters viz \nencountersCounts = encounters.groupby('DESCRIPTION').size().reset_index(name='count')\nencountersCounts = encountersCounts.sort_values(by='count', ascending=False)\ntop10Encounters = encountersCounts.head(10)\nprint(top10Encounters)\n\nplt.figure(figsize=(10, 6))\nplt.grid()\nplt.bar(top10Encounters['DESCRIPTION'], top10Encounters['count'], color='skyblue')\nplt.xlabel('Encounter Type')\nplt.ylabel('Count')\nplt.title('Top 10 Most Frequent Medical Encounters')\nplt.xticks(rotation=45, ha='right')\nplt.show()\n\n                    DESCRIPTION  count\n22         Outpatient Encounter   8629\n17        Encounter for symptom   2852\n24  Patient encounter procedure   1524\n27               Prenatal visit   1383\n23         Outpatient procedure   1281\n5    Consultation for treatment    899\n15     Encounter for 'check-up'    751\n13     Emergency room admission    694\n16        Encounter for problem    628\n6           Death Certification    461\n#what the 'average' patient looks like in terms of race and gender,, viz?\n\n#find most frequent race to represent average\npatientRaceCount = patients.groupby('race').size().reset_index(name='raceCount')\npatientRaceCount = patientRaceCount.sort_values(by='raceCount', ascending=False)\ntop5FreqRace = patientRaceCount.head(5)\nprint(top5FreqRace)\n\n#repeat for gender\npatientGenderCount = patients.groupby('gender').size().reset_index(name='genderCount')\npatientGenderCount = patientGenderCount.sort_values(by='genderCount', ascending=False)\ntop5FreqGender = patientGenderCount.head(5)\nprint(top5FreqGender)\n\n#repeat for ethnicity ? i dont find this super meaningful\n# patientEthnicityCount = patients.groupby('ethnicity').size().reset_index(name='ethnicityCount')\n# patientEthnicityCount = patientEthnicityCount.sort_values(by='ethnicityCount', ascending=False)\n# top5FreqEthnicity = patientEthnicityCount.head(5)\n# print(top5FreqEthnicity)\n\n       race  raceCount\n3     white       1085\n2  hispanic        155\n1     black        129\n0     asian         93\n  gender  genderCount\n1      M          741\n0      F          721\n#visualization for what the 'average' patient looks like in terms of body stat characteristics\n#want to create a radar chart but having a hard time eek\nobservationsRadar = observations.drop(columns = ['DATE', 'PATIENT', 'ENCOUNTER', 'CODE', 'UNITS'])\nobservationsRadar = observationsRadar.dropna()\nobservationsRadar['VALUE'] = pd.to_numeric(observationsRadar['VALUE'], errors='coerce')\naverageVals = observationsRadar.groupby('DESCRIPTION')['VALUE'].mean().reset_index()\naverageVals = averageVals[averageVals['DESCRIPTION'].isin(['Body Height', 'Body Weight', 'Body Mass Index', 'Systolic Blood Pressure', 'Diastolic Blood Pressure', 'Quality adjusted life years'])]\n#averageVals = averageVals.transpose()\naverageVals\n\n\n\n\n\n\n\n\n\nDESCRIPTION\nVALUE\n\n\n\n\n3\nBody Height\n145.246010\n\n\n4\nBody Mass Index\n27.837863\n\n\n5\nBody Weight\n65.032186\n\n\n16\nDiastolic Blood Pressure\n84.485896\n\n\n40\nQuality adjusted life years\n41.901321\n\n\n45\nSystolic Blood Pressure\n129.587277\n#CREATION OF RADAR CHART OBTAINED FROM ONLINE CODE \ndescriptions = averageVals['DESCRIPTION'].tolist()\nvalues = averageVals['VALUE'].tolist()\n\n#to close the circle\nvalues.append(values[0])\n\nnum_vars = len(descriptions)\n\n#compute angle for each axis\nangles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n\n#need to \"complete the loop\" and append the start value to the end since plot is a cirlce\nvalues += values[:1]\nangles += angles[:1]\nvalues.pop()\n\n#plot\nfig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\nax.fill(angles, values, color='skyblue', alpha=0.7)\n\nax.set_xticks(angles[:-1])\nax.set_xticklabels(descriptions, fontsize=12)\n\nax.set_yticklabels([])\n\n#add values to each point\nfor angle, value in zip(angles[:-1], values[:-1]):\n    ax.text(angle, value, str(round(value, 2)), ha='center', va='bottom', fontsize=10)\n\nplt.show()"
  },
  {
    "objectID": "posts/FinalProject/AnalysisDoc.html#pulmonary-diseases-eda",
    "href": "posts/FinalProject/AnalysisDoc.html#pulmonary-diseases-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Pulmonary Diseases EDA",
    "text": "Pulmonary Diseases EDA\n\nPULMONARY DISEASES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(lungs, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Pulmonary Conditions by Race')\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPULMONARY DISEASES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(lungs, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Pulmonary Conditions by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPULMONARY DISEASES BY BIRTHPLACE\nCreating a dataframe of populations of all the towns present in our data (all in Massachusetts), in order to calculate adjusted prevalance.\n\npopulations = pd.DataFrame()\nunique_birthplaces = lungs['birthplace'].unique()\npopulations['birthplace'] = unique_birthplaces\npopulations['pop'] = [37819, 101727, 35313, 93682, 3785, 53896, 87954,\n                       62698, 60803, 5025, 18317, 23662, 1734, 79762, 205319, 1641,\n                       25334, 650706, 6362, 31531, 59922, 6802, 58528, 87381, 19872,\n                       17612, 54980, 25121, 24747, 17489, 35022, 64065, 22992, 6196, 15946, 27395,\n                       44722, 25905, 9806, 11657, 65399, 8485, 15988, 113608, 28854, 4301, 104826, \n                       118488, 29155, 13885, 12265, 23315, 1677, 28501, 67153, 49350, 29327, 10293,\n                       29349, 8270, 154064, 19808, 43310, 29862, 22325, 16593, 6388, 6379, 17619, 14939, \n                       16720, 40971, 31747, 43646, 25050, 38637, 1802, 15702, 15101, 29195, 18448, 16732,\n                       16516, 11048, 7754, 11066, 19948, 4688, 100891, 33792, 18181, 11115, 37286, 100682,\n                       36229, 14570, 46601, 10911, 11625, 17669, 13697, 20902, 10667, 19163, 22666, 10580,\n                       32158, 11777, 1861, 2215, 6347, 64712, 23629, 34307, 752, 9547, 16705, 13866, 18510,\n                       43784, 35744, 21478, 70963, 42844, 40535, 16296, 16127, 7973, 31296, 26123, 3265, 13320,\n                       53241, 3234, 7839, 14749, 8055, 24498, 9640, 42235, 1730, 5798, 37973, 8316, 23923, 18662,\n                       10084, 6975, 15827, 49532, 11964, 136913, 6279, 7214, 17806, 41248, 6358, 10874, 19063, 6569,\n                       23184, 11753, 11386, 2985, 16450, 9182, 7764, 21374, 11802, 41502, 4111, 16053, 6183, 27003,\n                       15710, 6125, 8471, 4963, 29836, 1029, 15227, 10000, 12337, 1793, 491, 4678, 11988, 8168, 28950,\n                       1566, 12904, 57410, 6850, 13427, 11327, 6532, 917, 17456, 25209, 717, 17182, 15168, 12777, \n                       8153, 14313, 13435, 1264, 7884, 9230, 14180, 3056, 12418, 14382, 31248, 10169, 8541, 16188, \n                       36500, 31388, 17027, 16094, 13911, 28385, 9395, 11261, 1489, 12925, 27999, 5943, 16693, 5346,\n                       9811, 27400, 4871, 24296, 12133, 6346, 31635, 11688, 1245, 5284, 5966, 1458, 780, 27135, 316,\n                       5429, 17765, 12629, 3390, 6952, 7144, 2180, 7649, 4907, 5139, 4852, 5125, 5135, 2901, 5398, 4519]\n\n\ncondition_columns = [col for col in lungs.columns if col.endswith('_CONDITIONS')]\nconditions_summed = lungs.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatalungs = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatalungs[f'prevalence_{condition_column}'] = prevdatalungs[condition_column] / prevdatalungs['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatalungs[prevdatalungs[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "posts/FinalProject/AnalysisDoc.html#cancer-eda",
    "href": "posts/FinalProject/AnalysisDoc.html#cancer-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Cancer EDA",
    "text": "Cancer EDA\n\nCANCER BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(cancer, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Cancer by Race')\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCANCER BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(cancer, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cancer by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ncancer.head()\n\n\n\n\n\n\n\n\n\nPATIENT\nNon-small cell lung cancer (disorder)_CONDITIONS\nNon-small cell carcinoma of lung TNM stage 4 (disorder)_CONDITIONS\nPrimary small cell malignant neoplasm of lung TNM stage 4 (disorder)_CONDITIONS\nNon-small cell carcinoma of lung TNM stage 2 (disorder)_CONDITIONS\nNon-small cell lung cancer (disorder)_CONDITIONS.1\nSuspected lung cancer (situation)_CONDITIONS\nMalignant tumor of colon_CONDITIONS\nOverlapping malignant neoplasm of colon_CONDITIONS\nbirthdate\n...\nSodium_OBSERVATIONS\nSoybean IgE Ab in Serum_OBSERVATIONS\nSystolic Blood Pressure_OBSERVATIONS\nTotal Cholesterol_OBSERVATIONS\nTotal score [MMSE]_OBSERVATIONS\nTriglycerides_OBSERVATIONS\nUrea Nitrogen_OBSERVATIONS\nWalnut IgE Ab in Serum_OBSERVATIONS\nWheat IgE Ab in Serum_OBSERVATIONS\nWhite oak IgE Ab in Serum_OBSERVATIONS\n\n\n\n\n0\n00269bb7-e3ab-43a9-9cdf-cdf9b6e3b2b3\n0\n0\n0\n0\n0\n0\n0\n0\n1980-10-15\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n00341a88-1cc1-4b39-b0f9-05b0531991a0\n0\n0\n0\n0\n0\n0\n0\n0\n1972-07-16\n...\n0.0\n0.0\n466.0\n574.0\n0.0\n430.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n004a5922-7c4d-40cc-a0f8-68f607044c99\n0\n0\n0\n0\n0\n0\n0\n0\n2014-12-11\n...\n0.0\n0.0\n1208.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n00630ce3-e8eb-4ed4-889b-2c0ac257cbf4\n0\n0\n0\n0\n0\n0\n0\n0\n2003-03-20\n...\n0.0\n0.0\n1205.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n007cbcc1-7333-4c79-b5e9-ffa93822fa11\n0\n0\n0\n0\n0\n0\n0\n0\n1998-03-05\n...\n0.0\n0.0\n1082.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 342 columns\n\n\n\n\n\nimport squarify\nimport seaborn as sns\n\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\n\nnum_columns = len(condition_columns)\nnum_rows = num_columns  # Set the number of rows equal to the number of columns for one subplot per row\nfig, axes = plt.subplots(num_rows, 1, figsize=(10, 20*num_rows))  # Increase figure height\n\n# Use Seaborn color palette\ncolors = sns.color_palette(\"tab20\")  \n\nfor i, col in enumerate(condition_columns):\n    # Calculate proportion of ethnicities with a value of 1\n    proportions = cancer.groupby('ethnicity')[col].mean()\n    \n    # Check for proportions under a threshold and sum them up\n    threshold = 0.01  # Adjust this threshold as needed\n    small_proportions = proportions[proportions &lt; threshold]\n\n    if not small_proportions.empty:\n        proportions = proportions[proportions &gt;= threshold]\n        proportions['other'] = small_proportions.sum()\n\n    # Check for zero values and replace them with a small value (e.g., epsilon)\n    epsilon = 1e-6  # Small value to avoid division by zero\n    proportions = proportions.replace(0, epsilon)\n    \n    # Filter out proportions with zero values\n    proportions = proportions[proportions &gt; 0]\n\n    # Adjust figure size for each subplot\n    fig = plt.figure(figsize=(14, 6))\n\n    # Get the current axis\n    ax = axes[i] if num_rows &gt; 1 else axes\n    \n    squarify.plot(sizes=proportions, label=proportions.index, alpha=0.8, ax=ax, color=colors)\n\n    # Add text labels for proportions\n    for index, rect in enumerate(ax.patches):\n        x = rect.get_x() + rect.get_width() / 2\n        y = rect.get_y() + rect.get_height() / 2\n        proportion = proportions.iloc[index]\n        \n        # Adjust the y position of the proportion label\n        if proportion &gt;= 0.5:\n            y -= rect.get_height() * 0.2  # Move the text below the rectangle\n        else:\n            y += rect.get_height() * 0.2  # Move the text above the rectangle\n        \n        percentage = proportion * 100\n        ax.text(x, y, f'{percentage:.2f}%', ha='center', va='center', fontsize=10, color='black')\n        \nplt.show()\n\n\n\n\n\n\n\n\n&lt;Figure size 1400x600 with 0 Axes&gt;\n\n\n&lt;Figure size 1400x600 with 0 Axes&gt;\n\n\n&lt;Figure size 1400x600 with 0 Axes&gt;\n\n\n&lt;Figure size 1400x600 with 0 Axes&gt;\n\n\n&lt;Figure size 1400x600 with 0 Axes&gt;\n\n\n&lt;Figure size 1400x600 with 0 Axes&gt;\n\n\n&lt;Figure size 1400x600 with 0 Axes&gt;\n\n\n\n\nCANCER PREVALANCE BY BIRTHPLACE\n\ncondition_columns = [col for col in cancer.columns if col.endswith('_CONDITIONS')]\nconditions_summed = cancer.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatacanc = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatacanc[f'prevalence_{condition_column}'] = prevdatacanc[condition_column] / prevdatacanc['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatacanc[prevdatacanc[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "posts/FinalProject/AnalysisDoc.html#diabetes-eda",
    "href": "posts/FinalProject/AnalysisDoc.html#diabetes-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Diabetes EDA",
    "text": "Diabetes EDA\n\nDIABETES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(diabetes, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Diabetes by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDIABETES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(diabetes, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Diabetes by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDIABETES PREVALENCE BY BIRTHPLACE\n\ncondition_columns = [col for col in diabetes.columns if col.endswith('_CONDITIONS')]\nconditions_summed = diabetes.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatadiab = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatadiab[f'prevalence_{condition_column}'] = prevdatadiab[condition_column] / prevdatadiab['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatadiab[prevdatadiab[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "posts/FinalProject/AnalysisDoc.html#cardiovascular-diseases-eda",
    "href": "posts/FinalProject/AnalysisDoc.html#cardiovascular-diseases-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Cardiovascular Diseases EDA",
    "text": "Cardiovascular Diseases EDA\n\nCARDIOVASCULAR DISEASES BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(heart, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Cardiovascular Diseases by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCARDIOVASCULAR DISEASES BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(heart, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cardiovascular Diseases by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Assuming heart is your DataFrame\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(heart, id_vars=['ethnicity'], value_vars=condition_columns)\n\n# Calculate proportions for each ethnicity and condition\nproportions = melted_df.groupby(['variable', 'ethnicity']).value.mean().unstack()\n\nplt.figure(figsize=(23, 15))\nplt.grid()\n\n# Plotting a single bar for each condition showing proportions for each ethnicity\nproportions.plot(kind='bar', stacked=True, colormap='tab20')\nplt.xticks(ticks=range(len(condition_columns)), labels=condition_columns, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cardiovascular Diseases by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n/var/folders/nw/0c95j4ds0db00hkx4bqf7g5m0000gn/T/ipykernel_13907/117258494.py:24: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all axes decorations.\n  plt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCARDIOVASCULAR DISEASE PREVALENCE BY BIRTHPLACE\n\ncondition_columns = [col for col in heart.columns if col.endswith('_CONDITIONS')]\nconditions_summed = heart.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdataheart = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdataheart[f'prevalence_{condition_column}'] = prevdataheart[condition_column] / prevdataheart['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdataheart[prevdataheart[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "posts/FinalProject/AnalysisDoc.html#pregnancy-complicationsn-eda",
    "href": "posts/FinalProject/AnalysisDoc.html#pregnancy-complicationsn-eda",
    "title": "Final Project Methods, Exploratory Data Analysis",
    "section": "Pregnancy Complicationsn EDA",
    "text": "Pregnancy Complicationsn EDA\n\nPREGNANCY COMPLICATIONS BY RACE\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(pregnancy, id_vars=['race'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\n\nplt.figure(figsize=(12, 6))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='race', data=melted_df, errorbar=None)\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right')\nplt.xlabel('Condition')\nplt.ylabel('Proportion')\nplt.title('Proportion of Pregnancy Complications by Race')\nplt.tight_layout()\nplt.legend(loc='upper right', bbox_to_anchor=(1.125, 1))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPREGNANCY COMPLICATIONS BY ETHNICITY\n\n# Get the columns ending with '_CONDITIONS'\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\n\n# Melt the DataFrame to long format\nmelted_df = pd.melt(pregnancy, id_vars=['ethnicity'], value_vars=condition_columns)\n\nclean_condition_labels = [col.replace('_CONDITIONS', '') for col in condition_columns]\n\nplt.figure(figsize=(23, 15))\nplt.grid()\nsns.barplot(x='variable', y='value', hue='ethnicity', data=melted_df, errorbar=None, palette='tab20')\nplt.xticks(ticks=range(len(clean_condition_labels)), labels=clean_condition_labels, rotation=45, ha='right', fontsize=20)\nplt.xlabel('Condition', fontsize=20)\nplt.ylabel('Proportion', fontsize=20)\nplt.title('Proportion of Cardiovascular Pregnancy Complications by Ethnicity', fontsize=20)\nlegend = plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1), title='Ethnicity')\nfor label in legend.get_texts():\n    label.set_fontsize(20) \nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPREGNANCY COMPLICATIONS BY BIRTHPLACE\n\ncondition_columns = [col for col in pregnancy.columns if col.endswith('_CONDITIONS')]\nconditions_summed = pregnancy.groupby('birthplace')[condition_columns].sum().reset_index()\nprevdatapreg = pd.merge(conditions_summed, populations, on=\"birthplace\")\nfor condition_column in condition_columns:\n    prevdatapreg[f'prevalence_{condition_column}'] = prevdatapreg[condition_column] / prevdatapreg['pop']\n\n\nfor condition_column in condition_columns:\n    # Filter out birthplaces where prevalence is not zero\n    filtered_data = prevdatapreg[prevdatapreg[f'prevalence_{condition_column}'] != 0]\n\n    plt.figure(figsize=(20, 6))\n    plt.grid()\n    sns.barplot(x='birthplace', y=f'prevalence_{condition_column}', data=filtered_data)\n    plt.title(f'Prevalence of {condition_column[:-11]} by Birthplace')\n    plt.xlabel('Birthplace')\n    plt.ylabel('Prevalence')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "posts/FinalProject/ModelDoc.html",
    "href": "posts/FinalProject/ModelDoc.html",
    "title": "Final Project Results, Model Creation Document",
    "section": "",
    "text": "RESULTS: The Output of Our Model\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport seaborn as sns\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n\nReading in our Data\n\nconditions_diabetes = pd.read_csv('conditions_diabetes.csv')\nconditions_pregnancy = pd.read_csv('conditions_pregnancy.csv')\nconditions_cancer = pd.read_csv('conditions_cancer.csv')\nconditions_heart = pd.read_csv('conditions_heart.csv')\nconditions_lungs = pd.read_csv('conditions_lungs.csv')\n\nobservations = pd.read_csv('observations_pivot.csv')\npatients = pd.read_csv('patient_clean.csv')\n\nNote: All of our datasets are grouped by related diseases (for example diabetes and comorbitidies such as diabetic retinopathy), for the rest of the post, when we say “diabetes” or “pregnancy complications,” we are talking about diabetes and all present comorbidites, or a grouping of pregnancy complications such as pre/ante eclampsia and misscarriage.\n\n\n\nDiabetes Modeling & Analysis\n\n1. Prepping Data\nIn order to prep our data for modelling we label encoded each of the qualitative variables (keeping track so we could decode them again later). We created a function in order to do this easily multiple times.\n\nle = LabelEncoder()\n\n# our data-prepping function for modeling\ndef prep_data(df):\n    \n    # label encode all quantitative vars\n    df[\"race\"] = le.fit_transform(df[\"race\"]) \n    race_code = {code: race for code, race in enumerate(le.classes_)}\n\n    df[\"ethnicity\"] = le.fit_transform(df[\"ethnicity\"])\n    eth_code = {code: ethnicity for code, ethnicity in enumerate(le.classes_)}\n\n    df[\"gender\"] = le.fit_transform(df[\"gender\"])\n    gen_code = {code: gender for code, gender in enumerate(le.classes_)}\n\n    df[\"birthplace\"] = le.fit_transform(df[\"birthplace\"])\n    bp_code = {code: bp for code, bp in enumerate(le.classes_)}\n\n    df[\"curr_town\"] = le.fit_transform(df[\"curr_town\"]) \n    curr_code = {code: bp for code, bp in enumerate(le.classes_)}\n    \n    # split data into test and train\n    train, test = train_test_split(df, test_size=0.2, random_state=42)\n    \n    X_train = train.drop(columns=['y'])\n    y_train = train['y']\n    \n    X_test = test.drop(columns=['y'])\n    y_test = test['y']\n    \n    # return split x, y, and all of the code tracking dicts\n    return X_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code\n\n\nnp.random.seed(300)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_diabetes)\n\n\n\n2. Finding optimal model\nNext, we created a function we could reuse that identifies the best performing model on our data from the options random forest, SVC, logistic regression, and decision trees. The best model is what we use to predict the probability that each person has a certain disease (for our purposes, their risk score).\n\n# our model-finding function\ndef train_model(X_train, y_train):\n    \n    #LogisticRegression\n    LR = LogisticRegression(max_iter=10000000000000000000)\n    LRScore = cross_val_score(LR, X_train, y_train, cv=5).mean()\n\n    #DecisionTreeClassifier\n    param_grid = { 'max_depth': [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None ]}\n\n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n    DTCScore  = grid_search.best_score_\n    bestDTCDepth = grid_search.best_params_\n\n\n    # Random Forrest Classifier    \n    forrest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forrest, param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n\n    RFCScore  = grid_search.best_score_\n    bestRFCDepth = grid_search.best_params_\n\n    #SVC\n    SVM = SVC()\n\n    # use grid search to find best gamma for SVM\n    g = {'gamma': 10.0 ** np.arange(-5, 5) }\n    grid_search = GridSearchCV(SVM, g, cv=5)\n    grid_search.fit(X_train, y_train)\n\n    SVMScore  = grid_search.best_score_   \n\n\n    print(\"best LR :\", LRScore)\n    print(\"best DTC:\", DTCScore)\n    print(\"best max depth: \", bestDTCDepth)\n    print(\"best RFC: \", RFCScore)\n    print(\"best max depth: \", bestRFCDepth)\n    print(\"best SVM: \", SVMScore)\n\n    # store the scores of each model\n    max_score = 0\n    max_model = \"\"\n    if LRScore &gt; max_score:\n        max_score = LRScore\n        max_model = \"LR\"\n    if DTCScore &gt; max_score:\n        max_score = DTCScore\n        max_model = \"DTC\"\n    if RFCScore &gt; max_score:\n        max_score = RFCScore\n        max_model = \"RFC\"\n    if SVMScore &gt; max_score:\n        max_score = SVMScore\n        max_model = \"SVM\"\n\n    print(\"best score overall is: \", max_score, \" with model: \", max_model)\n\n\n    \n# run model finding function on our diabetes data\nnp.random.seed(500)\ntrain_model(X_train, y_train)\n\nbest LR : 0.9050401672719269\nbest DTC: 0.9178790213124979\nbest max depth:  {'max_depth': 3}\nbest RFC:  0.9153112505043837\nbest max depth:  {'max_depth': 5}\nbest SVM:  0.9016066908770772\nbest score overall is:  0.9178790213124979  with model:  DTC\n\n\nThe results of our function should that the decision tree classifier is the best model possible, with an accuracy of 91.78%. Our accuracies tend generally lower considering the limited information we allowed the model to have, as we really wanted to see what the model would do when it predicted on identity factors such as race, ethnicity, and birthplace, and not how it would predict given information on the specific procedures and allergies a patient had.\n\n\n3. Create Risk Scores\nPredict probabilities for all our entries using the best model we found.\n\ndtc = DecisionTreeClassifier(max_depth=3)\ndtc.fit(X_train, y_train)\npred_prob = dtc.predict_proba(X_test)\n\nFor ease we created a risk finding function that can be used across factors and disease probabilities.\n\ndef find_risk(code, col, probs):\n    # finds the corresponding subset of our probability data\n    indices = (X_test[col] == code)\n    prob_subset = probs[indices]\n    # finds the average of this subset\n    av_prob = np.mean(prob_subset[:, 1]) \n    return av_prob   \n\n\n\n4. Compare Across Race, Gender, Ethnicity\nNext, we find the average risk score for different demographic characteristics: Race, Gender, and Ethnicity.\n\nRace\n\ndiabetesRaceRisk = []\n\n# find risk for each race (after finding on their code from the label encoder)\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    diabetesRaceRisk.append(newRow)\n\n# print summary table\ndiabetesRaceRisk = pd.DataFrame(diabetesRaceRisk)\ndiabetesRaceRisk = diabetesRaceRisk.sort_values(by='risk', ascending=False)\ndiabetesRaceRisk\n\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.479592\n\n\n2\nhispanic\n0.340659\n\n\n3\nwhite\n0.312536\n\n\n1\nblack\n0.256158\n\n\n\n\n\n\n\n\nOur model tells us that the most susceptible group to diabetes is Asian, then Hispanic and White, with Black being the least susceptible. These results were interesting in that they do indeed indicate that there may be a difference according to race, and made us think of how we could explore demographic information about Massachussetts (where our data is “from”), to understand whether these trends are reflective of larger trends.\n\n\nGender\n\ndiabetesGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    diabetesGenderRisk.append(newRow)\n\ndiabetesGenderRisk = pd.DataFrame(diabetesGenderRisk)\ndiabetesGenderRisk = diabetesGenderRisk.sort_values(by='risk', ascending=False)\ndiabetesGenderRisk\n\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.375356\n\n\n1\nM\n0.263908\n\n\n\n\n\n\n\n\nOur model tells us that women are slightly more likely to experience diabetes (or comorbidities) than men, which is in line with medical research we’ve seen.\n\n\nEthnicity\n\nav_risk_eth = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    av_risk_eth.append(new_row)\n\nav_risk_eth_df = pd.DataFrame(av_risk_eth)\nav_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\nav_risk_eth_df\n\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n2\nasian_indian\n0.714286\n\n\n13\npolish\n0.558405\n\n\n9\ngerman\n0.492674\n\n\n12\nmexican\n0.428571\n\n\n1\namerican\n0.428571\n\n\n14\nportuguese\n0.397959\n\n\n6\nenglish\n0.369491\n\n\n17\nscottish\n0.333333\n\n\n15\npuerto_rican\n0.329670\n\n\n5\ndominican\n0.328571\n\n\n0\nafrican\n0.318681\n\n\n11\nitalian\n0.314127\n\n\n3\ncentral_american\n0.306122\n\n\n7\nfrench\n0.285714\n\n\n8\nfrench_canadian\n0.261905\n\n\n4\nchinese\n0.244898\n\n\n18\nswedish\n0.205128\n\n\n16\nrussian\n0.200000\n\n\n10\nirish\n0.182902\n\n\n19\nwest_indian\n0.000000\n\n\n\n\n\n\n\n\nThis table gives us lots of information about risk by ethnicity, most interestingly perhaps, it agrees with our race finding that Asian people are more likely to experience diabetes, in that our most at risk ethnicity was Asian Indian. However, Chinese and West Indian, the two other Asian ethnicities in the datasest are at the bottom of the risk hierarchy, which made us consider that the risk of Asian Indian people specifically, and alone, was what was driving our race findings.\n\n\n\n5. Compare Across Wealthier & Poorer Towns of Residence/Birthplace\nIn order to compare outcomes across towns of varying socioeconomic status, we compiled a list of the richest and poorest towns present in our dataset (using Census data).\n\n# richest towns in Mass\nrichTowns = [\"Dover\", \"Weston\", \"Wellesley\", \"Lexington\", \"Sherborn\", \"Cohasset\", \"Lincoln\", \"Carlisle\", \"Hingham\", \"Winchester\", \n                \"Medfield\", \"Concord\", \"Needham\", \"Sudbury\", \"Hopkinton\", \"Boxford\", \"Brookline\", \"Andover\",  \n                  \"Southborough\", \"Belmont\", \"Acton\", \"Marblehead\", \"Newton\", \"Nantucket\", \"Duxbury\", \"Boxborough\", \"Westwood\",\"Natick\", \n                  \"Longmeadow\", \"Marion\", \"Groton\", \"Newbury\", \"North Andover\", \"Sharon\", \"Arlington\", \"Norwell\", \"Reading\", \n                  \"Lynnfield\", \"Marshfield\", \"Holliston\", \"Medway\", \"Canton\", \"Milton\", \"Ipswich\", \"Littleton\", \"Westford\", \"North Reading\", \"Chelmsford\", \"Dedham\",\n                  \"Walpole\", \"Mansfield\", \"Shrewsbury\", \"Norwood\", \"Hanover\", \"Stow\", \"Newburyport\", \"Chatham\", \"Orleans\", \"Harwich\",\n                  \"Swampscott\",\"Fairhaven\", \"Salem\"]\n\n# poorest towns in Mass\npoorTowns = [\"Springfield\", \"Lawrence\", \"Holyoke\", \"Amherst\", \"New Bedford\", \"Chelsea\", \"Fall River\", \"Athol\", \"Orange\", \"Lynn\", \"Fitchburg\", \"Gardner\", \"Brockton\", \"Malden\", \"Worcester\", \"Chicopee\", \"North Adams\", \"Everett\",\n    \"Ware\", \"Dudley\", \"Greenfield Town\", \"Weymouth Town\", \"Montague\", \"Revere\", \"Taunton\", \"Adams\", \"Huntington\", \"Charlemont\", \"Leominster\", \"Florida\", \"Colrain\", \"Hardwick\",\n    \"Palmer Town\", \"Peabody\", \"Somerville\", \"Lowell\", \"Westfield\", \"Billerica\"]\n\nCreate a df with all the information for the rich and poor towns\n\ndef find_town_info_row(town, bp_code_swapped, townCounts_df, code_name):\n    code = bp_code_swapped[town]\n    \n    if not townCounts_df[townCounts_df[code_name] == code].empty:\n        count = townCounts_df[townCounts_df[code_name] == code]['count'].values[0]\n    else:\n        count = 0\n    \n    new_row = {code_name: town, 'code': code, 'count': count}\n    \n    new_row_df = pd.DataFrame([new_row])\n    \n    return new_row_df\n\n\ndef find_town_info_all(counts, code_name):\n    \n    townCounts_df = pd.merge(X_test, counts, on=code_name)\n    town_info_rich = pd.DataFrame(columns=[code_name, 'code', 'count'])\n    town_info_poor = pd.DataFrame(columns=[code_name, 'code', 'count'])\n\n    bp_code_swapped = {value: key for key, value in bp_code.items()}\n\n    for town in richTowns:\n        \n        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n        town_info_rich = pd.concat([town_info_rich, new_row_df], ignore_index=True)\n\n    for town in poorTowns:\n        \n        new_row_df = find_town_info_row(town, bp_code_swapped, townCounts_df, code_name)\n        town_info_poor= pd.concat([town_info_poor, new_row_df], ignore_index=True)\n        \n    return town_info_rich, town_info_poor\n\nbirthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\n\ntown_info_rich, town_info_poor = find_town_info_all(birthplace_counts, 'birthplace')\n\nWe proceed with the following code to get the list of towns that sum up to 65 people from the richest towns, and 65 people from the poorest towns.\n\ndef get_towns_by_sum_pop(town_info, code_name):\n    \n    townsUsed = set()\n    peopleCount = 0\n\n    for index, row in town_info.iterrows():\n        \n        if peopleCount &gt; 65:\n            break\n        \n        name = row[code_name]\n        count = row['count']\n        townsUsed.add(name)\n        peopleCount += count\n    \n    return townsUsed, peopleCount\n\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')\n\n\n\nBirthplace\n\ndef get_av_prob_bp(townsUsed, code_name, bp_code):\n    \n    town_codes = []\n    bp_code_swapped = {value: key for key, value in bp_code.items()}\n\n\n    for town_full in townsUsed:\n        town_codes.append(bp_code_swapped[town_full])\n        \n    indices = X_test[code_name].isin(town_codes)\n    prob_subset = pred_prob[indices]\n    av_prob = np.mean(prob_subset[:, 1]) \n\n    return av_prob\n\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.33305156382079454 av_poor_prob:  0.31947027331642713\n\n\nWe find that there is not much difference in the average risk of diabetes when comparing poor and rich birthplace towns.\n\n\nCurrent Town of Residence\nCreate a dataframe with the information for rich and poor towns. Then get the list of towns that sum up to 65 people from the richest towns, and 65 people from the poorest towns.\n\ncurr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(curr_counts, 'curr_town')\n\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'curr_town')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'curr_town')\n\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n#HERE\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.25274725274725274 av_poor_prob:  0.2827087442472057\n\n\nIn this comparison, we find that people currently residing in rich towns have slightly lower rates of diabetes than those residing in poorer towns.\n\n\n\nPregnancy Analysis\nWe repeated the same exact process as above for each of our condition subsets.\nFinding the best model:\n\nnp.random.seed(567)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_pregnancy)\n\n\nnp.random.seed(567)\ntrain_model(X_train, y_train) \n\nbest LR : 0.9538094714060378\nbest DTC: 0.9632185172957705\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.9632185172957705\nbest max depth:  {'max_depth': 1}\nbest SVM:  0.9632185172957705\nbest score overall is:  0.9632185172957705  with model:  DTC\n\n\n\nCompute Average Risk scores\nPredict probabilities for all our entries using the best model we found\n\nDTC = DecisionTreeClassifier(max_depth=1)\nDTC.fit(X_train, y_train)\npred_prob = DTC.predict_proba(X_test)\n\n\n\nRace\n\npregRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    pregRaceRisk.append(newRow)\n\npregRaceRisk = pd.DataFrame(pregRaceRisk)\npregRaceRisk = pregRaceRisk.sort_values(by='risk', ascending=False)\npregRaceRisk\n\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n1\nblack\n0.051395\n\n\n2\nhispanic\n0.038217\n\n\n0\nasian\n0.037262\n\n\n3\nwhite\n0.034260\n\n\n\n\n\n\n\n\nHere we can see that being black gives a patient a little less than double the risk of pregnancy issues than being white. Hispanics have the second highest rate of pregnancy complications.\n\n\nGender\n\npregGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    pregGenderRisk.append(newRow)\n\npregGenderRisk = pd.DataFrame(pregGenderRisk)\npregGenderRisk = pregGenderRisk.sort_values(by='risk', ascending=False)\npregGenderRisk\n\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.074523\n\n\n1\nM\n0.000000\n\n\n\n\n\n\n\n\nThis result may seem a bit redundant or silly, it makes sense as generally people identified as male do not get pregnant.\n\n\nEthnicity\n\nav_risk_eth = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    av_risk_eth.append(new_row)\n\nav_risk_eth_df = pd.DataFrame(av_risk_eth)\nav_risk_eth_df = av_risk_eth_df.sort_values(by='risk', ascending=False)\n\n\nav_risk_eth_df\n\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n5\ndominican\n0.074523\n\n\n17\nscottish\n0.074523\n\n\n1\namerican\n0.054199\n\n\n3\ncentral_american\n0.053231\n\n\n19\nwest_indian\n0.049682\n\n\n8\nfrench_canadian\n0.049682\n\n\n12\nmexican\n0.049682\n\n\n14\nportuguese\n0.047908\n\n\n4\nchinese\n0.042585\n\n\n7\nfrench\n0.039746\n\n\n11\nitalian\n0.038269\n\n\n6\nenglish\n0.036060\n\n\n0\nafrican\n0.034395\n\n\n2\nasian_indian\n0.031939\n\n\n15\npuerto_rican\n0.031529\n\n\n18\nswedish\n0.029809\n\n\n10\nirish\n0.025262\n\n\n13\npolish\n0.024841\n\n\n9\ngerman\n0.023289\n\n\n16\nrussian\n0.014905\n\n\n\n\n\n\n\n\nHere we see that our finding that Black patients are more likely to experience pregnancy-related complications is driven largely by Dominican patients.\n\n\nBirthplace\n\nbirthplace_counts = X_test.groupby('birthplace').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(birthplace_counts, 'birthplace')\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'birthplace')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'birthplace')\n\n\nnp.random.seed(234)\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.038981469137448335 av_poor_prob:  0.03668844154112784\n\n\n\n\nCurrent Town of Residence\n\ncurr_counts = X_test.groupby('curr_town').size().reset_index(name='count')\ntown_info_rich, town_info_poor = find_town_info_all(curr_counts, 'curr_town')\nrichTownsUsed, richPeopleCount = get_towns_by_sum_pop(town_info_rich, 'curr_town')\npoorTownsUsed, poorPeopleCount = get_towns_by_sum_pop(town_info_poor, 'curr_town')\n\n\nnp.random.seed(234)\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.04586055192640981 av_poor_prob:  0.03630627027507444\n\n\nThis finding was somewhat surprising to us, in that wealthier towns were found to have higher risks of pregnancy complications. We discuss the potential implications of this result in our results section.\n\n\n\nCancer Analysis\n\nnp.random.seed(2)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_cancer)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\nnp.random.seed(500)\ntrain_model(X_train, y_train)\n\nbest LR : 0.9486775980338212\nbest DTC: 0.9546641722607386\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.9546641722607386\nbest max depth:  {'max_depth': 1}\nbest SVM:  0.9546641722607386\nbest score overall is:  0.9546641722607386  with model:  DTC\n\n\nOnce again we find that the model with the best score is DTC, The Decision Tree Classifier, with about 98% accuracy.\n\nDTC = DecisionTreeClassifier(max_depth=1)\nDTC.fit(X_train, y_train)\npred_prob = DTC.predict_proba(X_test)\n\n\nRace\n\ncancerRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    cancerRaceRisk.append(newRow)\n\ncancerRaceRisk = pd.DataFrame(cancerRaceRisk)\ncancerRaceRisk = cancerRaceRisk.sort_values(by='risk', ascending=False)\ncancerRaceRisk\n\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n3\nwhite\n0.051942\n\n\n2\nhispanic\n0.051650\n\n\n1\nblack\n0.046859\n\n\n0\nasian\n0.034009\n\n\n\n\n\n\n\n\nWe find across the board cancer rates are somewhat even, but that at the extremes white patients have almost a 50% risk of being classified with cancer,and Asian patients have around a 30% risk.\n\n\nGender\n\ncancerGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    cancerGenderRisk.append(newRow)\n\ncancerGenderRisk = pd.DataFrame(cancerGenderRisk)\ncancerGenderRisk = cancerGenderRisk.sort_values(by='risk', ascending=False)\ncancerGenderRisk\n\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n1\nM\n0.053825\n\n\n0\nF\n0.047147\n\n\n\n\n\n\n\n\nWomen are slightly less likely to have cancer.\n\n\nEthnicity\n\ncancerEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    cancerEthRisk.append(new_row)\n\ncancerEthRisk = pd.DataFrame(cancerEthRisk)\ncancerEthRisk = cancerEthRisk.sort_values(by='risk', ascending=False)\n\ncancerEthRisk\n\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n9\ngerman\n0.105675\n\n\n15\npuerto_rican\n0.067085\n\n\n0\nafrican\n0.067085\n\n\n4\nchinese\n0.062675\n\n\n11\nitalian\n0.059576\n\n\n6\nenglish\n0.057127\n\n\n13\npolish\n0.049934\n\n\n10\nirish\n0.049557\n\n\n14\nportuguese\n0.048342\n\n\n18\nswedish\n0.045475\n\n\n5\ndominican\n0.045475\n\n\n1\namerican\n0.041827\n\n\n3\ncentral_american\n0.034009\n\n\n7\nfrench\n0.032097\n\n\n2\nasian_indian\n0.005342\n\n\n8\nfrench_canadian\n0.005342\n\n\n12\nmexican\n0.005342\n\n\n19\nwest_indian\n0.005342\n\n\n16\nrussian\n0.005342\n\n\n17\nscottish\n0.005342\n\n\n\n\n\n\n\n\nOur results for ethnicity largely match the results we found distinguishing by race.\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.06399830132085002 av_poor_prob:  0.04238804096017698\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.03621368085712754 av_poor_prob:  0.05164958111475114\n\n\nWe note that for both birthplace towns and town of current residence, people in rich towns are more likely to get diagnosed with cancer as opposed to people from poorer towns. For current town of residence, the opposite is true.\n\n\n\nHeart Analysis\n\nnp.random.seed(210)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_heart)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\nnp.random.seed(20)\ntrain_model(X_train, y_train)\n\n/Users/sophie/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n/Users/sophie/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nbest LR : 0.87766773045743\nbest DTC: 0.8973478595796193\nbest max depth:  {'max_depth': 1}\nbest RFC:  0.8999156303877335\nbest max depth:  {'max_depth': None}\nbest SVM:  0.8973478595796193\nbest score overall is:  0.8999156303877335  with model:  RFC\n\n\n\nCompute Average Risk scores\nWe found that the best model to predict probabilities for all our entries iin this case would be RFC.\n\nRFC = RandomForestClassifier(random_state=0, max_depth=1)\nRFC.fit(X_train, y_train)\n\npred_prob = RFC.predict_proba(X_test)\n\n\n\n4. Compare Across Race, Gender, Ethnicity\n\n\nRace\n\nheartRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    heartRaceRisk.append(newRow)\n\nheartRaceRisk = pd.DataFrame(heartRaceRisk)\nheartRaceRisk = heartRaceRisk.sort_values(by='risk', ascending=False)\nheartRaceRisk\n\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.510746\n\n\n3\nwhite\n0.502101\n\n\n1\nblack\n0.491353\n\n\n2\nhispanic\n0.491280\n\n\n\n\n\n\n\n\nWe find that the demographic with the highest likelihood of having heart problems is Asian, but overall the results are fairly even.\n\n\nGender\n\nheartGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    heartGenderRisk.append(newRow)\n\nheartGenderRisk = pd.DataFrame(heartGenderRisk)\nheartGenderRisk = heartGenderRisk.sort_values(by='risk', ascending=False)\nheartGenderRisk\n\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.508013\n\n\n1\nM\n0.492275\n\n\n\n\n\n\n\n\nAccording to our results, women and men are equally likely to have heart conditions, which disagrees with real medical trends that show men are much more likely to have these conditions.\n\n\nEthnicity\n\nheartEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    heartEthRisk.append(new_row)\n\nheartEthRisk = pd.DataFrame(heartEthRisk)\nheartEthRisk = heartEthRisk.sort_values(by='risk', ascending=False)\n\nheartEthRisk\n\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n2\nasian_indian\n0.556735\n\n\n17\nscottish\n0.556144\n\n\n13\npolish\n0.553649\n\n\n12\nmexican\n0.534791\n\n\n14\nportuguese\n0.519123\n\n\n1\namerican\n0.516831\n\n\n9\ngerman\n0.516270\n\n\n16\nrussian\n0.511412\n\n\n18\nswedish\n0.508385\n\n\n6\nenglish\n0.500485\n\n\n0\nafrican\n0.500460\n\n\n7\nfrench\n0.498784\n\n\n8\nfrench_canadian\n0.494611\n\n\n11\nitalian\n0.490876\n\n\n19\nwest_indian\n0.489579\n\n\n10\nirish\n0.489033\n\n\n3\ncentral_american\n0.487097\n\n\n15\npuerto_rican\n0.482365\n\n\n5\ndominican\n0.480579\n\n\n4\nchinese\n0.464757\n\n\n\n\n\n\n\n\nAgain, here we see very little variation.\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.10892307692307693 av_poor_prob:  0.1103076923076923\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.08661538461538462 av_poor_prob:  0.09256410256410255\n\n\nIt seems as if there are not significant differences between the risk of heart diseases between wealthier and less-wealthy birthplace towns or current towns of residence.\n\n\n\nLungs Analysis\n\nnp.random.seed(400)\nX_train, y_train, X_test, y_test, race_code, eth_code, gen_code, bp_code, curr_code = prep_data(conditions_lungs)\n\n#getting rid of few NaN values\nX_train.fillna(0.0, inplace=True)\n#train the model\ntrain_model(X_train, y_train)\n\nbest LR : 0.5705770147830234\nbest DTC: 0.6107626279300099\nbest max depth:  {'max_depth': 5}\nbest RFC:  0.6210850665786289\nbest max depth:  {'max_depth': 4}\nbest SVM:  0.5971387696709585\nbest score overall is:  0.6210850665786289  with model:  RFC\n\n\n\nCompute Average Risk scores\nWe found that the best model to predict probabilities for all our entries iin this case would be RFC.\n\nRFC = RandomForestClassifier(random_state=0, max_depth=4)\nRFC.fit(X_train, y_train)\n\npred_prob = RFC.predict_proba(X_test)\n\n\n\n4. Compare Across Race, Gender, Ethnicity\n\n\nRace\n\nlungsRaceRisk = []\n\nfor code, race in race_code.items():\n    avRisk = find_risk(code, 'race', pred_prob)\n    newRow = {'race': race, 'risk': avRisk}\n    lungsRaceRisk.append(newRow)\n\nlungsRaceRisk = pd.DataFrame(lungsRaceRisk)\nlungsRaceRisk = lungsRaceRisk.sort_values(by='risk', ascending=False)\nlungsRaceRisk\n\n\n\n\n\n\n\n\n\nrace\nrisk\n\n\n\n\n0\nasian\n0.514338\n\n\n1\nblack\n0.509092\n\n\n3\nwhite\n0.507817\n\n\n2\nhispanic\n0.492106\n\n\n\n\n\n\n\n\n\n\nGender\n\nlungsGenderRisk = []\n\nfor code, gender in gen_code.items():\n    avRisk = find_risk(code, 'gender', pred_prob)\n    newRow = {'gender': gender, 'risk': avRisk}\n    lungsGenderRisk.append(newRow)\n\nlungsGenderRisk = pd.DataFrame(lungsGenderRisk)\nlungsGenderRisk = lungsGenderRisk.sort_values(by='risk', ascending=False)\nlungsGenderRisk\n\n\n\n\n\n\n\n\n\ngender\nrisk\n\n\n\n\n0\nF\n0.519215\n\n\n1\nM\n0.493550\n\n\n\n\n\n\n\n\n\n\nEthnicity\n\nlungsEthRisk = []\n\nfor code, name in eth_code.items():\n    av = find_risk(code, 'ethnicity', pred_prob)\n    new_row = {'eth': name, 'risk': av}\n    lungsEthRisk.append(new_row)\n\nlungsEthRisk = pd.DataFrame(lungsEthRisk)\nlungsEthRisk = lungsEthRisk.sort_values(by='risk', ascending=False)\n\nlungsEthRisk\n\n\n\n\n\n\n\n\n\neth\nrisk\n\n\n\n\n13\npolish\n0.583031\n\n\n12\nmexican\n0.580761\n\n\n17\nscottish\n0.577455\n\n\n2\nasian_indian\n0.577379\n\n\n18\nswedish\n0.560368\n\n\n0\nafrican\n0.559980\n\n\n16\nrussian\n0.549236\n\n\n1\namerican\n0.548145\n\n\n9\ngerman\n0.521056\n\n\n14\nportuguese\n0.516368\n\n\n6\nenglish\n0.514735\n\n\n11\nitalian\n0.498196\n\n\n8\nfrench_canadian\n0.487928\n\n\n7\nfrench\n0.485426\n\n\n10\nirish\n0.481814\n\n\n15\npuerto_rican\n0.479351\n\n\n5\ndominican\n0.473627\n\n\n3\ncentral_american\n0.463493\n\n\n19\nwest_indian\n0.457941\n\n\n4\nchinese\n0.451298\n\n\n\n\n\n\n\n\n\n\nBirthplace\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'birthplace', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'birthplace', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.4872429058942045 av_poor_prob:  0.5189382015534418\n\n\n\n\nCurrent Town of Residence\n\nav_rich_prob = get_av_prob_bp(richTownsUsed, 'curr_town', bp_code)\nav_poor_prob = get_av_prob_bp(poorTownsUsed, 'curr_town', bp_code)\n\nprint(\"av_rich_prob: \", av_rich_prob, \"av_poor_prob: \", av_poor_prob)\n\nav_rich_prob:  0.5039833131472166 av_poor_prob:  0.5124758651408807\n\n\nThe results for the risk score for patients of different race, gender, ethnicity, birthplace town, and current town of residence are curious as every risk score hovers around a 0.5."
  },
  {
    "objectID": "posts/penguins/penguinblog.html",
    "href": "posts/penguins/penguinblog.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post is to identify a machine learning model that can classify penguins as one of three species, Gentoo, Chinstrap, and Adelie, with 100% accuracy. I explore five different algorithms of classification: logistic regression, decision trees, random forests, support vector machine, and k-nearest neighbors. I use cross validation and grid-search cross validation to find the most accurate measurement of each model’s effectiveness, and to avoid overfitting. I then compared the results of all five modelsto identify the most accurate model overall, which was a logistic regression model trained on either island, culmen length, culmen depth, or sex, culmen length, and culment depth. In the end I chose to train my final model on island, culmen length, and culmen depth, due to concerns about the real-world value of sex as a predictor for penguins."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#abstract",
    "href": "posts/penguins/penguinblog.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The aim of this post is to identify a machine learning model that can classify penguins as one of three species, Gentoo, Chinstrap, and Adelie, with 100% accuracy. I explore five different algorithms of classification: logistic regression, decision trees, random forests, support vector machine, and k-nearest neighbors. I use cross validation and grid-search cross validation to find the most accurate measurement of each model’s effectiveness, and to avoid overfitting. I then compared the results of all five modelsto identify the most accurate model overall, which was a logistic regression model trained on either island, culmen length, culmen depth, or sex, culmen length, and culment depth. In the end I chose to train my final model on island, culmen length, and culmen depth, due to concerns about the real-world value of sex as a predictor for penguins."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#data-preperation",
    "href": "posts/penguins/penguinblog.html#data-preperation",
    "title": "Classifying Palmer Penguins",
    "section": "Data Preperation",
    "text": "Data Preperation\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\n\nX_train.head()\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n\n\n\n\n\n\n\n# changing species name for plotting ease\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)"
  },
  {
    "objectID": "posts/penguins/penguinblog.html#data-exploration",
    "href": "posts/penguins/penguinblog.html#data-exploration",
    "title": "Classifying Palmer Penguins",
    "section": "Data Exploration",
    "text": "Data Exploration\n\n1. SCATTER PLOT – Delta 15 N and Delta 13 C\nFor my first graph, I chose to explore how delta 15 N values and delta 13 C values vary according to species.\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"darkgrid\")\nsns.scatterplot(data=train, x='Delta 15 N (o/oo)', y='Delta 13 C (o/oo)', hue='Species', palette = \"magma\")\nplt.title(\"Delta 15 N and Delta 13 C Values by Species\")\nplt.legend(title=\"Species\", loc='upper left')\n\n\n\n\n\n\n\n\nThis graph shows the delta 15 N values and the delta 13 C values by species. Delta 15 N is a measure of the ratio between two stable nitrogen isotopes, nitrogen-15 and nitrogen-14. In biological contexts, this measure is often used to relative food chain position of an organism, with higher a higher value indicating a higher trophic position. Delta 13 C is a measure of the ratio between two stable carbon isotopes, carbon-13 and carbon-12. Delta 13 C is used in biology to analyze the diet of an organism, and can also be used to determine its trophic levels, with a higher delta 13 C value indicating greater productivity. Because chinstrap penguins have on average both a higher delta 15 n and delta 13 c value, this could indicate they exist slightly at a slightly higher trophic level, and consume food in a climate with a distinct inclination towards plants containing one carbon signature over the other. On the other hand adelie penguins seem to inhabit mostly an environment with a more balanced mix of the two in relation to chinstrap penguins, and gentoo penguins seems to be relatively skewed towards the other carbon signature. This would also indicate that adelie penguins are slightly lower in trophic level than chinstrap, and gentoo penguins even more so. For further exploration this raises the question of both relative sizes (body mass) of the penguins, and whether this agrees with the relative trophic levels seen here, and the islands on which the penguins are more likely to exist. For example does one island contain mostly chinstrap penguins and plants of one carbon signature while the other has mostly gentoo penguins and plants containing more of the other carbon signature.\n\n\n2. VIOLIN PLOT: BODY MASS BY SPECIES\nFor my second graph I wanted to explore the body mass distribution within the different species.\n\nsns.violinplot(data = train, x=\"Species\", y=\"Body Mass (g)\", palette = \"viridis\")\nplt.title(\"Body Mass by Species\")\nsns.pointplot(x=\"Species\", y=\"Body Mass (g)\", data=train, estimator='mean', color='red', markers='o', linestyles='')\n\n\n\n\n\n\n\n\nThis plot examines the body masses, and their relative frequency, within the three penguin species. The chinstrap figure shows that most chinstrap penguins fall within the interquartile range, and are very likely to exist near the median of the overall species body mass (shown in red). This contrasts somewhat with the adelie and gentoo penguins, who are not quite as densely centered around the median body mass of their species. This could be helpful in identifying chinstrap penguins in particular, as this tells us that a penguin with a body mass largely different from the chinstrap median is much less likely to be a chinstrap than a gentoo or an adelie. This plot is also interesting in terms of the derived trophic levels from the first figure, as it does not intuitively agree with the relative levels shown there. This could tell us that delta 15 N and delta 13 C might be less valuale predictors than we thought, as the patterns shown using these predictors are not necessarily reflected in other predictors.\n\n\n3. SUMMARY TABLE: SPECIES BY ISLAND\n\npd.crosstab(train['Species'], train['Island'])\n\n\n\n\n\n\n\n\nIsland\nBiscoe\nDream\nTorgersen\n\n\nSpecies\n\n\n\n\n\n\n\nAdelie\n33\n45\n42\n\n\nChinstrap\n0\n57\n0\n\n\nGentoo\n98\n0\n0\n\n\n\n\n\n\n\n\nThis table shows the relative counts of each of the three penguin species on the three islands surveyed. This table agrees with the patterns in delta 13 C values we saw in the first plot, as chinstrap and gentoo are found exclusively on Dream and Biscoe island respectively, which would confirm the hypothesis that the delta 13 C values vary according to a difference in the available plants carbon signatures. Adelie penguins, on the other hand, are found at all three islands, but also are the only penguins on Torgersen island, speaking to their relatively large variation in delta 13 C values."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#creating-the-model",
    "href": "posts/penguins/penguinblog.html#creating-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Creating the Model",
    "text": "Creating the Model\nFirst I created a dataframe that would later store the best, cross-validated values for each of the model types.\n\n# creating dataframe to keep track of the max accuracies/best predictors for each algorithm\nbest_tracker = pd.DataFrame(columns = ['Logistic_Regression', 'Decision_Tree', 'Random_Forest', 'SVC', 'KNN'])\nbest_tracker.head()\n\n\n\n\n\n\n\n\n\nLogistic_Regression\nDecision_Tree\nRandom_Forest\nSVC\nKNN\n\n\n\n\n\n\n\n\n\n\nThen, using Professor Chodrow’s code from the assignment description that generates every possible combination of two quantitative features and one qualitative feature, I created a loop that ran through the process of both fitting and cross-validating each model individually and storing the maximum result (in terms of accuracy and tuning parameters) to the previously created dataframe. For models with tuning parameters I used GridSearchCV to test many possible options for these parameters before finding the best one.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC \nimport numpy as np\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", \"Stage_Adult, 1 Egg Stage\"] # qualitative columns\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)'] # quantitative columns\n\n# initialize all the trackers of the best model \nbest_tree_score = 0\nbest_svc_score = 0\nbest_lr_score = 0\nbest_forest_score = 0\nbest_knn_score = 0\n\nbest_tree_depth = None\nbest_forest_depth = None\nbest_gamma = None\nbest_knn_neigh = None\n\nbest_tree_cols = []\nbest_svc_cols = []\nbest_lr_cols = []\nbest_forest_cols = []\nbest_knn_cols = []\n\n#######################\n\n# for each combination of 1 qualitative predictor and 2 quantitative predictors\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    # LOGISTIC REGRESSION\n    LR = LogisticRegression(max_iter=200000000000000) # was running into max. iterations reached error\n    LR.fit(X_train[cols], y_train)\n    cv_scores_LR = cross_val_score(LR, X_train[cols], y_train, cv=5) \n    curr_score = cv_scores_LR.mean() # mean of the cross valid. scores is considered the best score we can get w/ the model\n    \n    # update vars if necessary\n    if curr_score &gt; best_lr_score:\n      best_lr_score = curr_score\n      best_lr_cols = cols\n    \n    # DECISION TREE\n    param_grid = {\n    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None] \n    } # None means no maximum depth, try a max. depth. Testing all depths from 1 to 10 (the total number of features) and no max. depth.\n    tree = DecisionTreeClassifier()\n    grid_search = GridSearchCV(tree, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    curr_score = grid_search.best_score_ # mean of the cross valid. scores is considered the best score we can get w/ the model\n\n    # update vars if necessary\n    if curr_score &gt; best_tree_score:\n      best_tree_score = curr_score\n      best_tree_cols = cols\n      best_tree_depth = grid_search.best_params_\n\n    # RANDOM FOREST\n    param_grid = {\n    'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, None]\n    } # same idea for max. depth as the decision tree\n    forest = RandomForestClassifier(random_state=0)\n    grid_search = GridSearchCV(forest, param_grid, cv=5)\n    grid_search.fit(X_train[cols], y_train)\n    curr_score = grid_search.best_score_ # mean of the cross valid. scores is considered the best score we can get w/ the model\n\n    # update vars if necessary\n    if curr_score &gt; best_forest_score:\n      best_forest_score = curr_score\n      best_forest_cols = cols\n      best_forest_depth = grid_search.best_params_\n\n    # SVC\n    g = {'gamma': 10.0**np.arange(-5, 5)} # testing all gammas in the range given in the blog post instructions\n    grid_search = GridSearchCV(SVC(),g, cv = 5)\n    grid_search.fit(X_train[cols],y_train)\n    curr_score = grid_search.best_score_ # mean of the cross valid. scores is considered the best score we can get w/ the model\n\n    # update vars if necessary\n    if curr_score &gt; best_svc_score:\n      best_svc_score = curr_score\n      best_svc_cols = cols\n      best_gamma = grid_search.best_params_\n\n    # K-NEAREST NEIGHBORS\n    param_grid = {\n    'n_neighbors': [5, 6, 7, 8, 9, 10]  \n    }  # testing number of nearest neighbors from 5 to 10, didn't want groups to be too specialized or too generalized\n    KNN = KNeighborsClassifier(n_neighbors=5)\n    grid = GridSearchCV(KNN, param_grid, cv=5)\n    grid.fit(X_train[cols], y_train)\n    curr_score = grid.best_score_ # mean of the cross valid. scores is considered the best score we can get w/ the model\n\n    # update vars if necessary\n    if curr_score &gt; best_knn_score:\n      best_knn_score = curr_score\n      best_knn_cols = cols\n      best_knn_neigh = grid.best_params_\n    \n# update dataframe using the tracker variables for each model\nbest_tracker['Decision_Tree'] = [best_tree_score, best_tree_depth, best_tree_cols]\nbest_tracker['Logistic_Regression'] = [best_lr_score, None, best_lr_cols]\nbest_tracker['SVC'] = [best_svc_score, best_gamma, best_svc_cols]\nbest_tracker['Random_Forest'] = [best_forest_score, best_forest_depth, best_forest_cols]\nbest_tracker['KNN'] = [best_knn_score, best_knn_neigh, best_knn_cols]\n\n\n/opt/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of f AND g EVALUATIONS EXCEEDS LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nThe final dataframe shows the best results for each model, which shows that logistic regression was the most accurate model, with the parameters sex, culmen length, and culmen depth.\n\n# best results for each of the five models\nbest_tracker.head()\n\n\n\n\n\n\n\n\n\nLogistic_Regression\nDecision_Tree\nRandom_Forest\nSVC\nKNN\n\n\n\n\n0\n0.988311\n0.976546\n0.984465\n0.980543\n0.976621\n\n\n1\nNone\n{'max_depth': 5}\n{'max_depth': 5}\n{'gamma': 0.1}\n{'n_neighbors': 5}\n\n\n2\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Cul...\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Cul...\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Fli...\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Cul...\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Cul..."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#testing-the-model",
    "href": "posts/penguins/penguinblog.html#testing-the-model",
    "title": "Classifying Palmer Penguins",
    "section": "Testing the Model",
    "text": "Testing the Model\n\n# read in and prepare test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nX_test, y_test = prepare_data(test)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\nRetraining and testing model with selected columns (1)\n\ncols = [\"Sex_FEMALE\", \"Sex_MALE\", \"Culmen Length (mm)\", \"Culmen Depth (mm)\"]\nLR1 = LogisticRegression(max_iter=20000000000000)\nLR1.fit(X_train[cols], y_train)\nLR1.score(X_test[cols], y_test)\n\n0.9852941176470589\n\n\nNOTE: the last time I ran my code the parameter of sex gave me less than a 100% accuracy rate despite previously being 100%, but I moved ahead anyways because I chose island, which I explain below, and the model trained on those features still reached 100% accuracy.\nOut of curiosity, I created a version of the dataframe that saved the best model and parameters for all of the features, which I have removed because of how long the code was and because I manually went through it to confirm a hypothesis. My hypothesis was that sex was not actually one of the best features for prediction, despite the dataframes finding, unless the data was skewed towards one sex or the other for a certain species, as biologically it says nothing about the species of a penguin to know its sex. When I looked through the data, I found that the logistic regression model using the island feature had an identical accuracy to the model fit to sex. This told me that the dataframe chose sex simply because they were tied and it was the latter one run, meaning the more recent model to reach the max accuracy. Scientifically, it makes more sense to train on island, as we’ve seen that certain penguins do inhabit certain islands at different rates, rather than sex which seems more like a quirk of the model. Therefore, I also fit a model to these three parameters and scored the predictions of this model.\n\n# retraining and testing model with selected columns (2)\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nLR = LogisticRegression(max_iter=20000000000000)\nLR.fit(X_train[cols], y_train)\nLR.score(X_test[cols], y_test)\n\n1.0\n\n\nI reached 100% accuracy using logisitic regression with the predictions parameters of sex, culmen length, and culmen depth, AND seperately with the prediction parameters of island, culmen length, and culment depth. Based on the context of predicting species, where sex is not a realistically valuable feature, I chose to move on using island instead."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#plotting-decision-regions",
    "href": "posts/penguins/penguinblog.html#plotting-decision-regions",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting Decision Regions",
    "text": "Plotting Decision Regions\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nBelow is the code I used to graph the decision regions, adapted from Professor Chodrow’s lecture notes.\n\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nThe decision regions are shown split for each island, since they are seperated into three quantitative parameters, and the x-axis is the culmen length while the y-axis is the culmen depth.\n\n# plotting decision boundaries for model trained with island parameter\ncols = [\"Culmen Length (mm)\", \"Culmen Depth (mm)\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nplot_regions(LR, X_train[cols], y_train)"
  },
  {
    "objectID": "posts/penguins/penguinblog.html#confusion-matrix",
    "href": "posts/penguins/penguinblog.html#confusion-matrix",
    "title": "Classifying Palmer Penguins",
    "section": "Confusion Matrix",
    "text": "Confusion Matrix\n\n# creating confusion matrix\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ny_pred = LR.predict(X_test[cols])\nclasses = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n\n\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\ndisp.plot(cmap=plt.cm.Greens, colorbar=False)\nplt.title(\"Confusion Matrix for Logistic Classification Model\")\nplt.show()\n\n\n\n\n\n\n\n\nThe confusion matrix shows exactly what we would expect with a 100% accurate model, which is that all examples of each of the three species (31 for Adelie, 11 for Chinstrap, 26 for Gentoo) are correctly identified as their respective species."
  },
  {
    "objectID": "posts/penguins/penguinblog.html#discussion",
    "href": "posts/penguins/penguinblog.html#discussion",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion",
    "text": "Discussion\nThe findings of this blogpost include a 100% accurate logistic classification model for determining whether a penguin, based on its sex/island, culmen length, and culmen depth, is an Adelie, Chinstrap, or Gentoo penguin. In order to determine the most accurate model, I examined all combinations of one qualitative and two quantitative features, and five different models: logistic regression, decision trees, random forests, SVC, and K-nearest neighbors. I stored the results of the best version of each model, found using either regular cross-validation (logistic regression), or grid search cross validation (in order to determine the best values for the tuning factors of each model), in a dataframe, and then was able to compare the accuracy across models. Interestingly, all the models chose the columns of sex, culmen depth, and culmen length, as the most valuable predictors for the data. This was concerning to me, as sex is not usually a useful species differentation, and made me wonder whether this meant that our penguin data was skewed towards either male or female for one species of penguin or another. Upon examining the results for each column individually, I found that logistic regression performed equally as accurately when predicting based on island, culmen length, and culment depth. Creating models trained seperately on these two different sets of predictors showed that both were 100% accurate on the testing data. Because of my suspicion about using sex as a predictor of species, I chose to graph the boundaries and confusion matrix only for the model trained on island instead of sex, assuming it would be more generalizable if it were to be applied to different datasets.\nThrough the process of fitting and analyzing the various models I created I learned a lot about the standard machine learning workflow in Python, and became much more comfortable incoorporating new functions and modules that we hadn’t previously discussed in class. For example, I remembered K-nearest neighbors from my statistical learning class, and felt like it would be an effective algorithm for classifying penguin species, and so I sought out whether there was a built-in function that would allow me to implement it as one of the choices in my analysis."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html",
    "href": "posts/logistic/LogisticBlogPost.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Link to my logistic regression implementation."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#abstract",
    "href": "posts/logistic/LogisticBlogPost.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implement logistic regression, a fundamental algorithm of of machine learning, and perform experiments to explore the accuracy and behavior of my implementation. I implement vanilla gradient descent, and a variant of gradient descent known as gradient descent with momentum, aimed at achieving faster convergence. I conduct experiments and create visualizations to validate my implementation, examining the behavior of vanilla gradient descent versus gradient descent with momentum and addressing the phenomenon of overfitting. Through experimentation and analysis, I gained insights into the behavior and performance of logistic regression models under various conditions, as well as best practices when using the algorithm to make predictions.\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer"
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#vanilla-gradient-descent",
    "href": "posts/logistic/LogisticBlogPost.html#vanilla-gradient-descent",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent",
    "text": "Vanilla Gradient Descent\nThe first experiment I performed was implementing vanilla gradient descent using my model. This form of gradient descent involves ignoring beta as a paramater (setting it to 0), and instead only using alpha as a learning parameter. With these parameters and 2-dimensional data I hoped to observe a monotonous decrease in loss over time, eventually, over 1000 iterations, becoming very small, which would show that our model is learning and becoming better at separating our data.\nI used the classification_data function from the blog post description to create an instance of 2-D data for our model.\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\n\n# DO NOT RE-RUN\n\nX, y = classification_data(noise = 0.5)\n\nBelow I plotted the data in order to visualize the separation.\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# adapted from perceptron blog post\ndef plot_classification_data(X, y, ax):\n    plt.figure(figsize=(10, 6))\n    targets = [0, 1]\n    markers = [\"o\" , \"X\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \n    \nfig, ax = plt.subplots(1, 1)\n\n# create and plot data\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nNext, I created a logistic regression instance, and ran a 6000 iteration training loop over my generated data. I chose 6000 after experimenting to see when the line began to level out and make slower progress.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\n\n# store each iterations loss in a vector, update weights each time for 1000 times\nfor _ in range(6000):\n    loss = LR.loss(X, y)\n    loss_vec.append(loss)\n    opt.step(X, y, a = 0.6, b=0)\n\n\n# taken from the perceptron blog post\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\nfig, ax = plt.subplots(1, 1)\n\n\n# plot data\nplot_classification_data(X, y, ax)\n\n# plot best dividing line\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2));\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nAs we can see, the line our model found does a very good job of estimating the divide between our two data groups. Our final loss was around 0.003. Next, I plotted the change in our loss over time using our loss vector.\n\n# taken from perceptron blog post, plots loss over iterations\ndef plot_loss(lvec):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(lvec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(lvec)), lvec, color = \"slategrey\", s=5)\n    plt.gca().set(xlabel = \"Logistic Regression Steps\", ylabel = \"Loss\")\n    \n    plt.title(f\"Final loss: {lvec[len(lvec)-1]:.3f}\")\n\nplot_loss(loss_vec);\n\n\n\n\n\n\n\n\nWe see that the loss changes as we would hope, in that it does in fact decrease with our iterations, and starts to converge at low numbers as our iterations increase."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#benefits-of-momentum",
    "href": "posts/logistic/LogisticBlogPost.html#benefits-of-momentum",
    "title": "Implementing Logistic Regression",
    "section": "Benefits of Momentum",
    "text": "Benefits of Momentum\nThe next experiment I performed was to use a non-zero beta value to explore momentum. What we expect to see is our model converging faster (in fewer iterations) to the same or smaller loss values. I chose 3000 iterations as I wanted to see how few iterations I could run the model for in order to acheive the same result as our first model.\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec_m = []\n\n# a = 0.06, b = 0.05\n# took some experimentation because values too high caused loss to become so small it wouldn't format as a number\nfor _ in range(3000):\n    loss = LR.loss(X, y)\n    loss_vec_m.append(loss)\n    opt.step(X, y, a = 0.6, b=0.5)\n\n# plot data and loss\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2));\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nWe see here that we do in fact end with the same loss value in 3000 iterations that our training without momentum found in 6000 iterations. Next, I plotted the loss over iterations of the model with momentum over the loss over iterations of our vanilla model to see whether the loss decreased faster than the vanilla gradient descent loss.\n\nimport numpy as np\n\nplt.figure(figsize=(10,6))\n\n# plotting vanilla\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\", s=5)\nplt.gca().set(xlabel = \"Logistic Regression Steps\", ylabel = \"Loss\")\n\n# plotting with momentum\nplt.plot(loss_vec_m, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec_m)), loss_vec_m, color = \"red\", s=5)\nplt.gca().set(xlabel = \"Logistic Regression Steps\", ylabel = \"Loss\");\n\n# plotting points where y = 0.0035\nplt.scatter(5778, loss_vec[5777].item(), color=\"blue\", label=f'Vanilla: ({5778}, {0.0035})', s=15, zorder=3)\nplt.scatter(2884, loss_vec[2883].item(), color=\"orange\", label=f'Momentum: ({2884}, {0.0035})', s=15, zorder=3)\n\nplt.legend()\n\n\nplt.show()\n\n\n\n\n\n\n\n\nIn this graph, the red line shows the model trained with momentum, and the grey line shows the vanilla model. It’s clear that the red line is moving towards convergence at or around zero faster than the grey line. As we saw in the model training, both models were able to reach a final (rounded) loss of 0.003 in the given iterations. In order to visualize the difference in speeds between the two models, I plotted where each model first reached a value \\(\\approx\\) 0.0035 (my cut off was anything less than 0.0036, which would then feasibly be rounded down to 0.003), these points are shown in orange and blue. We can see that the red line, the orange dot, reached 0.0035 in 2,884 iterations, while the grey line (the blue dot) took almost double the iterations to reach the same loss, 5,778. I decided to stop the momentum line in fewer iterations than the vanilla model also partly because of visualizations issues, as the lines start to look indistinguishable at very small values, but I found that 3000 iterations was enough to show the steeper initial slope of the momentum model, and to show the earlier appearance of the value our vanilla model settled on in a larger number of iterations."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#overfitting",
    "href": "posts/logistic/LogisticBlogPost.html#overfitting",
    "title": "Implementing Logistic Regression",
    "section": "Overfitting",
    "text": "Overfitting\nThe next experiment I performed was to examine the effects of overfitting our model on training and testing data. I created two sets of data, one training and one testing, both with 100 dimensions (features) and 50 data points. This creates a dataset with many more features than actual instances, which is very susceptible to overfitting.\n\nX_train,y_train = classification_data(n_points = 50, noise = 0.2, p_dims = 100)\nX_test,y_test = classification_data(n_points = 50, noise = 0.2, p_dims = 100)\n\nI trained the model on our training data with varying alpha, beta values until I found a training accuracy of 100%. I looped my training process untill I reached a loss of at most 0.005, which to me was close enough to zero to be considered fully trained.\n\nfrom sklearn.metrics import accuracy_score\n\nLR = LogisticRegression()\nopt = GradientDescentOptimizer(LR)\n\nloss_vec = []\nloss = 1.0\n\n# train model\nwhile loss &gt; 0.005:\n    loss = LR.loss(X_train, y_train)\n    loss_vec.append(loss)\n    opt.step(X_train, y_train, a = 0.06, b=0.9)\n\n# generate predictions\ny_train_pred = LR.predict(X_train)\n# calculate training accuracy using scikitlearn function\ntrain_acc = accuracy_score( y_train.numpy(), y_train_pred.numpy())\nprint(train_acc)\n\n1.0\n\n\nTo visualize the complete training of our model, I also plotted the loss over iterations.\n\nimport math\ncleaned_list = [x for x in loss_vec if not math.isnan(x)]\nplot_loss(cleaned_list)\n\n\n\n\n\n\n\n\nThen I used the trained model to generate predictions and accuracy for our testing data.\n\n# generate predictions\ny_test_pred = LR.predict(X_test)\n# calculate testing accuracy using scikitlearn function\naccuracy_test = accuracy_score(y_test.numpy(), y_test_pred.numpy())\nprint(accuracy_test)\n\n0.96\n\n\nAs we can see, despite acheiving 100% accuracy on our training data, we can only acheive 96% accuracy on our testing data, speaking to the drawbacks of overfitting a model to training instances."
  },
  {
    "objectID": "posts/logistic/LogisticBlogPost.html#discussion",
    "href": "posts/logistic/LogisticBlogPost.html#discussion",
    "title": "Implementing Logistic Regression",
    "section": "Discussion",
    "text": "Discussion\nThese experiments on my implementation of logistic regression revealed intriguing insights into the behavior and performance of logistic regression models trained using gradient descent with/without momentum, and into overfitting with logistic regression. Firstly, my findings demonstrated that gradient descent with momentum outperforms vanilla gradient descent in terms of convergence speed. By incorporating momentum, the optimization process exhibits faster convergence towards an optimal solution (smaller loss). This observation underscores the effectiveness of momentum as a tool for logistic regression models. The experiments also shed light on the phenomenon of overfitting in logistic regression models. When the number of dimensions exceeds the number of data points, logistic regression models are susceptible to overfitting, as demonstrated by the attainment of 100% accuracy on the training data while achieving less optimal performance on testing data. This highlights the importance of model evaluation strategies to mitigate the risk of overfitting and ensure the generalization ability of the trained model. Overall, by understanding the nuances of optimization techniques and model behavior, I feel more able to make informed decisions that enhance the performance and robustness of logistic regression models for various classification tasks."
  },
  {
    "objectID": "posts/FinalProject/DataCleaning.html",
    "href": "posts/FinalProject/DataCleaning.html",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "",
    "text": "import pandas as pd\nfrom sklearn.preprocessing import LabelEncoder"
  },
  {
    "objectID": "posts/FinalProject/DataCleaning.html#read-in-data",
    "href": "posts/FinalProject/DataCleaning.html#read-in-data",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Read in data",
    "text": "Read in data\n\nallergies = pd.read_csv('allergies.csv')\ncareplans = pd.read_csv('careplans.csv')\nconditions = pd.read_csv('conditions.csv')\nencounters = pd.read_csv('encounters.csv') ##NOT USING RN, DO WE NEED?\nimmunizations = pd.read_csv('immunizations.csv')\nmedications = pd.read_csv('medications.csv')\nobservations = pd.read_csv('observations.csv')\npatients = pd.read_csv('patients.csv')\nprocedures = pd.read_csv('procedures.csv')\n\n\nlen(patients[patients['ethnicity'] == 'scottish']['ethnicity'])\n\n27"
  },
  {
    "objectID": "posts/FinalProject/DataCleaning.html#clean-up-dataframes-have-one-row-per-patient",
    "href": "posts/FinalProject/DataCleaning.html#clean-up-dataframes-have-one-row-per-patient",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Clean up dataframes: have one row per patient",
    "text": "Clean up dataframes: have one row per patient\n\n## ALLERGIES\nallergies_pivot = pd.get_dummies(allergies['DESCRIPTION'])\nallergies_pivot['PATIENT'] = allergies['PATIENT']\nallergies_pivot = allergies_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## CAREPLANS\ncareplans_pivot = pd.get_dummies(careplans['DESCRIPTION'])\ncareplans_pivot['PATIENT'] = careplans['PATIENT']\ncareplans_pivot = careplans_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## CONDITIONS\nconditions_pivot = pd.get_dummies(conditions['DESCRIPTION'])\nconditions_pivot['PATIENT'] = conditions['PATIENT']\nconditions_pivot = conditions_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## IMMUNIZATIONS\nimmunizations_pivot = pd.get_dummies(immunizations['DESCRIPTION'])\nimmunizations_pivot['PATIENT'] = immunizations['PATIENT']\nimmunizations_pivot = immunizations_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## MEDICATIONS\nmedications_pivot = pd.get_dummies(medications['DESCRIPTION'])\nmedications_pivot['PATIENT'] = medications['PATIENT']\nmedications_pivot = medications_pivot.groupby('PATIENT').sum().reset_index()\n\n\n## OBSERVATIONS\nobservations['VALUE'] = pd.to_numeric(observations['VALUE'], errors='coerce')\n\n# Pivot table with mean aggregation\nobservations_pivot= observations.pivot_table(index=observations.index, columns='DESCRIPTION', values='VALUE', fill_value=0, aggfunc='mean')\nobservations_pivot['PATIENT'] = observations['PATIENT']\nobservations_pivot = observations_pivot.groupby('PATIENT').sum().reset_index()\n\nobservations_pivot.to_csv('observations_pivot.csv', index=False)\n\n\n## PROCEDURES\nprocedures_pivot = pd.get_dummies(procedures['DESCRIPTION'])\nprocedures_pivot['PATIENT'] = procedures['PATIENT']\nprocedures_pivot = procedures_pivot.groupby('PATIENT').sum().reset_index()\n\n\nREMOVE MA US from Patient info (they are all from mass)\n\n#import re\nplaces = patients['birthplace']\ncleaned_places = [place.replace(\" MA US\", \"\") if \" MA US\" in place else place for place in places]\n\n\npatients['birthplace'] = cleaned_places\n\n\ntown_names = patients['birthplace'].unique().tolist()\n#some towns do not appear in birthplace but appear in current addresses, I modified the function below to print these towns and create this list, then\n# reverted to the function so it wouldn't have a long output\ntowns_not_in_bp = ['Sandwich', 'Uxbridge', 'Holland', 'Monson', 'Wendell', 'Wayland', 'Rochester', 'Belchertown', 'Lanesborough', 'Hatfield',\n                  'Georgetown', 'Lakeville', 'Princeton', 'Blackstone', 'Hinsdale', 'Harvard', 'Chesterfield', 'Wellfleet', 'Northfield', 'Hubbardston',\n                  'Windsor', 'Wales', 'Sandisfield', 'Bolton', 'Truro', 'Southwick', 'Sheffield', 'Scituate', 'Halifax', 'Nahant', 'Stockbridge', 'Berlin']\n                \nfor town in towns_not_in_bp:\n    town_names.append(town)\n\n\ndef extract_town_name(address, town_list):\n    # Check if any part of the address matches any town name in the list\n    for town in town_list:\n        if town in address:\n            return town\n    print(address)\n    return None\n\npatients['curr_town'] = \"\"\n\nfor index, address in enumerate(patients['address']):\n    # Extract the town name\n    town_name = extract_town_name(address, town_names)\n    \n    # Assign the extracted town name to the corresponding entry in the 'curr_town' column\n    patients.at[index, 'curr_town'] = town_name\n\n\npatients.to_csv('patient_clean.csv', index=False)"
  },
  {
    "objectID": "posts/FinalProject/DataCleaning.html#add-suffixes-to-columns",
    "href": "posts/FinalProject/DataCleaning.html#add-suffixes-to-columns",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Add suffixes to columns",
    "text": "Add suffixes to columns\n\npatients = patients.rename(columns={'patient': 'PATIENT'})\n\ndef add_suffix(df, suffix):\n    renamed_columns = {}\n    for col_name in df.columns:\n        if col_name != 'PATIENT':\n            renamed_columns[col_name] = col_name + '_' + suffix\n        else:\n            renamed_columns[col_name] = col_name\n    return df.rename(columns=renamed_columns)\n\nallergies_clean = add_suffix(allergies_pivot, 'ALLERGIES')\ncareplans_clean = add_suffix(careplans_pivot, 'CAREPLANS')\nconditions_clean = add_suffix(conditions_pivot, 'CONDITIONS')\nimmunizations_clean = add_suffix(immunizations_pivot, 'IMMUNIZATIONS')\nmedications_clean = add_suffix(medications_pivot, 'MEDICATIONS')\nobservations_pivot = add_suffix(observations_pivot, 'OBSERVATIONS')\nprocedures_clean = add_suffix(procedures_pivot, 'PROCEDURES')"
  },
  {
    "objectID": "posts/FinalProject/DataCleaning.html#prep-data",
    "href": "posts/FinalProject/DataCleaning.html#prep-data",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Prep Data",
    "text": "Prep Data\n\nle = LabelEncoder()\n\n# our data-prepping function for modeling\ndef prep_data(patients, conditions, illness_descriptions, observations):\n\n    # make patients column match others for merging, drop unnecessary information and NA vals\n    patients.rename(columns={'patient':'PATIENT'}, inplace=True)\n    patients = patients.drop(columns=['birthdate', 'marital','deathdate','ssn', 'address', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden'])\n    patients = patients.dropna()\n    conditions = conditions.dropna()\n\n    # merge datasets (patient info and corresponding conditions)\n    merged_df = pd.merge(patients, conditions, on='PATIENT', how='left')\n    merged_df = pd.merge(merged_df, observations, on='PATIENT', how='left')\n\n    # print(observations)\n    # create y\n    merged_df[\"y\"] = (merged_df[illness_descriptions] == 1).any(axis=1).astype(int)\n    merged_df = merged_df.drop(columns=illness_descriptions)\n    \n    # return split x, y, and all of the code tracking dicts\n    return merged_df"
  },
  {
    "objectID": "posts/FinalProject/DataCleaning.html#merge-datasets",
    "href": "posts/FinalProject/DataCleaning.html#merge-datasets",
    "title": "Final Project Methods, Data Cleaning Document",
    "section": "Merge datasets",
    "text": "Merge datasets\nSeparate conditions dataframe based on disease (now just with diabetes but ultimately need to do with each disease)\n\n## DIABETES\nillness_descriptions = ['PATIENT','Diabetes_CONDITIONS','Prediabetes_CONDITIONS','Diabetic retinopathy associated with type II diabetes mellitus (disorder)_CONDITIONS', \n                        'Nonproliferative diabetic retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Macular edema and retinopathy due to type 2 diabetes mellitus (disorder)_CONDITIONS', \n                        'Microalbuminuria due to type 2 diabetes mellitus (disorder)_CONDITIONS', 'Diabetic renal disease (disorder)_CONDITIONS', 'Neuropathy due to type 2 diabetes mellitus (disorder)_CONDITIONS']\n\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n# dataset to be used for analysis\nsubset_conditions.to_csv('conditions_diabetes.csv', index=False)\n\n\n## PREGNANCY\nillness_descriptions = ['PATIENT','Miscarriage in first trimester_CONDITIONS','Miscarriage in second trimester_CONDITIONS',\n                        'Complication occuring during pregnancy_CONDITIONS','Preeclampsia_CONDITIONS', 'Antepartum eclampsia_CONDITIONS',\n                        'Tubal pregnancy_CONDITIONS', 'Congenital uterine anomaly_CONDITIONS', 'Blighted ovum_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n# dataset to be used for analysis\nsubset_conditions.to_csv('conditions_pregnancy.csv', index=False)\n\n\n## CANCER\nillness_descriptions = ['PATIENT','Non-small cell lung cancer (disorder)_CONDITIONS', 'Non-small cell carcinoma of lung  TNM stage 4 (disorder)_CONDITIONS',\n                        'Primary small cell malignant neoplasm of lung  TNM stage 4 (disorder)_CONDITIONS','Non-small cell carcinoma of lung  TNM stage 2 (disorder)_CONDITIONS',\n                        'Non-small cell lung cancer (disorder)_CONDITIONS', 'Suspected lung cancer (situation)_CONDITIONS', 'Malignant tumor of colon_CONDITIONS',\n                        'Overlapping malignant neoplasm of colon_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\n\nsubset_conditions.to_csv('conditions_cancer.csv', index=False)\n\n\n## HEART\nillness_descriptions = ['PATIENT','Coronary Heart Disease_CONDITIONS', 'History of cardiac arrest (situation)_CONDITIONS', 'Cardiac Arrest_CONDITIONS',\n                        'History of myocardial infarction (situation)_CONDITIONS', 'Myocardial Infarction_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\nsubset_conditions.to_csv('conditions_heart.csv', index=False)\n\n\n## LUNGS\nillness_descriptions = ['PATIENT','Asthma_CONDITIONS', 'Pulmonary emphysema (disorder)_CONDITIONS', 'Seasonal allergic rhinitis_CONDITIONS', \n                        'Acute bronchitis (disorder)_CONDITIONS', 'Chronic obstructive bronchitis (disorder)_CONDITIONS',\n                        'Childhood asthma_CONDITIONS', 'Perennial allergic rhinitis with seasonal variation_CONDITIONS',\n                        'Perennial allergic rhinitis_CONDITIONS', 'Acute bacterial sinusitis (disorder)_CONDITIONS', 'Chronic sinusitis (disorder)_CONDITIONS',\n                        'Sinusitis (disorder)_CONDITIONS']\n\nsubset_conditions = conditions_clean.loc[:, illness_descriptions]\nsubset_conditions = prep_data(patients, subset_conditions, illness_descriptions, observations_pivot)\n\nsubset_conditions.to_csv('conditions_lungs.csv', index=False)"
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html",
    "href": "posts/FinalProject/PostHomePage.html",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "",
    "text": "In this project we explore the implementation of a machine learning model for making predictions concerning health outcomes. We seek to understand the models results in the context of differentiating any potential bias in the results from risk scores that mirror true higher/lower risks reported in medical studies. We use two model types (decision trees and random forests), found to be the optimal performers for our data, to generate risk scores for every patient in a synthetic medical data set created by Synthea (Hoyt and Muenchen 2019). We chose six condition “groups” to study, combining data from multiple related conditions into one has/does not have feature, these groups included pulmonary diseases, diabetes and comorbidities, cardiovascular diseases, cancer, and pregnancy and pregnancy complications. We then interpreted these risk factors across race, ethnicity, birthplace, and current town of residence, to look for differential risk scores across groups. We found some results that showed that patients of colors, and patients born in, or residing in less wealthy towns have higher risk factors for some conditions, pointing to the influence of environmental and social inequity factors. Some results, however, were more evenly spaced and harder to interpret."
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html#abstract",
    "href": "posts/FinalProject/PostHomePage.html#abstract",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "",
    "text": "In this project we explore the implementation of a machine learning model for making predictions concerning health outcomes. We seek to understand the models results in the context of differentiating any potential bias in the results from risk scores that mirror true higher/lower risks reported in medical studies. We use two model types (decision trees and random forests), found to be the optimal performers for our data, to generate risk scores for every patient in a synthetic medical data set created by Synthea (Hoyt and Muenchen 2019). We chose six condition “groups” to study, combining data from multiple related conditions into one has/does not have feature, these groups included pulmonary diseases, diabetes and comorbidities, cardiovascular diseases, cancer, and pregnancy and pregnancy complications. We then interpreted these risk factors across race, ethnicity, birthplace, and current town of residence, to look for differential risk scores across groups. We found some results that showed that patients of colors, and patients born in, or residing in less wealthy towns have higher risk factors for some conditions, pointing to the influence of environmental and social inequity factors. Some results, however, were more evenly spaced and harder to interpret."
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html#introduction",
    "href": "posts/FinalProject/PostHomePage.html#introduction",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Introduction",
    "text": "Introduction\nFor our project, we aim to explore the relationship between diseases and social factors such as sex, race, and town, and how these may reflect societal and environmental inequities. We understand from academic literature like “Diabetes Complications in Racial and Ethnic Minority Populations in the USA” (Haw et al. 2021) and “The Black-White Disparity in Pregnancy-Related Mortality from 5 Conditions: Differences in Prevalence and Case-Fatality Rates” (Tucker et al. 2007) that there exist severe inequities in risk rates across different identity groups. Building on this literature, we are interested in building a machine learning model that reveal these patterns in the US healthcare system.\nOur approach is to identify the most accurate predictive model for our dataset, then use this model to generate risk likelihood scores and evaluate the relationship between different diseases and characteristics indicative of societal inequalities. We will then analyze the implications of these risk factors for inequitable, identity-based risk factors in health outcomes and complications. Our project consists of three documents, one in which we clean our original data, one in which we explore this data visually, and the final one, this one, in which we build and explore our models. Taking the general trends we witness in our data visualization document, we carried out the second half of our project; building a model that predicts risk scores.\nWe were inspired by Obermeyer and colleagues work (Obermeyer et al. 2019) in analyzing the bias present in machine learning models used to guide healthcare decisions by using health costs as a proxy measure for health. This model used pre-existing bias in our healthcare system to make decisions that further marginalized oppressed identity groups. We are hoping that our model could bring to light existing inequities. Comparing the risk scores, we analyzed whether trends emerged in terms of socioeconomic status (which we measure by the proxy of town of birth and residence), race, gender, and ethnicity. We then used Barocas et al.’s paper (Barocas, Hardt, and Narayanan 2023) to analyze the implications of our findings in terms of the three definitions of fairness. Finally, we referenced articles like “Explanatory learner models: Why machine learning (alone) is not the answer” (Rosé et al., n.d.) to analyze the potential negative impact of relying on machine learning models in important decision contexts and proposed solutions to this dilemma."
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html#values-statement",
    "href": "posts/FinalProject/PostHomePage.html#values-statement",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Values Statement",
    "text": "Values Statement\nThe motivation behind our project was to uncover potential inequities in the manifestations of certain conditions, for example does a persons race or socioeconomic status predispose them to certain conditions more than others. Our goal was to identify potential societal and environmental factors that unjustly, or disproportionately contribute to disparities in health outcomes. Our focus on this project stems from a desire to understand and address societal and evironmental inequities that contribute to disparities in health outcomes, and our personal commitments to promoting equity and social justice in healthcare.\nThe primary potential users of our project would include researchers, policymakers, and public health organizations interested in understanding and addressing health inequities. However, the project’s findings and potential implications could also affect the communities we study, especially those that we find experience disparities in health outcomes due to social determinants.\nIf our research were to be taken out of context by researchers and health professionals, and taken to be a study of biological predisposition, and not of the manifestation of social factors, our results may reinforce assumptions about health outcomes by race and ethnicity in the medical field, enforcing harmful stereotypes or leading to further marginalization of certain groups. Additionally, if the data or models have inherent biases, they could perpetuate or amplify existing disparities.\nWith proper usage and implementation though, we hope our results would positively impact public health programs and initiatives that work in preventative measures in the most at-risk communities. With our data, we hope that these measures would more easily idenity communities in which to center efforts and awareness campaigns, by shedding light on health inequities and informing efforts to address them."
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html#material-methods",
    "href": "posts/FinalProject/PostHomePage.html#material-methods",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Material & Methods",
    "text": "Material & Methods\n\nOur Data\nOur project utilizes a synthetic data set created for an Introduction to Biomedical Data Science Textbook, (Hoyt and Muenchen 2019). The data was created using Synthea, a synthetic patient generator that models the medical history of synthetic patients. Synthea’s mission is “to output high-quality synthetic, realistic but not real, patient data and associated health records covering every aspect of healthcare.” This allowed for much easier access than real patient data, as well as alleviating any privacy concerns that would arise from using real patient data. The link to the data can be found here: https://data.world/siyeh/synthetic-medical-data.\nOur dataset was originally quite large, with over 200 Million entries. After thorough data cleaning and preprocessing, the data was then transformed to multiple CSV documents, generally with the format of each row representing a different patient with one-hot-encoded values for multiple disease conditions.\nWhile using synthetic data has its benefits, it is essential to acknowledge certain inherent limitations. Firstly, despite efforts to create diverse and representative synthetic patients, there may still be discrepancies in representing certain demographic groups or medical conditions accurately. Certain rare or uncommon medical conditions may be underrepresented in the dataset due to the limitations of the modeling and analyses processes. This data is generated to represent patients from Massachusetts, so any generalization of results must proceed with caution. Thus while this synthetic dataset serves as a valuable resource for educational purposes, researchers and practitioners should approach its use with an understanding of its limitations.\nAfter cleaning our data, we perfomed exploratory data analysis in order to visualize out datset, the results of which are found in this extension of our materials and methods section, our exploratory data analysis section.\n\n\nOur Approach\nSince our original dataset was quite large, a thorough process of data cleaning and preprocess was needed, as well as an evaluation of which parts and features of our data should be actively used as predictors for our models. We subset our data into different CSV files, each entry to a given CSV corresponding to the different type of condition. This allowed for our models to be trained more concisely and efficiently, as well as increasing interpretability of results. This extension of methods and materials shows our data cleaning pocess in more depth.\nMultiple models were trained for each analysis of a condition group, including a logistic regression model, a decision tree classifier, a random forest classifier, and a support vector machine. These models were then evaluated for best score, using cross-validation, given its performance for a specific condition group. The best model, i.e. the one returning the highest cross-validated accuracy, was chosen as the predictive model for our general risk scores. We then trained this optimal model on our training dataset, and created predictions for our testing data that represented the probability of each entry being 1 (having a certain condition) or 0 (not having a certain condition). A risk score could then be anything between 0.00 and 1.00, where 0.50 would represent a 50% probability that the given patient has a condition. The models ran on our own personal devices, on the ML-0451 class kernel.\n\n\nCritical Discussion\nThe goal of our presentation is to analyze the bias present in our healthcare system and the risk of certain groups of different illnesses and health conditions. There are many organizations that might find this type of model useful or interesting. One interested party could be a hospital that wants to allocate resources based on the communities they serve. This could be helpful as they could adapt to real community needs. A similar use case could be if a town is building or allocating healthcare resources and wants to understand the risks of their township or locality. Hopefully, this model could help allow resources to go to the places in which there is great need. However, an important note is that this dataset measures the recorded rates of a hospital setting. This could widely vary from real illness rates, as certain communities are under-treated or under-diagnosed in the US healthcare system.\nA more harmful use case could be an insurance company that could incorporate this model into their decision to cover individuals or not. Therefore, our model has the risks, if put in the wrong hands, to have a negative impact on already marginalized communities. Seeing as insurance companies are incredibly wealthy, it is likely that this could be a body that would be financing this project. This raises the question of whether this model should be allowed to be employed in decision-making scenarios.\nWe completed this work out of curiosity as part of an educational pursuit. If used for knowledge or understanding of the impact of different illnesses and conditions on identity groups, it can be helpful and informative. However, there is also the risk of further harming groups that have already been historically marginalized in medicine."
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html#results",
    "href": "posts/FinalProject/PostHomePage.html#results",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Results",
    "text": "Results\nThe first extension of our results section is this document, in which we run our model and generate the risk scores and comparisons we discuss in further depth here.\nThe results of our risk score predictions varied widely. It is important to remember that the foundation of our project is based on interpreting our model’s perceptions of different group’s likelihoods of having a certain condition, and so they might reasonably disagree with actual trends in condition prevelance. With this in mind, in this section we will delve more deeply into the findings of our model.\nFirst, I wanted to inspect prevelance by race, to understand whether one racial group was more often assigned higher risk scores than others across conditions.\n\nResults by Race:\n\n\n\n# importing results for visualization\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nrace_risk = pd.DataFrame()\n\nrace_risk['Race'] = ['White', 'Black', 'Hispanic', 'Asian']\nrace_risk['Diabetes'] = [0.312536, 0.256158, 0.340659, 0.479592]\nrace_risk['Pregnancy'] = [0.034260, 0.051395, 0.038217, 0.037262]\nrace_risk['Cancer'] = [0.051942, 0.046859, 0.051650, 0.034009]\nrace_risk['Heart'] = [0.502101, 0.491353, 0.491280, 0.510746]\nrace_risk['Lung'] = [0.507817, 0.509092, 0.492106, 0.514338]\n\nprint(race_risk)\nprint('Fig. 1. Table of predicted risk score values by race for each condition subset.')\n\n       Race  Diabetes  Pregnancy    Cancer     Heart      Lung\n0     White  0.312536   0.034260  0.051942  0.502101  0.507817\n1     Black  0.256158   0.051395  0.046859  0.491353  0.509092\n2  Hispanic  0.340659   0.038217  0.051650  0.491280  0.492106\n3     Asian  0.479592   0.037262  0.034009  0.510746  0.514338\nFig. 1. Table of predicted risk score values by race for each condition subset.\n\n\n\n\n# plotting\nrace_risk.plot(x=\"Race\", y=[\"Diabetes\", \"Pregnancy\", 'Cancer', 'Heart', 'Lung'], kind=\"bar\") \nplt.title('Bar Chart of Health Risks by Race')\nplt.xlabel('Race')\nplt.ylabel('Risk Score %')\n# scale y axis by 100 to show percentage\nplt.yscale('linear')  \nplt.ylim(0, 1) \n\nplt.xticks(rotation=45)\nplt.show()\n\nprint('Fig. 2. Bar chart of predicted risk score values by race for each condition subset. Risk scores are given in percentage.')\n\n\n\n\n\n\n\n\nFig. 2. Bar chart of predicted risk score values by race for each condition subset. Risk scores are given in percentage.\n\n\nFrom this table and plot we can see that risk scores are generally evenly-distributed across racial categories, meaning our model does not take one racial identity as a strong risk factor for all diseases indiscriminately.\nWhat was interesting as we generated our predictions was that both heart and lung disease both predicted near equal risk scores, all around 50%, for all racial groups. This finding disagrees with real medical literature, which shows that Black patients are statistically more likely to experience any pulmonary defects (A. T. Moffett 2021). Part of what could explain our high risk for lung disease across the board, and the lack of differentiation by race we would expect to see, could be due to the fact that we included a large variety of pulmonary diseases, including seasonal allergies and acute bacterial sinusitis. The latter of these, a bacterial sinus infection, is an incredbily common condition; one in 100 common colds lead to sinusitis (“Sinusitis” 2023). Especially as we ignore the fact that our data is simulated to have occured over a long period of time (years), it is entirely reasonable to expect that, over a certain period of time, anyone may have a 50% risk of experiencing symptoms of sinusitis.\nIn terms of heart conditions, once again our model does not match trends in medical findings about cardiovascular outcomes by race, which also finds Black patients at higher risk for cardiovascular conditions (Zulqarnain Javed 2022). Unlike our lung conditions subset, the cardiovascular conditions identified are indicators of larger health issues, and not common complications of everyday illnesses. The fact that our model assigns equal scores regardless of race has large implications for the potential applications of a model like ours, built on synthetic data as ours was. If a model systematically underestimates the risk of groups compared to each other, for example our model would indicate treating all patients similarly regardless of their race, doctors and systems implementing our system may then systemically under-diagnose and treat Black patients, as they are unaware of their true increased risk compared to other racial groups.\nIn terms of diabetes, our model predicted that Asian patients are more likely to experience diabetes, prediabetes or a diabetes-related comorbidity. This agrees with medical findings, that even at lower BMIs, Asian patients are at higher risk for type-2 diabetes (“Diabetes and Asian American People” 2022). Our model also predicted that Black patients are at the highest risk for pregnancy complications, which again agrees with prevailing medical literature (Venicia Gray 2023). This is a promising result as it shows that our model may have a positive usage if implemented in healthcare settings, as it can draw attention to differentiated risks based on risk for diabetes and pregnancy complications.\n\n\nResults by Ethnicity:\nOur results for ethnicity were harder to interpret than our results by race. This following figure shows to top two most at risk ethnic groups and bottom two least at risk ethnic groups for each condition subset. We found that those in the middle often hovered around the same risk scores, and so did not provide as much valuable information for the diagnostic process.\n\n\n\ncancer = pd.DataFrame()\npreg = pd.DataFrame()\nheart = pd.DataFrame()\ndia = pd.DataFrame()\nlungs = pd.DataFrame()\n\n\ncancer['Ethnicity'] = ['German', 'Puerto Rican', 'Russian', 'Scottish']\ncancer['Risk'] = [0.105675, 0.067085, 0.005342, 0.005342]\n\npreg['Ethnicity'] = ['Dominican', 'Scottish', 'German', \"Russian\"]\npreg['Risk'] = [0.074523, 0.074523, 0.023289, 0.014905]\n\n\nheart['Ethnicity'] = ['Asian Indian', 'Scottish', 'Dominican', 'Chinese']\nheart['Risk'] = [0.556735, 0.556144, 0.480579, 0.464757]\n\ndia['Ethnicity'] = ['Asian Indian', 'Polish', 'Russian', 'Irish']\ndia['Risk'] = [0.714286, 0.558405, 0.200000, 0.182902]\n\nlungs['Ethnicity'] = ['Polish', 'Mexican', 'West Indian', 'Chinese']\nlungs['Risk'] = [0.583031, 0.580761, 0.457941, 0.451298]\n\nfig, axs = plt.subplots(1, 5, figsize=(20, 5), sharey=True)\n\nfor i, (df, title) in enumerate(zip([cancer, preg, heart, dia, lungs], ['Cancer', 'Pregnancy', 'Heart', 'Diabetes', 'Lungs'])):\n    axs[i].bar(df['Ethnicity'], df['Risk'], color='skyblue')\n    axs[i].set_title(title)\n    axs[i].tick_params(axis='x', rotation=45)\n\n# scale y axis by 100 to show percentage\nplt.yscale('linear')  \nplt.ylim(0, 1) \n\nplt.tight_layout()\nplt.show()\n\nprint('Fig. 3. The risk score, shown in percentage on the y-axis, by the top and bottom two most at-risk ethnic groups based on condition subset.')\n\n\n\n\n\n\n\n\nFig. 3. The risk score, shown in percentage on the y-axis, by the top and bottom two most at-risk ethnic groups based on condition subset.\n\n\nThe first thing we checked with these results was whether the risks were skewed based on representation, i.e. whether some ethnicities appear much more frequently or much less frequently in our overall dataset. If for example Russian appeared infrequently in our dataset, and had a lower score overall for many of the conditions, we could not be sure whether this ethnic Russians truly have a lower risk, or are just underrepresented in our dataset. After comparing the distributions of ethnic groups in our dataset, we found that Russian and Scottish ethnic groups seemed to be slightly underrepresented, but that overall no ethnic group appeared drastically less or more frequently than any other, making us confident that most of the driving forced behind our results are true patterns in diagnosis in our dataset, and not issues of skew.\nIn figure three, we see that our most at-risk ethnic groups by condition align with our most at-risk racial groups by condition, which is promising. The diabetes, lung, and heart conditions also all show increased risk overall, which matches our models predictions that those groups of conditions appear to be more common than cancer and pregnancy-related conditions. We hope that the figure above could serve a starting point for further investigation into the implications of increased/decreased risk for conditions based on having these ethnic identities. Without being intimately familiar with the demographic history and trends of Massachussetts, it’s hard to say whether environmental or genetic factors are the main factors influencing these results. Further research into community trends might find it valuable to draw on these scores as quantitative background for the effects of observed environmental, genetic, and societal factors\n\n\nResults by Town of Birth/Current Residence:\nThe last category we examined our data through was the town of birth and the the town current residence. We chose to compare both, as we weren’t certain whether the place of someone’s birth, or the place they currently reside (and therefore may have been residing for much of their adult lives) effects their health outcome more.\n\n\n\nbptown = pd.DataFrame()\n\nbptown['TownCat'] = ['Wealthy', 'Non-Wealthy']\nbptown['Lung'] = [0.4872429058942045, 0.5189382015534418]\nbptown['Heart'] = [0.10892307692307693, 0.1103076923076923]\nbptown['Cancer'] = [0.06399830132085002, 0.04238804096017698]\nbptown['Pregnancy'] = [0.038981469137448335, 0.03668844154112784]\nbptown['Diabetes'] =  [0.33305156382079454, 0.31947027331642713]\n\ncrtown = pd.DataFrame()\ncrtown['TownCat'] = ['Wealthy', 'Non-Wealthy']\ncrtown['Lung'] = [0.5039833131472166, 0.5124758651408807]\ncrtown['Heart'] = [0.08661538461538462, 0.09256410256410255]\ncrtown['Cancer'] = [0.03621368085712754, 0.05164958111475114]\ncrtown['Pregnancy'] = [0.04586055192640981, 0.03630627027507444]\ncrtown['Diabetes'] = [0.25274725274725274, 0.2827087442472057]\n\nprint(bptown)\nprint('Fig. 4. Average Risk Score by Condition Group According to Wealth Designation of Birthplace.')\n\n       TownCat      Lung     Heart    Cancer  Pregnancy  Diabetes\n0      Wealthy  0.487243  0.108923  0.063998   0.038981  0.333052\n1  Non-Wealthy  0.518938  0.110308  0.042388   0.036688  0.319470\nFig. 4. Average Risk Score by Condition Group According to Wealth Designation of Birthplace.\n\n\n\n\n\nprint(crtown)\nprint('Fig. 5. Average Risk Score by Condition Group According to Wealth Designation of Current Town of Residence.')\n\n       TownCat      Lung     Heart    Cancer  Pregnancy  Diabetes\n0      Wealthy  0.503983  0.086615  0.036214   0.045861  0.252747\n1  Non-Wealthy  0.512476  0.092564  0.051650   0.036306  0.282709\nFig. 5. Average Risk Score by Condition Group According to Wealth Designation of Current Town of Residence.\n\n\nFrom the tables above, we see that for most conditions, wealthy versus non-wealthy towns have relatively similar risk scores. This could be due to the lack of great variety in our dataset in terms of environment. Of course some towns are wealthier than others, but they still all exist within Massachussetts, a smaller state (area-wise) compared to most others, and therefore not many of the towns we include truly exist in their own bubble where direct correlations between town wealth and health of its inhabitants can easily be measured.\nThat being said, there are some differences worth nothing. We found the disparity in pulmonary condition risk score between wealthy and non-wealthy towns being wider by birthplace interesting, as we knew from the literature that childhood asthma is often linked to environmental factors such as air quality and poverty (Christina M. Pacheco 2014). We also see that patients with a non-wealthy town of current residence are slightly more at risk for developing diabetes, which makes sense considering the food you eat as a baby will have less of an effect on your present risk for developing diabetes than the kinds of food you may have access to now, which in non-wealthy towns may be lacking access to fresh fruits and vegetables, and other more expensive, but less-processed food items. Interestingly both patients with a wealthy birthplace and those with a wealthy current town are at a greater risk of pregnancy complications according to our model. Speculatively, this could be due to differences in mothers age, as younger mothers are more likely to be lower income than older mothers, but older mother are at a much higher risk for complications (K A Moore 1993). Our cancer risk is higher for those born in wealthy towns, but lower for those currently residing in a wealthy town. The latter makes sense considering potentially increased access to early-intervention/preventative healthcare, but we are not sure about the birthplace results meaning. Our risk for heath diseases are almost equal across birthplaces and current towns of residence."
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html#concluding-discussion",
    "href": "posts/FinalProject/PostHomePage.html#concluding-discussion",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOur project was able to accomplish our goal of analyzing risk rates for various illnesses and conditions for different identities. Due to the large quantity of data we possessed, we were unable to analyze all of the data we had access to to make predictions. Ideally, we would have been able to predict medication use or various observations in addition to specific conditions. Also, if we had access to more data, we could have made more specific predictions- like for asthma instead of general lung ailments. If we had more time, computational resources, and data we would like to extend our study to include healthcare information for different conditions as well as different geographical regions outside of Massachusetts. By amplifying the range of data we include we would be able to come to more concrete conclusions on different risk rates. However, we were able to complete our aspirations for this project by generating risk rates for race, gender, ethnicity, birthplace, and current address for five different ailments.\nOur results compare to the results of those who have studied similar problems. For example, there is a large quantity of scientific data that shows that people at lower socio-economic status are more likely to get diabetes (Yelena Bird 2015). Furthermore, race has been strongly connected to material mortality and health. Specifically, black and hispanic women are at much higher risk of issues with pregnancy than their white counterparts (Eran Bornstein 2020). We saw both these trends and more replicated in our model’s predictions. Therefore we can conclude that our model is creating predictions that are correlated with real life trends."
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html#the-3-definitions-of-fairness",
    "href": "posts/FinalProject/PostHomePage.html#the-3-definitions-of-fairness",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "The 3 Definitions of Fairness",
    "text": "The 3 Definitions of Fairness\nIn their paper (Barocas, Hardt, and Narayanan 2023), Barocas, Hardt, and Narayanan outline three relative notions of fairness: the narrow, middle, and broad views. The narrow view of fairness suggests that we should treat similar individuals in the same manner, given how currently similar they are. The broad perspective advocates for structuring society to facilitate similar outcomes for people with comparable abilities and ambitions. The middle-ground stance proposes that we treat different people equally, under the assumption that their apparent dissimilarities stem from factors just as past injustices or misfortunes that should be disregarded.\nSince our project involves comparing people of different demographic and characteristics, evaluating risk scores and examining fairness, we must look into our project and identify what is fair, as well as what we are choosing to define as fair.\nUnder the narrow view of fairness, since the comparison is between individuals and not directly concerned with the way members of specific groups might be treated, The narrow view only commands that similar people be treated similarly. In our models, similar people (eg of the same demographic factor being studied such as race, gender, ethnicity…) are being treated similarly in our models.\nUnder the middle view of fairness, since the decision makers have an obligation to avoid perpetuating injustice, our evolution of our model and data’s biases in attempts to expose the perpetuation of injustice keeps us in alignment with the middle view of fairness.\nUnder the broad view of fairness, since the comparison focuses on the degree to which society overall is structured to allow people of similar ability and ambition to achieve similar success and attributes outcomes to whether they possess different ability or ambition, the fact that we were able to find clear differences in risk scores for differing groups of demographics, suggests that society is not structured to allow people people of similar ability and ambition to achieve similar success. Such as people from less-wealthy neighborhoods having a significant double the risk score of their wealthier counterparts, or men being two times as likely to get cancer than their female counterparts. Our models results suggest that these groups do not experience fair equality in some way, with these disproportionate affects of diseases suggesting that environmental or systemic factors may be at play. Intervention and change are needed at the basic level of societal structure, in this case we ideally would find that there are not environmental or societal impacts on the risk of different people contracting different diseases, seeing similar risk scores for differing peoples."
  },
  {
    "objectID": "posts/FinalProject/PostHomePage.html#group-contributions",
    "href": "posts/FinalProject/PostHomePage.html#group-contributions",
    "title": "Exploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes",
    "section": "Group Contributions",
    "text": "Group Contributions\nImportant Note: The additions/deletions on Github look skewed because of the creations/deletions of the csv files, not the actual code.\nLindsey: At the beginning of the project, I worked alongside Sophie to clean the data and figure out how to merge our information without the kernel dying. This included pivoted columns, filtering the data into various conditions, and merging the datasets. The next large role I took on was creating the code to train models on the dataframes we had created in our DataCleaning file. Together, the three of us worked to create code to evaluate the models based on race, gender, ethnicity, birthplace, and current address. I took a leading role in figuring out how to evaluate birthplace risk based on wealth of cities in Massachusetts. I also worked to re-organize this evaluation code so that we could reuse it for all of the conditions we studied. In terms of the blog post writing, I completed the two discussion sections and found sources to reference in our introduction that helped us develop our analysis throughout the Blog Post. Although our attention was divided among various aspects of the project, we collaborated effectively as a team, supporting one another whenever any member encountered a challenge.\nJulia: In the beginning of the project, I created data visualizations allowing us to better understand our data and project question goals. The large role I took on was to create code to evaluate the models based on race, gender, ethnicity, birthplace, and current address. I implemented the evaluation of cancer, heart diseases, and lung diseases. I worked alongside with Lindsey to find and import our Massachusetts wealth information, in order to evaluate birthplace and current town residence risk based on wealth.\nI additionally went through our model code and repaired seeding and randomness issues, to ensure our models were performing to the same caliber and ‘accuracy’ across our code. Throughout the project, I took on the role of keeping our data thoroughly cleaned and organized, as we frequently found ourselves with extraneous and additional code we did not need, as well as a need to organize our code for comprehensibility and readability as we worked both separably and together. In terms of the blog post writing, I completed our Values Statement, Material & Methods section, as well as the discussion on the three views of fairness. Our efforts were comprehensive and collaborative throughout this project. Pair-programming was utilized alongside our individual divide-and-conquer. We functioned cohesively as a group, completing our project with an happy divide of labor and effort.\nSophie: Lindsey and I started by working on cleaning and merging our data into usable (i.e. not large enough to crash our kernel every time) datasets for model training. Afterwards, I contributed to the exploratory data analysis document by creating graphs showing differences in condition prevelance by race, ethnicity, and birthplace (for which I had to find the populations of each town in our dataset to calculate prevelance). We worked together on writing the code to generate models and risk scores for our conditions. I started to address the problem of random seeding in our data, which was causing variable results each time we ran, which Julia took on later. Afterwards I focused on the organization aspect of our blog post, writing explanatory comments for all our documents/code lines, and creating the format for our post in terms of linking all of our various working documents together into this one, more streamlined document. I also wrote our introduction, abstract, and results section, and Julia and I worked together on our values statement, and I wrote parts of our approach. Overall, I think we worked very well as a group in terms of division of labor and coding together. We were all proactive in taking the lead on certain aspects of the project, and worked very collaboratively together when we were stuck on certain parts.\nIn your group contributions statement, please include a short paragraph for each group member describing how they contributed to the project:\nWho worked on which parts of the source code? Who performed or visualized which experiments? Who led the writing of which parts of the blog post? Etc."
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html",
    "href": "posts/newtons/newtonblogpost.html",
    "title": "Implementing Newton’s Optimizer",
    "section": "",
    "text": "First I read in my logistic regression with newton’s optimizer .py file.\n%load_ext autoreload\n%autoreload 2\nfrom newton import LogisticRegression, NewtonOptimizer, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html#abstract",
    "href": "posts/newtons/newtonblogpost.html#abstract",
    "title": "Implementing Newton’s Optimizer",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implement Newton’s Method for Logistic Regression, building on my previous implementation of logistic regression. Newton’s Method, a second-order optimization technique, requires knowledge of both the first and second derivatives of the loss function. I first implementation a NewtonOptimizer class, extending my logistic regression model with a new method to compute the Hessian matrix and perform the optimization step for logistic regression using this new factor. Through experiments and visualizations, I explore the effectiveness of Newton’s method in converging to the correct solution and its potential for faster convergence compared to standard gradient descent under certain conditions. Additionally, I calculate the computational costs associated with Newton’s method and gradient descent, providing insights into the trade-offs between the two methods."
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html#b.-experiments",
    "href": "posts/newtons/newtonblogpost.html#b.-experiments",
    "title": "Implementing Newton’s Optimizer",
    "section": "B. Experiments",
    "text": "B. Experiments\n\n1. Testing Convergence Behavior of Newtown Optimizer\nI used the classification data generation function from the logistic and perceptron blog posts to create linearly seperable data.\n\nimport torch\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)\n\nAfter generating my data, I plotted it to show what the class seperation looks like. Again, this code is taken from the logistic regression and perceptron blog posts.\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\n# adapted from logistic & perceptron blog post\ndef plot_classification_data(X, y, ax):\n    plt.figure(figsize=(10, 6))\n    targets = [0, 1]\n    markers = [\"o\" , \"X\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n    \n    \nfig, ax = plt.subplots(1, 1)\n\n# create and plot data\nX, y = classification_data(noise = 0.2)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nWe can see the data is clearly linearly seperable. Next, I defined a few functions, again from the previous blog posts, to help me visualize the progress of me model through its optimization steps\n\n# helpful functions from previous blog posts\n\n# draws found seperating line on graph\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n# finds final model accuracy on data\ndef find_accuracy(X, y):\n    predictions = LR.predict(X)\n    correct_preds = (predictions == y).float()\n    accuracy = torch.mean(correct_preds)\n\n    print(f\"Accuracy: {accuracy.item()}\")\n\n# plots the generated loss vectors\ndef plot_loss(lvec):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n    \n    plt.plot(lvec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(lvec)), lvec, color = \"slategrey\", s=5)\n    plt.gca().set(xlabel = \"Logistic Regression Steps\", ylabel = \"Loss\")\n    \n    plt.title(f\"Final loss: {lvec[len(lvec)-1]:.3f}\")\n\nFirst, I wanted to observe whether my model implemented with the newton optimizer would converge regularly on seperable data.\n\n# instantiate a model\nLR = LogisticRegression() \nopt = NewtonOptimizer(LR)\n\n# instantiate a loss vector\nloss_vec = []\n\n# for 6000 iterations, track the progress of the loss of our model\nfor index in range(6000):\n    \n    opt.step(X, y, a = 0.5)\n    \n    loss = LR.loss(X, y).item()\n    loss_vec.append(loss)\n\n# find our final accuracy\nfind_accuracy(X, y)\n\n# plot the final classification line against our data\n\nfig, ax = plt.subplots(1, 1)\n\nplot_classification_data(X, y, ax)\n\ndraw_line(LR.w, x_min=-1, x_max=2, ax=ax, color=\"black\")\nax.set_title(f\"loss = {loss:.3f}\")\nax.set(xlim=(-1, 2), ylim=(-1, 2))\n\nAccuracy: 1.0\n\n\n\n\n\n\n\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;\n\n\nFrom this we see that our model does in fact converge to the correct seperating line for our data, and ends with a loss of 0.00 and an accuracy of 100%.\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nAgain, here we can see our loss converging to zero as expected across our 6000 iterations.\n\n\n2. Comparing Newtown’s Optimizer to Standard Gradient Descent\nIn order to compare the performance of my Newtown’s optimizer implementation to my previously implemented standard gradient descent optimizer, I ran both optimizers for 8000 iterations, tracking the loss at each step, and then graphed them side-by-side to observe the difference. My goal was to observe that in at least some cases Newtown’s optimizer converges faster than gradient descent.\n\n# Initialize logistic regression model and Newton optimizer\nLR = LogisticRegression()\n\nopt_newton = NewtonOptimizer(LR)\nopt_gradient_descent = GradientDescentOptimizer(LR)\n\n# Lists to store losses for each iteration\nloss_vec_newton = []\nloss_vec_gradient_descent = []\nloss_gradient_descent = float('inf')\nloss_newton = float('inf')\n\n# Run Newton's method for 8000 iterations\nfor _ in range(8000):\n    loss_newton = LR.loss(X, y).item()\n    opt_newton.step(X, y, a=4)\n    loss_vec_newton.append(loss_newton)\n\n# Re-initialize weights for gradient descent\nLR.w = None\nLR.pastw = None\n\n# Run standard gradient descent for 8000 iterations\nfor _ in range(8000):\n    loss_gradient_descent = opt_gradient_descent.step(X, y, a=0.1, b=0)\n    loss_vec_gradient_descent.append(loss_gradient_descent)\n\n# Print final loss values\nprint(\"Final Loss (Newton's Method):\", loss_newton)\nprint(\"Final Loss (Gradient Descent):\", loss_gradient_descent)\n\n# Plot the convergence of both methods\nplt.figure(figsize=(10, 6))\nplt.plot(loss_vec_newton, label=\"Newton's Method\")\nplt.plot(loss_vec_gradient_descent, label=\"Gradient Descent\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Convergence Comparison: Newton's Method vs. Gradient Descent\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nFinal Loss (Newton's Method): 0.0\nFinal Loss (Gradient Descent): tensor(0.0081)\n\n\n\n\n\n\n\n\n\nFrom this graph we see that Newtown’s optimizer does in fact converge faster than gradient descent with these respective alphas on this linearly seperable data.\n\n\n3. When Alpha Is Too Large, Newtown’s Should Not Converge\nIn order to test this, I ran my loss vector creation loop over a small number of iterations for a very large alpha (745). At very large numbers I had an error, but still being able to graph the vector we can see it’s trend towards non-convergence.\n\nX, y = classification_data(n_points=300, noise = 0.5)\n\n# Initialize logistic regression model and Newton optimizer\nLR = LogisticRegression()\nopt_newton = NewtonOptimizer(LR)\n\n# Lists to store losses for each iteration\nloss_vec_newton = []\n\n# Run Newton's method for 10 iterations\nfor _ in range(10):\n    loss_newton = LR.loss(X, y).item()\n    opt_newton.step(X, y, a=745)\n    loss_vec_newton.append(loss_newton)\n\n\n# Print final loss values\nprint(\"Final Loss (Newton's Method):\", loss_vec_newton[-1])\n\nplot_loss(loss_vec_newton)\n\nFinal Loss (Newton's Method): 8.40453815460205\n\n\n\n\n\n\n\n\n\nFrom this we can see that our model is not starting to converge in the way we saw it do with smaller alpha values."
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html#c.-operation-counting",
    "href": "posts/newtons/newtonblogpost.html#c.-operation-counting",
    "title": "Implementing Newton’s Optimizer",
    "section": "C. Operation Counting",
    "text": "C. Operation Counting\nWorking with the following assumptions on computing costs:\nLoss, \\(L\\) = \\(c\\)\nGradient, \\(G\\) = \\(2c\\)\nHessian, \\(H\\) = \\(pc\\)\nInverting a \\(p\\times p\\) matrix = \\(k_1p^\\gamma\\)\nMultiplying by \\(H\\) = \\(k_2p^{2}\\)\nFinding the total computational costs of Newtown’s method.\nAssuming: Newtown’s method converges to an adequate solution in \\(t_\\mathrm{nm}\\) steps.\n\\(1.\\) First, we calculate the computing cost of a single step: - Computing \\(L\\) = \\(c\\) - For as many steps as it takes to converge, i.e. \\(t_\\mathrm{nm}\\) steps - Computing \\(G\\) = \\(2c\\) - Computing \\(H\\) = \\(pc\\) - Inverting hessian matrix = \\(k_1p^\\gamma\\) - Multiplying by \\(H\\) = \\(k_2p^{2}\\)\n\\(2.\\) Adding up all of the steps together, we get the total computational cost for our Newtown’s optimizer, which is: - \\(O(c + t_\\mathrm{nm} \\times (2c + pc + k_1p^\\gamma + k_2p^{2}))\\)\nFinding the total computational costs of standard gradient descent.\nAssuming: standard gradient descent converges to an adequate solution in \\(t_\\mathrm{gd}\\) steps.\n\\(1.\\) First, we calculate the computing cost of a single step: - Computing \\(L\\) = \\(c\\) - For as many steps as it takes to converge, i.e. \\(t_\\mathrm{gd}\\) steps - Computing \\(G\\) = \\(2c\\)\n\\(2.\\) Adding up all of the steps together, we get the total computational cost for our Newtown’s optimizer, which is: - \\(O(c + t_\\mathrm{gd} \\times (2c))\\)\nLooking at these two equations, we have a general sense that Newtown’s method is more computationally costly. However, to compare which is less computationally expensive mathematically, we can create a ratio between the two:\n\n\\(\\frac{t_\\mathrm{nm}}{t_\\mathrm{gd}} = \\frac{c + t_\\mathrm{nm} \\times (2c + pc + k_1p^\\gamma + k_2p^{2})}{c + t_\\mathrm{gd} \\times (2c)}\\)\n\nThe numerator of the second fraction, which represents the computational cost of Newton’s optimizer, contains terms that grow polynomially with the values of \\(p\\) and \\(\\gamma\\). This polynomial growth in the numerator’s terms indicates that the computational cost of Newton’s optimizer will increase significantly compared to the denominator as the values of \\(p\\) and \\(\\gamma\\) increase. This suggests that as the problem complexity or the size of the optimization space increases, the computational cost associated with Newton’s optimizer will grow polynomially relative to the computational cost associated with the denominator, the computational cost of standard gradient descent, as it does not depend on either \\(p\\) or \\(\\gamma\\). Therefore, for large or complex optimization problems, the computational resources required by Newton’s optimizer may become an obstacle that renders the method impossible to implement."
  },
  {
    "objectID": "posts/newtons/newtonblogpost.html#discussion",
    "href": "posts/newtons/newtonblogpost.html#discussion",
    "title": "Implementing Newton’s Optimizer",
    "section": "Discussion",
    "text": "Discussion\nThrough completing this blog post, I’ve gained a deeper understanding of the inner workings of Newton’s method for optimization in terms of it’s mathematical process and its potential benefits and limitations in regression tasks. Understanding the pros in terms of faster convergences, but the cons in terms of greater computational expense and failure at large alphas reinforced for me the importance of selecting the right optimization algorithm for a task. By carefully examining computational costs and performance considerations, I was able to practice the computations required for selecting an optimization algorithm. The higher time complexity of Newton’s method, particularly when dealing with large feature spaces, again emphasized the need for thoughtful consideration in choosing optimization strategies. Overall, this post has enhanced my technical skills and broadened my perspective on the intricacies of optimization techniques in machine learning."
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html",
    "href": "posts/perceptron/PerceptronBlogPost.html",
    "title": "Implementing the Perceptron",
    "section": "",
    "text": "In this blog post, I explore the implementation and experimentation of the perceptron algorithm. Beginning with an implementation of the algorithm itself in the file perceptron.py, I then explore its functionality and effectiveness through various experiments. Through code implementation and visualizations, I demonstrate the algorithm’s behavior on both linearly separable and non-linearly separable data in 2D, as well as its extension to greater dimensions. Additionally, I explore the mini-batch perceptron algorithm and its impact on convergence and performance. Throughout this blog post I learned a lot about the advantages and disadvantages of the perceptron algorithm on various kinds of data.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#abstract",
    "href": "posts/perceptron/PerceptronBlogPost.html#abstract",
    "title": "Implementing the Perceptron",
    "section": "",
    "text": "In this blog post, I explore the implementation and experimentation of the perceptron algorithm. Beginning with an implementation of the algorithm itself in the file perceptron.py, I then explore its functionality and effectiveness through various experiments. Through code implementation and visualizations, I demonstrate the algorithm’s behavior on both linearly separable and non-linearly separable data in 2D, as well as its extension to greater dimensions. Additionally, I explore the mini-batch perceptron algorithm and its impact on convergence and performance. Throughout this blog post I learned a lot about the advantages and disadvantages of the perceptron algorithm on various kinds of data.\n\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#linearly-separable-experiments",
    "href": "posts/perceptron/PerceptronBlogPost.html#linearly-separable-experiments",
    "title": "Implementing the Perceptron",
    "section": "Linearly Separable Experiments",
    "text": "Linearly Separable Experiments\nMy first step, after implementing the perceptron class, was to test my work on known linearly seperable data. I used the code from lecture 7 to generate linearly seperable data and a visualization of said data.\n\nimport torch\nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\n# function which creates linearly seperable data\ndef perceptron_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\n# creating data instance\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\n# function that plots perceptron data\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nThis graph shows a clear distinction between labels in the data that can be easily divided by a single line.\n\n# My classmate Lindsey Schweitzer's idea to create a plot_loss function for easier plotting\ndef plot_loss(loss):\n\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n\n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    labs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\nI then ran an update cycle on the data, continuing to update the weights until the total loss reached 0.0 (since I knew the data would eventually reach this point).\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThis did indeed find that the data was linearly separable, and that my perceptron class and implementation was able to successfully find that separating line.\n\nVisualizing Perceptron’s Process\nI adapted code, again from lecture 7 to visualize a few specific steps of the perceptron algorithm, showing how the line changes as the weights are updated until it eventually finds where loss is equal to zero.\n\n# function from lec 7 to add line to plot\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 3, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # find i, random data point\n    i = torch.randint(n, size=(1,))\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(X[[i],:], y[i])\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n        current_ax += 1\nplt.tight_layout()"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#non-linearly-separable-experiments",
    "href": "posts/perceptron/PerceptronBlogPost.html#non-linearly-separable-experiments",
    "title": "Implementing the Perceptron",
    "section": "Non-Linearly Separable Experiments",
    "text": "Non-Linearly Separable Experiments\nNext, I tested my perceptron against data that I know to be non-linearly separable to see how it would perform. For this purpose, I created a function that creates data that cannot be separated by a single line.\n\n# creates non-linearly separable data\ndef perceptron_data_nonlinear(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    # same as the linearly seperable data\n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # add random noise to y\n    y = y ^ (torch.randint(0, 2, size=(n_points,)).bool())\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data_nonlinear()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\nClearly, looking at this data, there is no easy way to classify points using a perceptron-generated line. Next, I used my perceptron update algorithm on an instance of this data, to see how the loss would change over the iterations. In order to prevent looping forever, as the loss will never reach 0.0, I instead limited the algorithm to 1000 iterations.\n\nX, y = perceptron_data_nonlinear()\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\nweights = []\n\nn = X.size()[0]\n\nmaxit = 1000 # max iterations\n\nwhile maxit &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    weights.append(torch.clone(p.w))\n\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    maxit-=1\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThe algorithm is obviously struggling with this unseparable data, and isn’t able to land anywhere close to a 0.0 loss like it did for our previous data. After 1000 iterations it found a final loss of 0.47.\n\nVisualizing Perceptron’s Last Move\nLike for the linearly separable perceptron, I wanted to visualize the line my perceptron had drawn on the data. In this case, since there was build-up in terms of the line approaching a better separation as in the linear data, I just visualized the very last loss found.\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X, y, ax)\ndraw_line(weights[-2], x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\ndraw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\nax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n# draw_line(w, -10, 10, ax, color = \"black\")\nax.set_title(f\"loss = {loss_vec[-1]:.3f}\")\nax.set(xlim = (-1, 2), ylim = (-1, 2))\nplt.tight_layout()"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#data-with-5-dimensions",
    "href": "posts/perceptron/PerceptronBlogPost.html#data-with-5-dimensions",
    "title": "Implementing the Perceptron",
    "section": "Data with 5 Dimensions",
    "text": "Data with 5 Dimensions\nTo test whether the data is linearly seperable in the case of higher dimensions, I set the dimension to 5 for the creation of the data (using the regular, theoretically linearly-separable function), and ran the same perceptron algorithm and visualization process. I again used an iteration max instead of a loop until loss = 0, as I wasn’t sure whether or not the data would end up being linearly separable.\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims=5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nmaxit = 1000\n\nwhile maxit &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    maxit-=1\n    \nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nInterestingly, I found that the data in 5-dimensions was in fact linearly seperable within 1000 perceptron iterations. I noticed though, that upon increasing the noise in the data to 0.5 and above, the data become non-linearly seperable. The visualization of that noisier data is below.\n\nX, y = perceptron_data(n_points = 300, noise = 0.5, p_dims=5)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nmaxit = 1000\n\nwhile maxit &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n    maxit-=1\n    \nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nWith noise set to 0.5, the algorithm could only reach a final loss of 0.053."
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#mini-batch-implementation",
    "href": "posts/perceptron/PerceptronBlogPost.html#mini-batch-implementation",
    "title": "Implementing the Perceptron",
    "section": "Mini-Batch Implementation",
    "text": "Mini-Batch Implementation\nNext, I implemented the mini-batch perceptron algorithm, which computes a perceptron update using k points at once, rather than a single point. It also involves a hyper-parameter \\(\\alpha\\), which is the learning rate, and determines how quickly the weight vectors change over iterations.\n\nfrom perceptron_minibatch import PerceptronMB, PerceptronMBOptimizer\n\n\nK = 1 for Linearly and Non-Linearly Separable Data\nMy first experiment was performing the algorithm for k = 1, so essentially recreating a regular update step with a single random point, for both linearly and non-linearly separable data.\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims=2)\n\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n\n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nI found that the mini-batch algorithm was able to converge with linearly separable data much like the regular perceptron algorithm.\n\nX, y = perceptron_data_nonlinear(n_points = 300, noise = 0.2, p_dims=2)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\nweights = []\n\nn = X.size()[0]\n\nmaxit = 1000 # max iterations\n\nwhile maxit &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    weights.append(torch.clone(p.w))\n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    k = 1\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    maxit-=1\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nFor non-linear data, I, again just as in the regular perceptron update, found that the algorithm wasn’t able to converge to loss equals zero.\n\n\nK = 10 for Linearly and Non-Linearly Separable Data\nNext, I tested the algorithm when it chose 10 random points in its update step, again for both linearly and non-linearly separable data.\n\nX, y = perceptron_data(n_points = 300, noise = 0.2, p_dims=2)\n\ntorch.manual_seed(1234567)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n\n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n\n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nThe algorithm was able to find loss = 0.0 for linearly separable data as expected.\n\nX, y = perceptron_data_nonlinear(n_points = 300, noise = 0.2, p_dims=2)\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\nweights = []\n\nn = X.size()[0]\n\nmaxit = 1000 # max iterations\n\nwhile maxit &gt; 0: \n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    weights.append(torch.clone(p.w))\n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement minibatch\n    k = 10\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    maxit-=1\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nAlso as expected, the algorithm was not able to find loss = 0.0 for non-linearly separable data.\n\n\nK = N for Non-Linearly Separable Data\nLastly, I checked whether, with k = n, in other words when the entire dataset is considered in a single update, the algorithm was able to converge even in the case of non-linearly separable data.\n\ntorch.manual_seed(1234567)\n\nX, y = perceptron_data_nonlinear()\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\n\n# for keeping track of loss values\nloss_vec_mini = []\n\nn = X.size()[0]\n\nfor index in range(0, 1000):\n    \n    # K is n\n    k = n\n    \n    # get a random submatrix of the feature matrix X and target vector y to implement mini-batch\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec_mini.append(loss)\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nInterestingly, I found that the algorithm was able to converge to close to zero (0.010) even with non-linearly separable data using k = n. I did have to adjust the learning rate to be very small (0.0001), in order for this to be possible."
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#perceptron-algorithm-runtime-analysis",
    "href": "posts/perceptron/PerceptronBlogPost.html#perceptron-algorithm-runtime-analysis",
    "title": "Implementing the Perceptron",
    "section": "Perceptron Algorithm Runtime Analysis",
    "text": "Perceptron Algorithm Runtime Analysis\nA single iteration of the perceptron algorithm involved taking the dot product of w, the weight vector, and any row of the feature matrix. This computation depends on the number of elements in each row of the feature matrix, i.e. the dimension of the feature matrix, p. A single iteration of the perceptron algorithm therefore takes \\(O(p)\\).\nA single iteration of the mini-batch perceptron algorithm involved taking the dot product of k points instead of just one. Therefore the runtime of a single iteration of the mini-batch algorithm is \\(O(k \\cdot p)\\)"
  },
  {
    "objectID": "posts/perceptron/PerceptronBlogPost.html#discussion",
    "href": "posts/perceptron/PerceptronBlogPost.html#discussion",
    "title": "Implementing the Perceptron",
    "section": "Discussion",
    "text": "Discussion\nThrough this blog post, I gained valuable insights into strengths and limiations of perceptron algorithm. I successfully implemented the perceptron algorithm, both the regular and mini-batch versions. I performed experiments to validate the accuracy of my implementation, allowing me to grasp the nuances of perceptron updates and its behavior across diverse data types, including higher dimensions and non-linearly separable data. By exploring the mini-batch approach, I demonstrated its functionality akin to a single-point process with k = 1, its effectiveness in identifying separating lines in linearly separable data with k=10, and its ability to converge even in the absence of linear separability when k=n."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Newton’s Optimizer\n\n\n\n\n\nImplementing and testing the newton’s optimizer method for logistic regression.\n\n\n\n\n\nMay 1, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Societal Inequity’s Effect on (Model-Perceived) Health Outcomes\n\n\n\n\n\nFinal Project Blog Post\n\n\n\n\n\nApr 18, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nImplementing and testing the logistic regression algorithm.\n\n\n\n\n\nApr 8, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron\n\n\n\n\n\nImplementing and testing the perceptron algorithm.\n\n\n\n\n\nMar 28, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nReplication Study - Dissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nReplicating the findings of Obermeyer et. al. (2019) on healthcare risk scores.\n\n\n\n\n\nMar 5, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nFinding a 100% accurate classification model for the palmer penguins dataset.\n\n\n\n\n\nFeb 19, 2024\n\n\nSophie Seiple\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Methods, Data Cleaning Document\n\n\n\n\n\nProcess of data cleaning.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Results, Model Creation Document\n\n\n\n\n\nProcess of model creation.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Methods, Exploratory Data Analysis\n\n\n\n\n\nVisualizing the data we used.\n\n\n\n\n\nJan 1, 2024\n\n\nSophie Seiple, Julia Joy, Lindsey Schweitzer\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  }
]